Job is using 1 GPU(s) with ID(s) 0 and 8 CPU core(s)
torch cuade is available: True
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wheretheressmoke
WER
[0.04207089 0.03975845 0.04010421 0.03554661 0.0406593  0.03491259
 0.03873168 0.03784912 0.03832665 0.03760811 0.03106324 0.04144817
 0.04078438 0.03279782 0.03584328 0.0353223  0.03758642 0.03571726
 0.03810674 0.04016526 0.03623029 0.03280262 0.03722642 0.03604771
 0.04237269 0.03834353 0.03466181 0.03915694 0.04395726 0.03909755
 0.04007107 0.04233708 0.0353146  0.03539313 0.03812897 0.03641498
 0.04108804 0.03770175 0.0388497  0.03896745 0.03910616 0.03759435
 0.03603846 0.04576593 0.03626688 0.04529647 0.03464036 0.0366249
 0.04153817 0.03665373 0.03728854 0.04297677 0.038457   0.03139175
 0.03940035 0.04100399 0.04067381 0.04109974 0.03509928 0.03878284
 0.03590718 0.03902198 0.03844558 0.03863596 0.03541504 0.03832522
 0.03605044 0.03380483 0.04439898 0.04107896 0.04063482 0.03394181
 0.03998031 0.03914803 0.0350556  0.04029947 0.03994065 0.04234517
 0.03732423 0.03524135 0.03787867 0.03928348 0.03876756 0.04142036
 0.0399773  0.04148235 0.03869311 0.03791951 0.03491991 0.03632593
 0.03558724 0.04181796 0.03598531 0.0325142  0.04304211 0.04878864
 0.04133851 0.03771452 0.03892913 0.03860557 0.03677646 0.03910263
 0.0348801  0.03450264 0.03653265 0.04671165 0.03416974 0.04258228
 0.0366188  0.03443623 0.03661498 0.03986515 0.03556767 0.03622412
 0.03965474 0.03652696 0.03631032 0.03241363 0.03955976 0.04125953
 0.03942445 0.04312664 0.04297009 0.03505505 0.04006731 0.03745686
 0.03759575 0.03759688 0.03510957 0.03306814 0.04985405 0.0440887
 0.03200105 0.04277804 0.03499831 0.03963315 0.04280134 0.03725939
 0.03520975 0.03805337 0.04079526 0.04003272 0.04095717 0.03271436
 0.03842025 0.03566816 0.03300833 0.04112739 0.03746461 0.03586659
 0.03485116 0.03223369 0.03195599 0.04102661 0.03625409 0.04055173
 0.04217463 0.03962388 0.0341637  0.04012756 0.04390052 0.04218819
 0.04034403 0.03946761 0.03811558 0.04013431 0.04244033 0.03618469
 0.03908738 0.03992683 0.0444328  0.04062036 0.03615873 0.03941185
 0.03944001 0.04001547 0.04098216 0.03504726 0.04495519 0.04001059
 0.03430811 0.04310232 0.03309673 0.04157505 0.03510315 0.03877477
 0.04684833 0.0462965  0.03617964 0.03564114 0.04349054 0.04202658
 0.03334719 0.04182267 0.03610119 0.04155472 0.04085513 0.03723302
 0.04049972 0.04121811]
wheretheressmoke
BLEU
[0.19710373 0.19773971 0.19101483 0.19815049 0.19138907 0.18037187
 0.20099261 0.21199418 0.18793408 0.18699334 0.18753008 0.1919208
 0.19524932 0.18261707 0.17675832 0.20370522 0.18975939 0.18729328
 0.19458669 0.18925927 0.19989896 0.18611199 0.18398542 0.18357707
 0.19888252 0.17363435 0.18145038 0.19526629 0.20876904 0.19093033
 0.19853174 0.2018356  0.18491192 0.19089354 0.18665765 0.1812417
 0.19885891 0.19497623 0.19664933 0.1895433  0.19834181 0.19616848
 0.19160177 0.20247101 0.19029434 0.20199858 0.18150336 0.19479892
 0.19356281 0.1814542  0.1959388  0.20003057 0.20003652 0.18256725
 0.18982598 0.19703983 0.19059172 0.19838142 0.18697785 0.18458353
 0.1920287  0.18513306 0.19085931 0.19516276 0.18778038 0.19538489
 0.18131636 0.18386167 0.19968853 0.18934075 0.19334825 0.18648831
 0.19902941 0.18845559 0.19486205 0.20094899 0.18899811 0.19148257
 0.20227382 0.18469873 0.18278153 0.18983699 0.1967429  0.19316139
 0.20364663 0.1951419  0.18982078 0.1953984  0.18287644 0.18291221
 0.19080015 0.18862339 0.17858561 0.19360414 0.19442385 0.20552809
 0.19250723 0.18659925 0.19186759 0.20334715 0.18059693 0.18981198
 0.18806958 0.1919829  0.1864454  0.19694698 0.18371365 0.20240866
 0.19139386 0.1883632  0.18546802 0.19720367 0.18478659 0.19221599
 0.18400724 0.18943134 0.19226561 0.18972168 0.18935309 0.19239197
 0.19801118 0.1963803  0.18462547 0.18398367 0.19804054 0.20121384
 0.1887813  0.19427454 0.19907203 0.19141853 0.21844061 0.20702261
 0.17965769 0.19841078 0.1889632  0.19166192 0.19432315 0.18664583
 0.19182981 0.18826165 0.19841835 0.1859211  0.20441532 0.18778324
 0.18841474 0.18997604 0.18995345 0.20606204 0.1841342  0.18575338
 0.18774339 0.18241178 0.18623764 0.19419151 0.1888923  0.20290744
 0.1910397  0.19659564 0.18645169 0.18747341 0.1978065  0.19405163
 0.19137723 0.20058045 0.18640055 0.18771981 0.19212869 0.19078896
 0.19892044 0.1900669  0.19696836 0.20194011 0.19427031 0.20301485
 0.20267624 0.19554215 0.19503361 0.1949777  0.19624791 0.19354283
 0.19152378 0.18970667 0.18141101 0.21010958 0.18977653 0.19516709
 0.19165502 0.20175369 0.19444111 0.18725474 0.19406505 0.20014368
 0.18654268 0.20185327 0.18507272 0.19120589 0.19857122 0.19005201
 0.17955524 0.19692197]
wheretheressmoke
METEOR
[0.13908125 0.14079568 0.13739486 0.14177846 0.13846639 0.13487207
 0.14445563 0.14996287 0.13557048 0.13625826 0.14067677 0.13783767
 0.14128328 0.1319098  0.13068464 0.1456664  0.13784917 0.13182903
 0.14081475 0.13386436 0.14070132 0.13105459 0.13569906 0.13458605
 0.14667675 0.12952372 0.13205629 0.14329777 0.14659766 0.14064547
 0.13952782 0.14414841 0.13214588 0.14062691 0.13582403 0.13623125
 0.14385658 0.13719216 0.14397366 0.14292653 0.14206377 0.14139816
 0.13465421 0.14701163 0.13950444 0.14485663 0.13939693 0.13882883
 0.13634673 0.13214593 0.14311106 0.13799809 0.13919525 0.13748659
 0.13530065 0.13824737 0.13660602 0.14543182 0.12949955 0.13759823
 0.1362921  0.1366773  0.13800085 0.13933411 0.1301584  0.13500358
 0.13364812 0.13475016 0.14333921 0.14106254 0.14059555 0.13918564
 0.14117726 0.13287093 0.14124258 0.14699029 0.13555791 0.13746515
 0.14264149 0.13831906 0.13761407 0.13764573 0.13791435 0.1400558
 0.14426657 0.1437544  0.13713783 0.14195507 0.13079921 0.13462681
 0.13990184 0.13779858 0.13252686 0.13923851 0.13982291 0.15091376
 0.14255513 0.14023629 0.13857422 0.14482484 0.13385906 0.14035027
 0.13448235 0.13721636 0.13899829 0.14018503 0.13271012 0.14833893
 0.13718056 0.14312624 0.13635439 0.14188437 0.13680929 0.13735037
 0.13378848 0.13566375 0.13447968 0.13884389 0.13811714 0.14233401
 0.1405311  0.14358244 0.13481653 0.13265037 0.13937411 0.14760164
 0.13353364 0.14036838 0.14438458 0.13511147 0.14948097 0.14466312
 0.12721419 0.14172054 0.13745561 0.14204096 0.14131578 0.13430935
 0.13650583 0.13556986 0.1409394  0.13561368 0.14051158 0.13486267
 0.1337229  0.14186197 0.13458808 0.14618584 0.13663501 0.14252944
 0.13953643 0.13157792 0.13227135 0.14098884 0.13654591 0.1432335
 0.13756475 0.14747954 0.13614612 0.13675792 0.14138261 0.13686847
 0.13576426 0.14461162 0.13340191 0.1373725  0.13924818 0.1328046
 0.14000553 0.13648101 0.13821939 0.14923414 0.137527   0.14126116
 0.13645859 0.14488228 0.14344338 0.14138504 0.14022201 0.14080143
 0.14130461 0.13607909 0.13530996 0.15158963 0.13479689 0.13938552
 0.1383337  0.14543531 0.14078748 0.13953371 0.14303237 0.14207761
 0.14053773 0.14260804 0.13531647 0.14001462 0.14766777 0.14322132
 0.12932468 0.14157126]
wheretheressmoke
BERT
[0.7907687  0.7901323  0.7900033  0.78921556 0.79040205 0.78864366
 0.7922332  0.7905336  0.7900549  0.78931457 0.78968805 0.7907688
 0.7906708  0.7901537  0.7895664  0.7898777  0.78911006 0.78880197
 0.7892161  0.78862107 0.7919904  0.7899849  0.7910636  0.79101276
 0.7902468  0.7885312  0.78893554 0.79179204 0.79145825 0.7901327
 0.7907618  0.7896979  0.78916264 0.7911466  0.79020655 0.7906373
 0.79210955 0.79034626 0.7912701  0.7910807  0.78894854 0.7908002
 0.7878109  0.7925843  0.7911324  0.79168296 0.7899408  0.79073465
 0.7903891  0.7900143  0.790765   0.7901131  0.79120153 0.7881794
 0.78870344 0.78970116 0.78832453 0.7906791  0.78847665 0.78989834
 0.7888742  0.7898447  0.7906238  0.78996253 0.78791094 0.79069257
 0.7885167  0.7882068  0.7899441  0.79131633 0.79196256 0.7903858
 0.79183334 0.7899835  0.7887081  0.79274076 0.78880405 0.7884095
 0.7913437  0.7898835  0.78995997 0.7916536  0.79023385 0.79014647
 0.7907503  0.7897898  0.7909637  0.791649   0.78915316 0.7890291
 0.7906189  0.7914071  0.79084915 0.7888727  0.79072475 0.79334724
 0.79156464 0.7898069  0.7896813  0.79128826 0.78870434 0.7907073
 0.7885346  0.7902097  0.7905366  0.79101896 0.788812   0.7928381
 0.78896534 0.7908205  0.7885857  0.79195136 0.7899913  0.78915894
 0.79080784 0.78984    0.7898403  0.7908683  0.7893367  0.79023063
 0.78919196 0.79178435 0.7889902  0.78878826 0.79236424 0.79026204
 0.7900352  0.7910904  0.7905498  0.78845906 0.79086363 0.790118
 0.78765476 0.78814757 0.7893404  0.79006124 0.78973025 0.7896297
 0.78939736 0.79103774 0.788976   0.7910625  0.79038936 0.7888296
 0.7902777  0.7907752  0.78902733 0.79060024 0.78926325 0.7910659
 0.789897   0.7902066  0.7899138  0.78763354 0.78772634 0.78943044
 0.79087627 0.7888331  0.78935087 0.7910121  0.79150134 0.7923266
 0.7892446  0.7912079  0.7902728  0.78942525 0.7890114  0.7875847
 0.78946525 0.7903436  0.79147476 0.79141545 0.79066527 0.7898873
 0.79106164 0.79049104 0.7905819  0.790896   0.7909003  0.7871015
 0.79085433 0.78818935 0.78953516 0.7916753  0.78748196 0.7885229
 0.7899582  0.79119354 0.7911286  0.7903703  0.7909172  0.79044867
 0.79030687 0.7915964  0.79058135 0.79053795 0.7902319  0.79125
 0.7889291  0.79109836]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/net/scratch/q12628ct/semantic-decoding/decoding/evaluate_predictions.py", line 50, in <module>
    ref_data = load_transcript(args.experiment, reference)
  File "/net/scratch/q12628ct/semantic-decoding/decoding/utils_eval.py", line 23, in load_transcript
    with open(grid_path) as f: 
FileNotFoundError: [Errno 2] No such file or directory: '/net/scratch/q12628ct/semantic-decoding/data_test/test_stimulus/perceived_movie/sintel.TextGrid'
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
presto
WER
[0.0243129  0.03940146 0.03000514 0.02033055 0.035361   0.03825857
 0.03759519 0.03171341 0.01948316 0.02247985 0.03376956 0.032177
 0.03145711 0.02440112 0.0277422  0.03264    0.02443218 0.03494725
 0.01982945 0.02025628 0.03037885 0.02281307 0.03324329 0.04140254
 0.02240283 0.03646537 0.03734462 0.02888426 0.02563696 0.02742079
 0.02994798 0.02509505 0.03603572 0.02626221 0.03677315 0.04112076
 0.03281632 0.02870503 0.03862203 0.03217053 0.03080061 0.0358517
 0.02828485 0.04238679 0.03327473 0.02650657 0.02858866 0.02441043
 0.03831443 0.02729553 0.03197539 0.03108235 0.02927819 0.02578931
 0.02743041 0.02936456 0.02964547 0.03095732 0.03918977 0.02684494
 0.03423268 0.02067525 0.03588718 0.02933657 0.03581939 0.03046733
 0.03202946 0.0339943  0.03548333 0.03048249 0.02718046 0.02695188
 0.03468522 0.02830885 0.03367495 0.02575914 0.02094455 0.03151511
 0.02497327 0.03121136 0.0374602  0.03367802 0.03470141 0.02843746
 0.01811074 0.03339048 0.02311916 0.02990031 0.03558657 0.03692689
 0.03049126 0.03100646 0.03779911 0.03069586 0.03052545 0.03419081
 0.02159485 0.03225773 0.02639436 0.02990709 0.03177262 0.02686101
 0.03742996 0.03557197 0.03329445 0.04032368 0.0348806  0.02444902
 0.03332665 0.03923276 0.0241152  0.04274427 0.02270016 0.02464262
 0.03059349 0.02771542 0.0311876  0.02855238 0.03508618 0.04171421
 0.04159133 0.03720281 0.029746   0.02200369 0.03167168 0.03457134
 0.0315834  0.02963818 0.03853018 0.02569801 0.03188425 0.02277967
 0.03964768 0.03083762 0.03508786 0.02919287 0.02158592 0.0260951
 0.03019501 0.03023747 0.03206299 0.03273833 0.0275511  0.02475004
 0.03170478 0.02622465 0.03176942 0.03006027 0.02919234 0.03168637
 0.04011035 0.03548118 0.02957052 0.02908936 0.03174539 0.02480777
 0.03041266 0.030459   0.03341536 0.03058743 0.03670775 0.03941771
 0.02449815 0.02826286 0.03334204 0.02525208 0.03176099 0.03082166
 0.0307768  0.02777049 0.03306    0.0319435  0.03279254 0.03462283
 0.04050862 0.03189609 0.04131355 0.0355394  0.03374807 0.0283138
 0.0349059  0.03030946 0.02900035 0.03578738 0.03166837 0.0235313
 0.03479696 0.03086394 0.02577369 0.03903394 0.02751465 0.02152199
 0.03627985 0.03685342 0.0267516  0.02494148 0.03720458 0.02215439
 0.02723182 0.02919002]
presto
BLEU
[0.10153299 0.11861791 0.11568994 0.10777393 0.12385657 0.12449272
 0.13828199 0.10704715 0.09335605 0.10588014 0.11604041 0.1270718
 0.13671765 0.09149824 0.10433527 0.12425649 0.10274677 0.12577482
 0.10910795 0.09533135 0.11383427 0.09569745 0.12784293 0.1480794
 0.11035548 0.12522055 0.13143559 0.12098051 0.09956573 0.11052378
 0.11616774 0.10387506 0.12968639 0.10813391 0.13303014 0.1375975
 0.11291762 0.11181874 0.13179318 0.11818091 0.11098758 0.12461253
 0.11467863 0.13664744 0.12223981 0.11257625 0.12140271 0.10857204
 0.13266276 0.10937506 0.11331999 0.11817326 0.11962076 0.11639073
 0.10803786 0.1106514  0.1123495  0.11327222 0.1390419  0.11098226
 0.11451117 0.088511   0.1204755  0.11338288 0.12507722 0.1152744
 0.10837161 0.12764743 0.12250436 0.12236973 0.11649098 0.08653599
 0.12892561 0.12416051 0.12829533 0.12164076 0.11034112 0.12408473
 0.11499012 0.12185273 0.13007889 0.13468655 0.12624591 0.12109693
 0.10665134 0.12039397 0.09945198 0.11520672 0.13142716 0.14015869
 0.10102154 0.10894066 0.13548835 0.11086761 0.11326696 0.12137681
 0.10343452 0.13026085 0.10785262 0.11818012 0.12311231 0.11827104
 0.13179077 0.13958916 0.12192867 0.13339458 0.12001168 0.10397187
 0.12966449 0.14195085 0.09689209 0.13260822 0.10167038 0.10456806
 0.11617476 0.10544982 0.13230625 0.11346425 0.12921443 0.13715668
 0.13262197 0.13354282 0.10528785 0.10504765 0.1095465  0.13202929
 0.11491087 0.11522305 0.12746732 0.11157809 0.12810044 0.09868888
 0.14269853 0.12387877 0.12746707 0.11983449 0.11020796 0.10020124
 0.11327679 0.11215947 0.12636009 0.1106655  0.12467079 0.09667655
 0.12746349 0.11212332 0.1248332  0.11161414 0.11284319 0.10975534
 0.13119471 0.11240509 0.10931638 0.11440585 0.12118169 0.10566347
 0.11992232 0.11429686 0.13498348 0.11542186 0.13309737 0.13264446
 0.11876972 0.11117375 0.11039574 0.11311058 0.11908289 0.11166805
 0.11879399 0.11081642 0.12642532 0.11714862 0.1157732  0.11748497
 0.12817062 0.11238024 0.13317686 0.13161848 0.11421848 0.1131927
 0.12099353 0.1206939  0.12143762 0.12192136 0.11517039 0.10936453
 0.123889   0.12110626 0.1097057  0.12721059 0.11145929 0.11425148
 0.13387995 0.12231815 0.10414061 0.09897721 0.11746545 0.10308249
 0.10916311 0.11854416]
presto
METEOR
[0.07649049 0.08453863 0.07784527 0.07855991 0.09124319 0.08683439
 0.09410567 0.07499492 0.06963369 0.07962379 0.07948376 0.09104715
 0.08922109 0.07133667 0.07783956 0.08489194 0.07004118 0.08533158
 0.07634052 0.06751797 0.07923785 0.07558664 0.08989708 0.09649896
 0.07968339 0.08163618 0.08737027 0.08533481 0.0708655  0.0797936
 0.07790706 0.07483815 0.08858104 0.0763344  0.08861591 0.0914022
 0.07995743 0.07766628 0.08453936 0.07951539 0.07741922 0.08414091
 0.07864799 0.0905045  0.08070094 0.07854545 0.08687301 0.08388105
 0.08326704 0.07476686 0.0788149  0.08134725 0.08567679 0.07896429
 0.07685559 0.07673789 0.078548   0.07429187 0.09426383 0.07937007
 0.08061595 0.06179359 0.0872844  0.08445004 0.08198337 0.07713557
 0.081714   0.09081504 0.07801156 0.0811991  0.08404799 0.06945269
 0.08706473 0.09049957 0.08784711 0.08749308 0.07633953 0.08648343
 0.07883496 0.08316729 0.08659855 0.08916335 0.09253834 0.08657116
 0.07383461 0.08298388 0.07546348 0.0801765  0.08547697 0.09575977
 0.07147172 0.07436134 0.08929528 0.07997726 0.07913534 0.08401136
 0.0686859  0.0855236  0.08049397 0.08197248 0.08758478 0.08660581
 0.09113163 0.09653603 0.08430127 0.09131035 0.08310713 0.07124035
 0.08588402 0.09132367 0.07052776 0.09393727 0.07548219 0.07728911
 0.07847005 0.07773033 0.0935444  0.07961388 0.08878362 0.09184843
 0.0968582  0.08865359 0.07464502 0.07776915 0.07941888 0.09060534
 0.0784767  0.08759642 0.08318701 0.07871567 0.08580976 0.06862652
 0.09437706 0.08378932 0.08553427 0.08672368 0.07861459 0.07372142
 0.07661745 0.08108955 0.08695002 0.0768245  0.08698747 0.07120138
 0.08518345 0.08052071 0.08591181 0.07672892 0.07817183 0.08106855
 0.0883824  0.07828757 0.08021937 0.07598977 0.07875463 0.07215437
 0.08209017 0.08273319 0.09380185 0.08156128 0.08404895 0.09463646
 0.07816124 0.07943867 0.07328001 0.07397215 0.08368476 0.0749603
 0.08401043 0.07903438 0.08745772 0.08230544 0.08147712 0.07608501
 0.08549714 0.07981612 0.08483922 0.08862696 0.07913164 0.07766828
 0.08140462 0.08388177 0.08621135 0.08103879 0.08283693 0.07836338
 0.08739567 0.08428995 0.07692626 0.08365495 0.07790504 0.07838221
 0.08835428 0.08336791 0.07455915 0.07397034 0.08681422 0.07710856
 0.07941401 0.08509289]
presto
BERT
[0.7688167  0.7695397  0.77109766 0.77003604 0.77245736 0.7716724
 0.77444404 0.7709081  0.7685646  0.77109814 0.7708783  0.77291936
 0.7712505  0.76729697 0.7689637  0.7707242  0.7714574  0.77495944
 0.7702892  0.7701515  0.7720135  0.7710514  0.7760876  0.77453446
 0.770802   0.77405685 0.7724408  0.76972604 0.767438   0.7727657
 0.7708363  0.7709249  0.7729787  0.77108335 0.7739458  0.77002096
 0.7694105  0.7703539  0.7749808  0.7694609  0.770549   0.77195585
 0.7698819  0.7710483  0.77326524 0.7704814  0.7729326  0.7744103
 0.7726908  0.7731547  0.7724389  0.76983225 0.77046233 0.7699357
 0.7692311  0.7718334  0.7713992  0.7723799  0.775592   0.77157897
 0.77244085 0.7688531  0.7750699  0.77152205 0.7737867  0.76992214
 0.7723154  0.77201056 0.7733622  0.77284414 0.77290726 0.7696289
 0.77231055 0.77326876 0.7728921  0.775083   0.7701263  0.77173567
 0.774523   0.7730665  0.77152455 0.7735327  0.77154297 0.7714796
 0.7690661  0.7703651  0.7721505  0.7726545  0.7717447  0.77604425
 0.77024686 0.7697137  0.775734   0.7715357  0.77023095 0.77337164
 0.7695349  0.77654886 0.7693477  0.7702772  0.77139086 0.77171564
 0.77253026 0.7713935  0.7724498  0.7747374  0.7727185  0.76873946
 0.7721576  0.7717116  0.76931906 0.7722661  0.7713426  0.77153164
 0.7713407  0.7683816  0.7711234  0.770114   0.771295   0.77372247
 0.7756598  0.77015877 0.7715641  0.7700907  0.7681733  0.7728317
 0.7709305  0.7732764  0.77125704 0.772894   0.77317715 0.7699917
 0.7749864  0.7738579  0.7728613  0.7735906  0.7684939  0.7696014
 0.77248836 0.7707127  0.7724955  0.7696475  0.77194655 0.7709226
 0.77408445 0.769508   0.770833   0.7696035  0.7722805  0.7693866
 0.77263165 0.7684559  0.7712061  0.771462   0.7728935  0.7713495
 0.77169067 0.7727346  0.7744681  0.77017474 0.77251774 0.77341497
 0.7730514  0.77214366 0.7700971  0.7715853  0.77249825 0.76921207
 0.77468    0.7690149  0.7727504  0.76973635 0.7690021  0.7759123
 0.7716556  0.77416307 0.7725608  0.77458453 0.77315456 0.7707845
 0.77308816 0.77190083 0.77356815 0.77285033 0.77073246 0.7712303
 0.7716745  0.77134115 0.76896495 0.7689219  0.7699524  0.77070755
 0.77349764 0.77146906 0.77095145 0.7708783  0.77113026 0.7749626
 0.7719397  0.7707708 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
partlycloudy
WER
[-0.14983047 -0.16033239 -0.14962002 -0.15189022 -0.15950992 -0.13785639
 -0.16325462 -0.15891757 -0.15440564 -0.15203597 -0.17168757 -0.14766006
 -0.15570125 -0.16064692 -0.16031925 -0.15801787 -0.15621177 -0.15988032
 -0.15842308 -0.15894988 -0.15241985 -0.15956458 -0.1614659  -0.16305679
 -0.16034874 -0.15370573 -0.14997313 -0.16077865 -0.15625233 -0.16669583
 -0.15724708 -0.16195952 -0.15863707 -0.15410502 -0.15408567 -0.14922897
 -0.16192288 -0.1610523  -0.15517064 -0.15591249 -0.15879253 -0.16214331
 -0.15112785 -0.15025616 -0.15822071 -0.1662877  -0.15123181 -0.16167434
 -0.15059825 -0.16664455 -0.15842216 -0.15763909 -0.15983315 -0.15448736
 -0.16217805 -0.15724226 -0.15049939 -0.14971738 -0.15909346 -0.16233395
 -0.15555784 -0.15052791 -0.14864102 -0.1603218  -0.15555531 -0.16344296
 -0.16312223 -0.15952693 -0.14714438 -0.16050234 -0.17082939 -0.15586878
 -0.15971728 -0.1681755  -0.14813552 -0.1561507  -0.15469659 -0.15076213
 -0.15948509 -0.15379044 -0.16069726 -0.16291743 -0.15603063 -0.15366308
 -0.15412567 -0.15392895 -0.16003715 -0.14834091 -0.14939426 -0.16301552
 -0.15864429 -0.14778927 -0.1508088  -0.15569015 -0.15885496 -0.1627264
 -0.16392219 -0.15725248 -0.15914947 -0.15769439 -0.1627699  -0.15361852
 -0.15514242 -0.14631912 -0.15671102 -0.16601386 -0.1504495  -0.15520937
 -0.14888598 -0.15514213 -0.15945477 -0.16709529 -0.15359237 -0.15252917
 -0.15147657 -0.16137986 -0.15354925 -0.15241763 -0.15228727 -0.15270055
 -0.16055619 -0.15798016 -0.15637827 -0.16229259 -0.15779701 -0.16088261
 -0.15716034 -0.15390859 -0.15720102 -0.15406861 -0.15880896 -0.15323535
 -0.15501329 -0.15695245 -0.15962082 -0.16037038 -0.14865103 -0.15111794
 -0.14790467 -0.15768103 -0.1384054  -0.16246872 -0.15230908 -0.15346966
 -0.15665982 -0.14951007 -0.16459823 -0.15527827 -0.15453585 -0.15578184
 -0.15776243 -0.15568986 -0.15808154 -0.16003176 -0.15714429 -0.15567557
 -0.15583784 -0.14898641 -0.15143208 -0.15730273 -0.15958016 -0.15487385
 -0.15436108 -0.15867954 -0.15114633 -0.159118   -0.15414321 -0.15086175
 -0.15689567 -0.15456577 -0.15365271 -0.16639887 -0.15650924 -0.15179972
 -0.15242623 -0.15451055 -0.15527825 -0.15389109 -0.16430127 -0.1605619
 -0.14710233 -0.13891103 -0.16143237 -0.15359893 -0.14342114 -0.15300875
 -0.1515292  -0.1562473  -0.16246356 -0.16193624 -0.15982901 -0.15707042
 -0.16153514 -0.15533469 -0.1604932  -0.15227372 -0.17004422 -0.15803688
 -0.16346462 -0.15003293]
partlycloudy
BLEU
[0.13664814 0.12081637 0.12869127 0.13304807 0.12001749 0.14466773
 0.10343189 0.12171078 0.12640191 0.1483493  0.10586086 0.13367162
 0.1239678  0.12220729 0.10581554 0.11990029 0.13212035 0.11940547
 0.12009352 0.1251176  0.12247835 0.12223289 0.12103472 0.11844599
 0.10616871 0.12664602 0.15280024 0.11363308 0.12727409 0.11082682
 0.13135997 0.11841977 0.11964099 0.12905693 0.11797453 0.14049826
 0.12625003 0.10873873 0.12115891 0.11688954 0.11783121 0.1179838
 0.13065344 0.1299374  0.1169005  0.10189077 0.12735522 0.12572144
 0.12166321 0.11678519 0.13191315 0.11272256 0.11874072 0.13602568
 0.1202915  0.12490506 0.13023544 0.12518357 0.12112844 0.12308882
 0.1216591  0.12756516 0.14145926 0.11577486 0.1278075  0.1105825
 0.11615041 0.11967932 0.15095104 0.11656853 0.10422593 0.11340983
 0.12400348 0.1103497  0.12829218 0.12122    0.12655387 0.11353829
 0.10386354 0.13737619 0.12087621 0.11250974 0.13342254 0.12045748
 0.13072447 0.12600645 0.12995932 0.13638335 0.14506666 0.12945702
 0.11689199 0.14335369 0.13792505 0.11642174 0.11667294 0.11599916
 0.11474498 0.11932622 0.11877785 0.12678227 0.11284398 0.13444286
 0.13449669 0.1311878  0.13995518 0.11220842 0.12238975 0.12107714
 0.13536664 0.13390745 0.12203063 0.11168412 0.12209451 0.12303823
 0.12627929 0.13148575 0.12601511 0.13364784 0.12619977 0.13102257
 0.1359192  0.12415639 0.11621098 0.1152395  0.11754326 0.1279609
 0.12868974 0.12726366 0.13112651 0.12744859 0.12355695 0.13573443
 0.13882776 0.13141823 0.12065755 0.11377525 0.12282749 0.11566716
 0.14065577 0.11615576 0.13772275 0.13295595 0.13768694 0.1218296
 0.12381019 0.14233994 0.12522112 0.12298244 0.13113123 0.12189751
 0.12056872 0.10957683 0.11137684 0.12579123 0.11885878 0.13118577
 0.12169764 0.12896172 0.13025573 0.12443568 0.12770441 0.12443052
 0.12594164 0.12767529 0.1247965  0.11199583 0.12256219 0.13676177
 0.14123939 0.11627412 0.123196   0.11055474 0.11891922 0.12852958
 0.14119706 0.11135189 0.12082971 0.12396335 0.13330962 0.12819543
 0.13256676 0.13618538 0.11799553 0.11859066 0.13224713 0.12062553
 0.13175025 0.12704296 0.11459134 0.12119818 0.11869361 0.1355484
 0.11005751 0.13450355 0.12583288 0.11915542 0.10665257 0.11893358
 0.12751583 0.13964679]
partlycloudy
METEOR
[0.10911787 0.10700041 0.10560125 0.10385238 0.10498988 0.11915346
 0.08734741 0.10822915 0.10005139 0.1188629  0.09366224 0.1084051
 0.10489954 0.09499167 0.08939255 0.10046913 0.10192995 0.09238986
 0.10188115 0.10193345 0.10348449 0.09920703 0.10246773 0.09346179
 0.09672737 0.10030838 0.12055453 0.09666915 0.10552966 0.09558121
 0.10916614 0.0919572  0.09640042 0.102617   0.0985947  0.11290752
 0.10128941 0.09804325 0.09906331 0.09329593 0.1000336  0.09788557
 0.10805065 0.11032156 0.09938342 0.09505489 0.10317614 0.10768883
 0.10168502 0.09840617 0.10562555 0.09419486 0.10014223 0.10446065
 0.10242697 0.10618959 0.10600151 0.10386636 0.0895064  0.10527413
 0.09666402 0.10489275 0.10999764 0.0980916  0.1074903  0.09361122
 0.0975716  0.09870643 0.11774343 0.09907845 0.09141165 0.09557968
 0.10010655 0.09434995 0.10729217 0.10039233 0.09768655 0.09484651
 0.09280158 0.10773031 0.10445208 0.10262357 0.10620783 0.1068221
 0.10611219 0.11045281 0.10611682 0.10914166 0.11596318 0.1035703
 0.1018289  0.11466643 0.11525236 0.09699695 0.0992323  0.09981549
 0.09119278 0.09927035 0.09664576 0.10464777 0.09408177 0.1139631
 0.1053459  0.10747083 0.11709184 0.09909395 0.10095163 0.09543637
 0.11248698 0.11093961 0.10091611 0.09213311 0.09899444 0.10212163
 0.10653998 0.10152236 0.09687889 0.11320301 0.10049981 0.10814077
 0.10342824 0.10131791 0.09408627 0.09536236 0.10595137 0.10372937
 0.1039928  0.10798977 0.10423266 0.10332071 0.10314358 0.1101653
 0.11146039 0.10884186 0.09968649 0.09402214 0.10079822 0.09607333
 0.1145992  0.10353742 0.1101256  0.10664202 0.10864668 0.09563568
 0.10108652 0.11135118 0.1011091  0.10571759 0.10690588 0.09861069
 0.10329337 0.09077738 0.0926071  0.10029291 0.09977495 0.10428037
 0.10706234 0.10856384 0.10643266 0.10035586 0.10469607 0.10017444
 0.10119968 0.1027113  0.10820794 0.09245454 0.09968035 0.10791967
 0.11454951 0.09823097 0.10555507 0.09219264 0.09745513 0.10812664
 0.11354841 0.09589385 0.10016303 0.10304987 0.10929339 0.10210096
 0.1071818  0.10911851 0.10075908 0.09803404 0.10836796 0.09692283
 0.10727258 0.10136961 0.09883337 0.09864091 0.10036234 0.10871006
 0.08973405 0.10644422 0.10112803 0.09542011 0.09140856 0.10572636
 0.10460191 0.11929655]
partlycloudy
BERT
[0.778585   0.77807885 0.7801814  0.7780595  0.77832943 0.7818943
 0.7793622  0.78039795 0.7809513  0.7816445  0.77730477 0.7821574
 0.7780344  0.77906656 0.780062   0.7807545  0.77891207 0.7779939
 0.7803906  0.77870166 0.77920216 0.77948487 0.777107   0.7778607
 0.7797731  0.78164536 0.78193915 0.7809821  0.7778657  0.78048116
 0.7815774  0.7795663  0.7805516  0.77917796 0.77947235 0.77868766
 0.7830996  0.77636945 0.78054315 0.7789684  0.77788097 0.7802319
 0.7807018  0.7786847  0.7810567  0.7770351  0.78128535 0.7809764
 0.78009695 0.778899   0.7805203  0.77721226 0.7803853  0.7835001
 0.7791395  0.781682   0.78013885 0.77889913 0.77967525 0.78093624
 0.77943474 0.77718246 0.780837   0.7789264  0.7836378  0.780631
 0.77794033 0.7797018  0.78287035 0.7783658  0.77793384 0.7796371
 0.7793902  0.7821289  0.78057915 0.78332525 0.77901816 0.7753762
 0.7786337  0.780148   0.7783999  0.7798029  0.77928734 0.77817094
 0.7826747  0.7786365  0.77837354 0.78037745 0.7799088  0.77945423
 0.7776345  0.781515   0.78028274 0.7798957  0.7809652  0.7806827
 0.7793481  0.7771089  0.77886057 0.77969    0.77592194 0.7785468
 0.78187233 0.7780003  0.78067374 0.779146   0.7784565  0.77964735
 0.77845544 0.7793357  0.7819798  0.77969617 0.7802144  0.77945566
 0.7785893  0.77976567 0.7777222  0.7821871  0.7793404  0.7794239
 0.77752674 0.7785593  0.7785457  0.7775338  0.77923167 0.7800146
 0.7780486  0.777527   0.78241247 0.7805052  0.7829549  0.78084594
 0.7824778  0.7784513  0.7808594  0.7775415  0.7817717  0.77902436
 0.7812462  0.778697   0.7802954  0.7803617  0.7802377  0.7802852
 0.7825711  0.7825351  0.77961874 0.78149843 0.7826357  0.78090954
 0.77721906 0.78068537 0.77880853 0.78278625 0.780244   0.7806687
 0.779972   0.7803219  0.7809012  0.7800797  0.78050333 0.7803622
 0.7785872  0.7829538  0.77904946 0.77817154 0.7779018  0.78272104
 0.77904516 0.78013676 0.78000784 0.7791025  0.776597   0.77770823
 0.780522   0.7810028  0.77971506 0.7793664  0.7796996  0.77927506
 0.78002226 0.7784276  0.7787067  0.7789014  0.78154385 0.780163
 0.7804709  0.7803291  0.77859706 0.7813907  0.78094447 0.7809964
 0.7805513  0.7829835  0.78181803 0.778389   0.77918065 0.7781751
 0.7795307  0.77910334]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
laluna
WER
[-0.05737508 -0.05693136 -0.05486822 -0.06075316 -0.05723739 -0.05632023
 -0.0515196  -0.05391911 -0.07089191 -0.06099822 -0.06005354 -0.05797124
 -0.05440687 -0.05722408 -0.05986112 -0.05192425 -0.05343977 -0.04552135
 -0.05597051 -0.05906242 -0.06363933 -0.0518144  -0.059228   -0.06527467
 -0.05140327 -0.06272438 -0.06051691 -0.05924231 -0.05501758 -0.05744348
 -0.05798686 -0.05423902 -0.06181557 -0.05381204 -0.05356654 -0.05525363
 -0.0618221  -0.05192429 -0.05834356 -0.05851496 -0.05684078 -0.05646916
 -0.05699778 -0.05814112 -0.05942659 -0.05487326 -0.06127819 -0.0577124
 -0.06316702 -0.05826727 -0.05002345 -0.05234131 -0.05202611 -0.06244596
 -0.05317947 -0.06261518 -0.06084005 -0.05775473 -0.05220272 -0.05616235
 -0.05763641 -0.04549778 -0.06027546 -0.05143604 -0.04985095 -0.05445158
 -0.04960322 -0.05548301 -0.05982709 -0.06037082 -0.05570507 -0.05881561
 -0.05432705 -0.0509644  -0.06276035 -0.0565213  -0.05865369 -0.05374243
 -0.05840455 -0.05696065 -0.06563938 -0.05510988 -0.04838049 -0.05934378
 -0.05906899 -0.05715682 -0.05399202 -0.05732143 -0.05130536 -0.05062588
 -0.04769986 -0.05189742 -0.04825635 -0.05064424 -0.05834268 -0.05750314
 -0.05746809 -0.04752801 -0.05379183 -0.06132578 -0.0581501  -0.05330815
 -0.06206126 -0.0535499  -0.05806286 -0.0584024  -0.05459265 -0.05868911
 -0.05369832 -0.06187875 -0.04282405 -0.0526326  -0.05985964 -0.05619098
 -0.04843779 -0.05763827 -0.05045878 -0.04854706 -0.05336025 -0.05568426
 -0.06662861 -0.05635761 -0.05572664 -0.05314711 -0.05262547 -0.05770905
 -0.04503411 -0.06441876 -0.05295453 -0.06213567 -0.06029999 -0.0653497
 -0.05359293 -0.0519241  -0.0591417  -0.05817047 -0.05048162 -0.04965758
 -0.05433275 -0.06203234 -0.05794474 -0.05287723 -0.0629888  -0.05774831
 -0.04836098 -0.05648517 -0.05569814 -0.05709855 -0.05976717 -0.06223223
 -0.05586935 -0.05848217 -0.05500875 -0.05622122 -0.05381876 -0.06027096
 -0.05478568 -0.05668745 -0.05504228 -0.06817359 -0.04544314 -0.0593131
 -0.06196027 -0.05456638 -0.05789097 -0.0603955  -0.06530705 -0.06877304
 -0.04658938 -0.05329198 -0.05728641 -0.0559923  -0.05416942 -0.05358599
 -0.05868654 -0.0594557  -0.0488824  -0.0584077  -0.0611292  -0.05096256
 -0.05776777 -0.05528751 -0.05976491 -0.06770279 -0.04863579 -0.06208566
 -0.06111326 -0.05603257 -0.0602293  -0.05048825 -0.060787   -0.06313002
 -0.05351849 -0.05604025 -0.05620511 -0.05967344 -0.05200015 -0.05471633
 -0.05120362 -0.05836244]
laluna
BLEU
[0.11448464 0.12518177 0.13206889 0.12620399 0.12331599 0.13368542
 0.12857562 0.14813216 0.08689635 0.11049876 0.11672004 0.12111363
 0.13186566 0.12211552 0.11741489 0.12658294 0.12294088 0.13233674
 0.12676169 0.126202   0.10962602 0.12064323 0.11680133 0.1096152
 0.14069982 0.11454183 0.11527363 0.11434692 0.12272323 0.12831409
 0.11900333 0.12968487 0.13064773 0.12079662 0.12656021 0.12830018
 0.11657226 0.13459341 0.12717552 0.12352764 0.1302819  0.125211
 0.12275893 0.11735669 0.12770611 0.13150665 0.12635937 0.1261049
 0.12528699 0.11015965 0.1302431  0.1409394  0.13256969 0.1154535
 0.12864681 0.12352881 0.12065055 0.12482114 0.12747156 0.13035247
 0.11674023 0.13968618 0.11542917 0.12404754 0.13077562 0.13093261
 0.13090457 0.12644183 0.11988472 0.11731251 0.13102092 0.11471245
 0.12342954 0.12839147 0.11564451 0.12673596 0.11812916 0.14359671
 0.11652653 0.13212206 0.11209434 0.13085521 0.15292158 0.11702707
 0.13642327 0.12287853 0.13841204 0.11840358 0.12941631 0.13308481
 0.14218926 0.12369927 0.14304696 0.13970831 0.13591982 0.11986662
 0.11256827 0.14855648 0.12892094 0.11246285 0.12596025 0.13299564
 0.12420132 0.13034344 0.12471535 0.12998108 0.12284116 0.12519304
 0.13536912 0.10978624 0.13099948 0.12210688 0.10580602 0.13041257
 0.13257494 0.11193631 0.13981453 0.14668935 0.12298606 0.12162856
 0.10435305 0.12404766 0.13333928 0.14372904 0.12902383 0.124911
 0.15833962 0.12808238 0.13777246 0.11331538 0.11851784 0.10506703
 0.13791799 0.1319235  0.13878535 0.12338718 0.13190555 0.1381127
 0.1310749  0.11563609 0.12514609 0.12592444 0.1184391  0.12589792
 0.14955321 0.13872172 0.1346368  0.1155864  0.1148163  0.12201221
 0.11859933 0.13435875 0.12964945 0.12553272 0.13480483 0.1272567
 0.12125715 0.13746993 0.12223739 0.10002793 0.14466626 0.1179157
 0.12195312 0.12826846 0.13238603 0.12136335 0.11130761 0.10400793
 0.14590631 0.13732853 0.12503763 0.13632219 0.12636905 0.13968542
 0.12349176 0.12233803 0.13388453 0.12572128 0.12336915 0.1517789
 0.1172212  0.13082558 0.12904564 0.10171771 0.14388255 0.11230364
 0.12264497 0.12724283 0.10954103 0.12564748 0.12049619 0.14608044
 0.1297575  0.12422352 0.11806495 0.13536465 0.12996711 0.13405699
 0.13858613 0.12179284]
laluna
METEOR
[0.09222078 0.09092648 0.09920901 0.09479543 0.09182316 0.09278975
 0.09358845 0.10636226 0.06809386 0.08552883 0.08626031 0.09579632
 0.09638829 0.08727665 0.09132625 0.09818897 0.09416212 0.09971039
 0.08653224 0.09601936 0.08588024 0.09507895 0.09101254 0.08828687
 0.10102424 0.08797083 0.08365188 0.08859697 0.09423195 0.09996974
 0.08706799 0.10002211 0.09392496 0.0935053  0.10163742 0.09702829
 0.08673692 0.0967732  0.09788841 0.09361994 0.09761325 0.09345144
 0.09084299 0.08914798 0.09591647 0.09349617 0.09422422 0.09327012
 0.09744348 0.0841419  0.09917552 0.10301573 0.09702142 0.08817508
 0.09841765 0.08901465 0.09082868 0.09067787 0.09536214 0.09402538
 0.08479159 0.10547836 0.08731357 0.0912633  0.09927821 0.10069686
 0.0993779  0.09548959 0.09255721 0.09117659 0.09686919 0.08741395
 0.08956518 0.09135903 0.08548371 0.09477939 0.08661745 0.10543518
 0.08999054 0.09951578 0.0857409  0.09254061 0.10427741 0.08329715
 0.09755842 0.09227203 0.10263403 0.08970967 0.0985293  0.09647092
 0.10624234 0.09548711 0.1087062  0.10408424 0.1000903  0.08868283
 0.08435329 0.10421289 0.10115651 0.08570416 0.09581545 0.0955954
 0.09405165 0.09439196 0.09294491 0.09557633 0.09027319 0.09580056
 0.09767743 0.0807563  0.09870087 0.08789891 0.08179341 0.09874181
 0.09901202 0.09172938 0.09701672 0.10471869 0.09036447 0.09083847
 0.07913575 0.09405984 0.09946614 0.10240589 0.09172815 0.09278811
 0.10824233 0.09340167 0.09629897 0.08181608 0.09402726 0.08378493
 0.10351846 0.09588245 0.10000443 0.09176087 0.09840271 0.0994854
 0.09574022 0.08708052 0.09189183 0.09841597 0.09096286 0.09321732
 0.1071457  0.10102291 0.09427023 0.09229142 0.08522152 0.09101176
 0.08938823 0.09985124 0.09396405 0.09216627 0.09588709 0.09121422
 0.09091001 0.09838549 0.09177097 0.07851249 0.10326735 0.0888129
 0.09044099 0.09320738 0.10009295 0.08540791 0.0839124  0.07820273
 0.1056745  0.09366579 0.09090004 0.09853496 0.09533762 0.10313053
 0.08946717 0.08964991 0.09661448 0.0917474  0.0919478  0.10916955
 0.08885333 0.09362905 0.09912171 0.07897398 0.10480903 0.08528544
 0.10158803 0.09263189 0.08565843 0.09079353 0.09083724 0.09843759
 0.09670929 0.09750194 0.08871836 0.09636932 0.09008196 0.09652068
 0.10236409 0.09104831]
laluna
BERT
[0.7843637  0.7836835  0.7850267  0.7826421  0.7835056  0.786863
 0.7819801  0.78525543 0.7819096  0.78467274 0.7840696  0.784059
 0.78300935 0.78235304 0.7861048  0.78106356 0.78194445 0.7833444
 0.781207   0.78434676 0.7814698  0.7843866  0.7850907  0.78253776
 0.7879727  0.7841045  0.7824468  0.78401375 0.782744   0.7846737
 0.7803387  0.7839025  0.78315526 0.78232676 0.7835142  0.7838153
 0.7821837  0.78462446 0.78374213 0.78537935 0.78741235 0.78451306
 0.7839826  0.7836845  0.783063   0.7843476  0.78180593 0.78350794
 0.7840126  0.78122085 0.7856336  0.784267   0.78625375 0.7818094
 0.78441745 0.7827344  0.78407294 0.78350866 0.78345704 0.7837375
 0.7830168  0.7865368  0.78216535 0.7856096  0.7847976  0.7850708
 0.7851273  0.784786   0.78256714 0.7821884  0.7850637  0.7827226
 0.7836759  0.78260696 0.78172225 0.78263724 0.78238237 0.7858509
 0.7803837  0.78435284 0.7832289  0.78597885 0.7883003  0.78367925
 0.78481346 0.7865528  0.78089505 0.78226435 0.78287685 0.78641033
 0.78365785 0.7849396  0.7852891  0.7828327  0.7840376  0.782614
 0.7816555  0.7819425  0.78405344 0.78183454 0.7837493  0.78606826
 0.7837226  0.7845103  0.7832151  0.78398955 0.7837083  0.7831278
 0.7836624  0.78202206 0.7899891  0.78429216 0.78361875 0.78660727
 0.78476965 0.7841035  0.78445536 0.7868453  0.7814899  0.78299004
 0.7830881  0.78526723 0.78336614 0.78516227 0.7849079  0.78382677
 0.7878405  0.784991   0.78253675 0.78132117 0.7820998  0.7830925
 0.78611845 0.7831045  0.7845238  0.7834135  0.7839482  0.7870652
 0.7846353  0.7801246  0.7851802  0.78383505 0.78679895 0.7821045
 0.78698343 0.7865958  0.7834985  0.7859364  0.7819443  0.78197193
 0.7821106  0.7851874  0.7845009  0.78489256 0.7835449  0.78345484
 0.78167003 0.78635204 0.78212494 0.7823162  0.7851049  0.78157896
 0.784513   0.7836022  0.78352934 0.78297246 0.7828201  0.782862
 0.7844091  0.785372   0.7825605  0.7863303  0.784423   0.7857893
 0.782104   0.78656036 0.780801   0.78224576 0.7855698  0.7848008
 0.7834181  0.781642   0.7829032  0.782293   0.7861883  0.78493774
 0.78443193 0.7830468  0.7809893  0.78516567 0.7837767  0.78388923
 0.78366816 0.784452   0.78029794 0.78539485 0.78295827 0.7874476
 0.7859618  0.78287286]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
attend-M
WER
[0.04829171 0.05083734 0.05571696 0.05649987 0.04663935 0.05552853
 0.04282061 0.0551744  0.0539341  0.04998504 0.0500218  0.0511144
 0.04963057 0.05217533 0.05365908 0.05407389 0.05622413 0.04263816
 0.04805698 0.05425138 0.04585438 0.04976929 0.0537797  0.04259296
 0.04209602 0.05132404 0.04659481 0.05481305 0.04981463 0.04366443
 0.04863967 0.05386256 0.04806613 0.05724207 0.05110772 0.05486271
 0.04719515 0.05238314 0.05506271 0.04804945 0.05529905 0.05097215
 0.0511632  0.05312809 0.05091481 0.05042658 0.04888229 0.05642402
 0.05054136 0.04709987 0.04648305 0.04994575 0.04511363 0.04597131
 0.05087496 0.04707986 0.04414465 0.05730282 0.0524699  0.05600621
 0.04891143 0.05566594 0.04825382 0.05211383 0.04554305 0.05533006
 0.05098846 0.05607865 0.04950058 0.0509559  0.05494087 0.04583881
 0.04531296 0.04626657 0.0532634  0.05369455 0.04836754 0.05242135
 0.05252552 0.0451786  0.04475399 0.05254498 0.0501328  0.04960832
 0.04706175 0.04815294 0.05374864 0.05070841 0.05538922 0.0491175
 0.04079778 0.05582011 0.05053992 0.05067688 0.04330594 0.04067925
 0.05459744 0.04953033 0.05068741 0.04416646 0.05261283 0.0538531
 0.04836315 0.05065379 0.04714401 0.04972885 0.0450576  0.04672629
 0.0492524  0.04714727 0.05149329 0.04755158 0.05599924 0.05543643
 0.04910952 0.04691357 0.04618928 0.04731262 0.04656473 0.05253057
 0.04842778 0.05149811 0.04947944 0.05581539 0.04691804 0.04754969
 0.0472118  0.0563666  0.04824024 0.04801892 0.0448548  0.05183043
 0.05878739 0.04916984 0.04871936 0.04598495 0.04527723 0.0518116
 0.04562781 0.04909221 0.05495919 0.04938484 0.05413451 0.05141567
 0.04708877 0.04546267 0.05255431 0.05281064 0.05092551 0.04283565
 0.04579946 0.05272067 0.05074481 0.05549412 0.05651409 0.04355286
 0.04509307 0.05000779 0.05384683 0.05114075 0.05463407 0.05331462
 0.04580093 0.05209918 0.04866086 0.04453171 0.04961469 0.05414962
 0.05126631 0.0510149  0.05194875 0.05027931 0.0518576  0.04708514
 0.04278807 0.04453331 0.04926174 0.04872485 0.0482501  0.04573175
 0.04621753 0.04915863 0.04671817 0.05417416 0.05084686 0.04916134
 0.0524849  0.04748864 0.05478493 0.05079628 0.05295124 0.04545704
 0.05149097 0.04480102 0.04707356 0.05097999 0.05092797 0.05370445
 0.05154728 0.04859807]
attend-M
BLEU
[0.18741476 0.1961821  0.18934527 0.20844588 0.19925    0.19863311
 0.19649548 0.21256329 0.19667547 0.18136168 0.19121953 0.18991807
 0.21691045 0.20492511 0.20104815 0.19406855 0.21213412 0.18575886
 0.19972165 0.20540358 0.18557323 0.1963295  0.19999143 0.19024053
 0.17604095 0.20298642 0.18498746 0.2057829  0.20279884 0.18979666
 0.17922553 0.18662216 0.18769101 0.20121517 0.19774896 0.20535416
 0.19961717 0.19550045 0.20981266 0.18364371 0.19958019 0.18037684
 0.19579456 0.20726913 0.20844488 0.18855559 0.18665449 0.19628705
 0.19348036 0.18511488 0.19991814 0.20173354 0.19511649 0.1901156
 0.20274676 0.19361453 0.18997357 0.20873704 0.1955497  0.20468841
 0.19417347 0.19038049 0.19487711 0.18790405 0.19134044 0.20853456
 0.19520603 0.19307901 0.19307411 0.18801291 0.20314703 0.18848024
 0.18504844 0.17952884 0.20089752 0.20294649 0.1823948  0.20831918
 0.19464657 0.19039125 0.1934816  0.20822391 0.1924335  0.18029905
 0.1782156  0.18254675 0.18946935 0.19325237 0.200255   0.19210422
 0.17740668 0.19221387 0.18274982 0.19555726 0.19905706 0.18922216
 0.19567413 0.20430894 0.19077396 0.18673339 0.19414271 0.19510903
 0.19545941 0.19434388 0.18560661 0.19388135 0.18999346 0.19975069
 0.19981895 0.19651524 0.1860218  0.19723245 0.19705272 0.21110275
 0.20316992 0.19013819 0.18740372 0.1920251  0.18995886 0.1944738
 0.1995558  0.20216713 0.18946883 0.20236879 0.17856183 0.19118816
 0.18880472 0.20609616 0.1903965  0.19779238 0.18745579 0.20176947
 0.20633555 0.19082434 0.18860751 0.18283802 0.18248557 0.19242229
 0.18963148 0.19508642 0.21934849 0.19324112 0.20399213 0.18813318
 0.18228228 0.17156993 0.19672601 0.19941407 0.18737462 0.20149734
 0.185455   0.20380972 0.1910314  0.19597797 0.19925601 0.18762128
 0.19268286 0.19176733 0.19083048 0.19970421 0.20606027 0.19940148
 0.17221327 0.19955639 0.20424515 0.20144346 0.1894239  0.20543536
 0.19697216 0.19553514 0.1922451  0.19805597 0.1944462  0.18539612
 0.17579407 0.18663582 0.19425279 0.18682975 0.17581136 0.1923108
 0.20027399 0.19205391 0.19033289 0.2068713  0.18788886 0.19065145
 0.20176141 0.1944645  0.19780891 0.19520354 0.20048569 0.18746268
 0.19047421 0.19904003 0.20078534 0.18996585 0.20020635 0.19453498
 0.19730795 0.18720319]
attend-M
METEOR
[0.13406593 0.14023561 0.13384465 0.14761425 0.13798052 0.14459483
 0.13653985 0.14937678 0.14033761 0.13179587 0.13741783 0.13558321
 0.1474894  0.14391346 0.1330729  0.14131135 0.14288769 0.13555629
 0.14376816 0.14809138 0.13180716 0.13723363 0.13762831 0.13878253
 0.12881618 0.14342862 0.12777812 0.14444495 0.14018859 0.13688885
 0.12436276 0.12691427 0.13133812 0.14161802 0.13621555 0.14373997
 0.14295054 0.13755542 0.14102619 0.12807018 0.13950455 0.13300734
 0.13525842 0.14741231 0.14060617 0.12944276 0.13704526 0.13632194
 0.13448049 0.13791705 0.14315693 0.13933662 0.13337994 0.13294825
 0.13885507 0.13713607 0.133047   0.14690616 0.14011144 0.14043981
 0.13680932 0.1369194  0.14010385 0.13514196 0.13202694 0.14480861
 0.13753638 0.13845484 0.13719915 0.13122272 0.14753326 0.13567811
 0.13086794 0.12563263 0.14158478 0.14048681 0.13147825 0.14022893
 0.14031855 0.13036869 0.13432666 0.14006925 0.1399528  0.12966054
 0.12673369 0.13122313 0.13223584 0.13409223 0.14410583 0.14487286
 0.12329424 0.13723951 0.13014621 0.14135592 0.14171239 0.13276655
 0.13491038 0.13963669 0.1274517  0.13549501 0.13784543 0.13336103
 0.14032471 0.13673049 0.13404868 0.13322655 0.12994065 0.14090264
 0.14363308 0.13872465 0.12939083 0.13575446 0.13729575 0.14593876
 0.14187173 0.13525922 0.13382809 0.13577325 0.13371465 0.13870763
 0.14062737 0.13220283 0.13458185 0.14362773 0.12661056 0.13763524
 0.13442529 0.14444467 0.13784018 0.14147654 0.13282157 0.13751432
 0.15102611 0.13193056 0.13849027 0.13251577 0.12536498 0.13290132
 0.13574366 0.13470214 0.14597278 0.13764838 0.13996541 0.14033042
 0.13391408 0.12406468 0.13777661 0.13995634 0.14208261 0.13713538
 0.12885577 0.14322435 0.13440264 0.139207   0.14471248 0.13026903
 0.14166258 0.14159072 0.13958584 0.1357585  0.14626632 0.14223075
 0.12778767 0.13980618 0.14974344 0.14272141 0.13998219 0.14051843
 0.14038066 0.13457222 0.13352986 0.13872308 0.14227048 0.13231272
 0.12740169 0.13445578 0.13253759 0.1344014  0.1277234  0.14075139
 0.13691324 0.13176449 0.13039218 0.14615909 0.1396252  0.13364578
 0.14684479 0.14033334 0.14391144 0.13849466 0.14100674 0.13398635
 0.13204261 0.14116237 0.13725595 0.13771018 0.13556722 0.14115465
 0.141466   0.13576826]
attend-M
BERT
[0.7886346  0.7866221  0.7894992  0.7890354  0.7877727  0.79029775
 0.78907984 0.78817964 0.78969    0.78801155 0.78847206 0.78807884
 0.79178447 0.7874124  0.7876857  0.788273   0.7913298  0.7893655
 0.7898421  0.7906428  0.78645706 0.7914613  0.78828907 0.78645873
 0.7865251  0.79033667 0.7867146  0.7898295  0.7899168  0.78986156
 0.78468144 0.78788996 0.79030764 0.7915921  0.7867116  0.7908863
 0.78898555 0.7880768  0.7897788  0.78718513 0.790259   0.7873813
 0.78736085 0.7893656  0.78828704 0.7859335  0.7887868  0.7875755
 0.78994143 0.7895377  0.79099655 0.7893446  0.78843415 0.78991747
 0.78899837 0.78802866 0.7887115  0.7888916  0.791286   0.7894442
 0.79009485 0.78755456 0.78838927 0.7901835  0.7865342  0.79106414
 0.7884136  0.7912711  0.7884695  0.7897945  0.7906824  0.7871733
 0.7897511  0.78906924 0.7893972  0.790414   0.7893752  0.7891833
 0.79176086 0.7887771  0.78736347 0.79039335 0.78944707 0.7867993
 0.787941   0.78830695 0.7869526  0.78981215 0.79030925 0.7912269
 0.7883354  0.78803617 0.78700274 0.7895077  0.7882369  0.7901501
 0.7882415  0.7900936  0.7889951  0.7876231  0.7896211  0.7921657
 0.7886584  0.78938085 0.78796005 0.78749996 0.7873953  0.78746486
 0.7866976  0.78881186 0.7896348  0.7887224  0.79034436 0.78994507
 0.78649205 0.7878115  0.79155767 0.7878406  0.78654164 0.7886339
 0.79064876 0.7891647  0.7886306  0.78744227 0.78829443 0.78772193
 0.78944176 0.7893431  0.7898016  0.79030913 0.7896168  0.78937525
 0.7900243  0.7875056  0.78783995 0.7874215  0.7883345  0.7905658
 0.78798306 0.7884005  0.7886118  0.7894526  0.7882955  0.788779
 0.78891337 0.78751737 0.7884886  0.7889819  0.7884528  0.7876744
 0.7882061  0.78835064 0.7883565  0.7884517  0.78861064 0.7887932
 0.78723854 0.79374325 0.7893122  0.79036593 0.790678   0.7895315
 0.7867938  0.7925112  0.7884069  0.7916032  0.7889958  0.789905
 0.7902844  0.7880077  0.78752774 0.7877882  0.7909379  0.7858492
 0.7875966  0.7889991  0.78752905 0.7864867  0.7869882  0.7884236
 0.7911885  0.788489   0.78935504 0.79091614 0.78766185 0.78795785
 0.7887108  0.78907704 0.7888123  0.7888754  0.79102117 0.7873973
 0.7887674  0.79038966 0.78772664 0.78842753 0.78774774 0.78871566
 0.78758836 0.7906422 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
attend-F
WER
[-0.1920268  -0.19046145 -0.19370678 -0.16901594 -0.19153176 -0.19214297
 -0.20803677 -0.19118998 -0.19043121 -0.18241432 -0.1932591  -0.19448093
 -0.18767253 -0.18858825 -0.17976169 -0.18760484 -0.18439389 -0.18825142
 -0.1984525  -0.18968651 -0.18510214 -0.18657977 -0.17745821 -0.20273152
 -0.19402535 -0.19126215 -0.1756741  -0.18679194 -0.18959109 -0.18364104
 -0.19195809 -0.18851157 -0.18992633 -0.19026121 -0.18721154 -0.19496347
 -0.17805271 -0.19392976 -0.18854925 -0.1975505  -0.18684088 -0.19216353
 -0.1851617  -0.19563526 -0.18314858 -0.19081533 -0.1735139  -0.19200117
 -0.20265321 -0.19663989 -0.19092709 -0.18531287 -0.17966077 -0.19303003
 -0.19694744 -0.19189386 -0.18713823 -0.18659646 -0.18221684 -0.1933257
 -0.18157968 -0.19663031 -0.18846618 -0.19479509 -0.19637166 -0.20039026
 -0.18530596 -0.19123653 -0.19112756 -0.17756185 -0.18824038 -0.18659497
 -0.18147925 -0.19270519 -0.18652256 -0.18505207 -0.19063128 -0.18014787
 -0.18971781 -0.18765948 -0.17552507 -0.19444098 -0.17864118 -0.18943657
 -0.17666158 -0.19091329 -0.18812653 -0.19611295 -0.18401357 -0.1972879
 -0.188763   -0.18456941 -0.19065916 -0.18335467 -0.18602965 -0.19094202
 -0.19010252 -0.18324767 -0.18830656 -0.1995113  -0.19502318 -0.17508701
 -0.18232537 -0.19012043 -0.18981688 -0.18572184 -0.19353333 -0.19888382
 -0.20024871 -0.17511566 -0.18706836 -0.19691773 -0.18460605 -0.18204685
 -0.1864643  -0.18231255 -0.18892284 -0.18084827 -0.18711881 -0.19508088
 -0.185895   -0.19404931 -0.17733554 -0.1924635  -0.19964007 -0.1949872
 -0.18560435 -0.19259215 -0.17671847 -0.18969071 -0.1943131  -0.18484477
 -0.18870025 -0.18321669 -0.19103404 -0.18178111 -0.18984244 -0.19809072
 -0.18910137 -0.18892492 -0.17937837 -0.1951604  -0.18462528 -0.18045155
 -0.18160378 -0.19201335 -0.1951968  -0.1836567  -0.19407717 -0.18985462
 -0.19387284 -0.19230201 -0.19176665 -0.18263377 -0.18499144 -0.19061008
 -0.19715336 -0.18205754 -0.19192246 -0.19418965 -0.19577701 -0.18391855
 -0.18462053 -0.19909773 -0.20004911 -0.20374298 -0.18864345 -0.18522933
 -0.19675165 -0.19414512 -0.18534086 -0.17625725 -0.18957787 -0.18975635
 -0.18335609 -0.19057148 -0.19429812 -0.18761691 -0.18589984 -0.18915255
 -0.17386115 -0.18787442 -0.18795056 -0.18379762 -0.18786088 -0.18585947
 -0.18816829 -0.18113417 -0.18724292 -0.19253127 -0.19097478 -0.1920829
 -0.17323209 -0.19683451 -0.1835248  -0.19241046 -0.18304894 -0.18874636
 -0.19685908 -0.19172358]
attend-F
BLEU
[0.17468822 0.18118848 0.17676108 0.2013457  0.17125712 0.15697302
 0.16097477 0.16966191 0.16802972 0.20322387 0.17224715 0.17038748
 0.19780578 0.17706012 0.19529837 0.18212483 0.18538237 0.18904963
 0.16945088 0.17703512 0.19456551 0.1998555  0.19961356 0.17565706
 0.17453217 0.18005643 0.19762211 0.18312496 0.1771529  0.19694381
 0.16814769 0.19016918 0.17187213 0.18587427 0.18836615 0.1768236
 0.18993291 0.16569114 0.18266499 0.17231612 0.18456438 0.17255496
 0.18393952 0.18178759 0.20672401 0.19576374 0.19764731 0.18146792
 0.14482042 0.15972878 0.17867232 0.18889855 0.19456852 0.17754564
 0.17616651 0.18011173 0.21161033 0.18866845 0.18963314 0.17027475
 0.18751563 0.17260917 0.18685626 0.16161562 0.17886289 0.1756306
 0.18310772 0.20881117 0.19511978 0.19215435 0.20219566 0.18958238
 0.19918255 0.16591039 0.20405365 0.20024648 0.18960428 0.19011486
 0.17928508 0.17858246 0.207075   0.17289777 0.19428313 0.18567315
 0.19461571 0.17790835 0.17492953 0.1851199  0.18921253 0.17009421
 0.19247605 0.19031528 0.17590999 0.19589801 0.20413605 0.17917919
 0.17788449 0.18660084 0.19227702 0.17152087 0.16211079 0.20517162
 0.18615577 0.18263457 0.17157521 0.18375448 0.17817957 0.16203764
 0.17229204 0.2086578  0.18187846 0.1741265  0.18562741 0.19296288
 0.19739377 0.20643617 0.20075384 0.19720421 0.17374002 0.16138924
 0.19234128 0.18828973 0.1959043  0.19533966 0.1779465  0.16914855
 0.18742266 0.19472648 0.20685308 0.19175169 0.17553663 0.1930018
 0.17395175 0.18648591 0.18270527 0.18846174 0.19057611 0.1632815
 0.17166539 0.18715943 0.18541764 0.16849295 0.18560543 0.19198806
 0.18541556 0.16726552 0.16811614 0.20231698 0.17543503 0.17390032
 0.16750983 0.18050032 0.18595566 0.20260176 0.19166841 0.17383232
 0.1732922  0.1989025  0.18021362 0.17910381 0.17709206 0.18616137
 0.19632364 0.17148207 0.1649182  0.17354771 0.17853705 0.18736784
 0.17714498 0.18016746 0.18806415 0.20554381 0.19069293 0.19461276
 0.1808421  0.1623027  0.17381516 0.18171981 0.1840608  0.17982697
 0.20534194 0.19409996 0.19022038 0.19679659 0.18049172 0.18913359
 0.1683869  0.19564988 0.18363822 0.18606967 0.16947561 0.19120777
 0.1992527  0.16450076 0.19640758 0.1755492  0.1909072  0.1881676
 0.16752082 0.18871755]
attend-F
METEOR
[0.12751509 0.13335943 0.13644936 0.15027441 0.12185226 0.12033861
 0.12029654 0.12332727 0.12808387 0.14733565 0.13014615 0.12389588
 0.14607017 0.13465113 0.14522824 0.1367081  0.13978741 0.13908227
 0.12742891 0.12461079 0.14951467 0.14665239 0.13968135 0.13287899
 0.12814858 0.1383831  0.15021681 0.13568152 0.13632491 0.14732241
 0.12163432 0.13850981 0.12335385 0.14834944 0.13792679 0.13270882
 0.13964071 0.12698533 0.13458582 0.12715608 0.14164043 0.13164281
 0.1359807  0.13561481 0.14822475 0.14667147 0.14678603 0.13397912
 0.11660141 0.11878341 0.13199322 0.14187468 0.14243244 0.12800553
 0.12981736 0.13732605 0.15934324 0.14473945 0.14121543 0.13146336
 0.13993751 0.13130635 0.13189425 0.11841918 0.13527863 0.12852124
 0.13733555 0.15024103 0.13987872 0.14428869 0.14911353 0.13180957
 0.15032438 0.12143397 0.15106391 0.15310045 0.14381619 0.14010734
 0.13330758 0.13172889 0.1553694  0.13361686 0.1437259  0.13993054
 0.13948872 0.1318892  0.12985558 0.14214562 0.14099612 0.12725049
 0.14284656 0.14163699 0.13430771 0.14099568 0.15160293 0.132901
 0.13378264 0.13635962 0.14216163 0.12807075 0.12224498 0.15257127
 0.14255241 0.13853055 0.12831072 0.14348326 0.12932006 0.12522428
 0.13123455 0.15761307 0.13684643 0.13449725 0.13960504 0.14263662
 0.14418982 0.14879163 0.1538969  0.14477957 0.12859686 0.12074632
 0.14631499 0.13639121 0.14173616 0.1416751  0.13131012 0.13272343
 0.14070187 0.14314692 0.14725887 0.14383142 0.13306065 0.14680058
 0.13101004 0.14009676 0.1413799  0.14044427 0.13369399 0.12371568
 0.13307372 0.13939834 0.13852976 0.12069646 0.1376287  0.14430669
 0.14213189 0.12869207 0.12730753 0.14481298 0.12783357 0.12631681
 0.12759047 0.14179392 0.14247097 0.15098131 0.14487583 0.1358503
 0.13049715 0.14603372 0.13017671 0.1343762  0.13222313 0.14582766
 0.14316804 0.12996198 0.12396417 0.13039278 0.13123428 0.13955215
 0.12543005 0.13269766 0.14454141 0.15677186 0.13698781 0.13819279
 0.13216219 0.1211217  0.12841965 0.13577842 0.13738838 0.13116811
 0.15147672 0.14294107 0.14327683 0.14766827 0.14165819 0.13775802
 0.12623721 0.14446369 0.1381051  0.13738961 0.12685947 0.14185375
 0.14773301 0.12346175 0.14906577 0.13211355 0.14440444 0.13495115
 0.12874176 0.14035095]
attend-F
BERT
[0.78978026 0.7891228  0.78726333 0.7909066  0.7881304  0.7865915
 0.78800386 0.78573453 0.7895765  0.7913784  0.7899145  0.7852174
 0.7904079  0.7902642  0.78818256 0.7904261  0.7858131  0.7887989
 0.78713405 0.78640485 0.7898583  0.78910685 0.7904157  0.78652966
 0.788249   0.7881532  0.7905563  0.78759766 0.78652316 0.7908463
 0.7885865  0.7885066  0.7867106  0.791232   0.79066867 0.78692263
 0.79090434 0.7887802  0.7894528  0.7852659  0.7880402  0.78796476
 0.78832453 0.78791976 0.78866774 0.7876622  0.7875897  0.7881072
 0.7858632  0.7864314  0.7895829  0.78883547 0.79177517 0.7891982
 0.78778446 0.79229724 0.79281706 0.79089874 0.7876316  0.7894646
 0.78744227 0.78769165 0.7883077  0.78814995 0.7890516  0.78814346
 0.7889603  0.7899259  0.7898741  0.7911436  0.7886452  0.7882303
 0.79104626 0.78938675 0.7897981  0.79036826 0.78794456 0.7905233
 0.78807765 0.7894383  0.7912399  0.78804725 0.7889568  0.7889284
 0.7897836  0.7895953  0.7897192  0.7909855  0.7879203  0.78840864
 0.7893726  0.78939384 0.7878698  0.79036593 0.7898814  0.7905403
 0.7894355  0.7884385  0.79138386 0.78552854 0.7877871  0.79068947
 0.7901995  0.7898854  0.7880124  0.78990287 0.7881102  0.7872076
 0.78800976 0.7909823  0.78943205 0.7877589  0.78941196 0.7923183
 0.78957653 0.79112    0.7900363  0.7902018  0.78677124 0.78716326
 0.79079103 0.78985053 0.7895723  0.78748614 0.7896332  0.78565985
 0.7912039  0.78810275 0.7900552  0.79045296 0.78735137 0.7886067
 0.7903594  0.7879048  0.7885971  0.79041207 0.79003996 0.7881474
 0.7888286  0.7886413  0.78828496 0.7881441  0.7905666  0.7896521
 0.79029655 0.7898284  0.78782797 0.7894857  0.78802603 0.7885279
 0.7847587  0.790051   0.7891071  0.78684664 0.78777003 0.7880895
 0.78785884 0.7891464  0.78729135 0.78868985 0.78753686 0.7904755
 0.78827965 0.78814757 0.784409   0.78773946 0.78780895 0.787429
 0.7865697  0.7884414  0.78992105 0.78924704 0.791187   0.79012024
 0.78914565 0.7867526  0.7872449  0.7881592  0.7885163  0.7880811
 0.7903585  0.78869987 0.78944856 0.7901002  0.79061174 0.78817517
 0.78826517 0.79123586 0.7882175  0.7886673  0.7884526  0.79037607
 0.78981364 0.78741854 0.78814864 0.7884351  0.78947693 0.79070693
 0.78859186 0.78928894]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
alpha_repeat-1
WER
[-0.2538407  -0.20949029 -0.21828961 -0.25335059 -0.21383573 -0.22075156
 -0.23302887 -0.26199764 -0.2403304  -0.21108253 -0.2435099  -0.23333498
 -0.24166461 -0.24851243 -0.2423996  -0.25218173 -0.23232729 -0.23976512
 -0.23427567 -0.21594605 -0.22637731 -0.23550799 -0.27562849 -0.2382015
 -0.2485942  -0.24276883 -0.21405906 -0.22032389 -0.22974911 -0.20142858
 -0.19053846 -0.22822832 -0.23757753 -0.22109347 -0.21741054 -0.2154079
 -0.22706607 -0.22692981 -0.22655763 -0.21441549 -0.23272072 -0.21911819
 -0.2110131  -0.21605312 -0.23918724 -0.2171672  -0.25486389 -0.19732685
 -0.24410748 -0.22905225 -0.27700474 -0.22890173 -0.25049585 -0.21717857
 -0.24584474 -0.24508218 -0.22190201 -0.21507754 -0.21891612 -0.24488289
 -0.22698043 -0.23066356 -0.21148891 -0.2171151  -0.23949587 -0.2546368
 -0.23866782 -0.24813642 -0.23078397 -0.23065676 -0.240212   -0.23374009
 -0.24492172 -0.26316103 -0.2375693  -0.25922867 -0.20336428 -0.21241102
 -0.24607387 -0.23687792 -0.26369726 -0.28218867 -0.247358   -0.21854857
 -0.24574093 -0.24564399 -0.25088137 -0.21834196 -0.22730708 -0.209508
 -0.2120462  -0.25083871 -0.21804686 -0.22847088 -0.22132593 -0.22780612
 -0.24783133 -0.23397146 -0.24036061 -0.21927601 -0.22252892 -0.22358877
 -0.20013206 -0.24899524 -0.24831867 -0.2172099  -0.22747596 -0.26668756
 -0.25647305 -0.22958962 -0.25215899 -0.20034027 -0.25796825 -0.22567087
 -0.23427452 -0.23580604 -0.25271332 -0.20684696 -0.2051465  -0.24944084
 -0.24087788 -0.2531966  -0.21531955 -0.21053019 -0.22078124 -0.24032997
 -0.19997879 -0.21983696 -0.23508657 -0.24341665 -0.2390512  -0.22410173
 -0.23213526 -0.2203043  -0.21939791 -0.22689062 -0.2295828  -0.23057965
 -0.19486172 -0.22062457 -0.23336786 -0.22129658 -0.26155857 -0.2284539
 -0.1979959  -0.23336674 -0.20018579 -0.25730809 -0.21520293 -0.22351428
 -0.22605262 -0.22879801 -0.24567359 -0.24739305 -0.25286519 -0.28045427
 -0.25980686 -0.23416928 -0.2282234  -0.22449831 -0.23130038 -0.2421622
 -0.26214926 -0.25577936 -0.25716963 -0.25250134 -0.21514956 -0.22493994
 -0.26439197 -0.23308682 -0.24079643 -0.22589559 -0.23791718 -0.22911273
 -0.24454076 -0.24035013 -0.22526966 -0.22667782 -0.22678988 -0.25085203
 -0.23678014 -0.24896089 -0.21803598 -0.22200765 -0.26819898 -0.24621318
 -0.22887816 -0.24756021 -0.24114184 -0.2208439  -0.24309171 -0.23386383
 -0.25110334 -0.22956776 -0.26539342 -0.25359788 -0.25279059 -0.23037398
 -0.2452841  -0.21643789]
alpha_repeat-1
BLEU
[0.16417417 0.19645486 0.21666844 0.12693312 0.21960744 0.20312591
 0.18696895 0.13786661 0.19551508 0.19125797 0.18078197 0.17474088
 0.15653365 0.15454351 0.18768321 0.17790621 0.18713952 0.20564719
 0.16634772 0.18486429 0.18339887 0.18268519 0.11516192 0.17130597
 0.17175847 0.16454517 0.21848486 0.20730967 0.1969364  0.20727399
 0.20690097 0.16311609 0.17269087 0.16973026 0.17166536 0.19069093
 0.17207073 0.17607274 0.20156781 0.1885015  0.17464493 0.21906706
 0.19213698 0.19394771 0.15201937 0.22606938 0.14656397 0.20387753
 0.17876969 0.18142386 0.09942143 0.16898395 0.16062303 0.16957104
 0.16787421 0.16243848 0.1744396  0.19872498 0.21351374 0.14491568
 0.21177481 0.16153616 0.22721363 0.23095183 0.1554616  0.15669357
 0.17304321 0.16183534 0.14735565 0.17399345 0.13755107 0.19877446
 0.160238   0.16761702 0.20215124 0.13085551 0.19944191 0.19004923
 0.19514589 0.19128337 0.1362493  0.12344635 0.15823313 0.20134522
 0.1408688  0.19231507 0.14558716 0.20014123 0.21968089 0.1899079
 0.17479879 0.16330212 0.21289607 0.18630808 0.18918509 0.22070959
 0.16017277 0.16319047 0.18612199 0.17390939 0.2015059  0.19985532
 0.22591008 0.15088835 0.16746822 0.17466563 0.1733881  0.13038698
 0.15315099 0.19398921 0.12386689 0.22330071 0.14130336 0.17036515
 0.20764112 0.16966561 0.17034593 0.2250168  0.20911913 0.12147609
 0.17644665 0.15738988 0.16930426 0.20491606 0.22441244 0.17799385
 0.19639257 0.19764496 0.16186624 0.16363909 0.19166764 0.21021217
 0.1712749  0.18325437 0.21684437 0.21092164 0.18530898 0.16758479
 0.24622914 0.22535322 0.163926   0.21820215 0.17228818 0.21614001
 0.24133159 0.18074319 0.19960485 0.15729712 0.20382145 0.23019093
 0.15141909 0.1865834  0.14322254 0.17037067 0.16528446 0.09050613
 0.15348289 0.19003064 0.19135302 0.18091421 0.19292796 0.14301034
 0.14689334 0.18315737 0.17416385 0.17729548 0.17709074 0.19719899
 0.16374924 0.19002086 0.20193195 0.20843129 0.169549   0.21421572
 0.13227097 0.17975631 0.17258998 0.18280806 0.14740114 0.15109307
 0.18932845 0.18396663 0.17987524 0.18429609 0.13446004 0.15060713
 0.19657526 0.16441755 0.14913102 0.18240206 0.14283057 0.17284046
 0.13943777 0.20873521 0.13086316 0.16926784 0.15567379 0.18837295
 0.19954711 0.21309798]
alpha_repeat-1
METEOR
[0.14203536 0.14211701 0.16640108 0.10242792 0.17475811 0.14527998
 0.16146342 0.11085658 0.15262263 0.1418792  0.13334679 0.1399055
 0.11085275 0.12689917 0.15114522 0.13517167 0.15895196 0.15025524
 0.13236296 0.14458608 0.13057018 0.13780533 0.08853303 0.15835491
 0.12975    0.14145096 0.17024169 0.16741715 0.14822609 0.16095461
 0.1714442  0.12287206 0.12895873 0.12897254 0.12876703 0.14909439
 0.15470603 0.14926037 0.14851979 0.15236319 0.14474506 0.17107526
 0.16325068 0.15664839 0.10830532 0.1770938  0.1265407  0.16672363
 0.14405599 0.14425232 0.0733041  0.13538222 0.12440396 0.13666569
 0.1522227  0.15437367 0.14527659 0.1557072  0.17099341 0.14025807
 0.18339956 0.14698352 0.18677317 0.19065588 0.12999461 0.14399794
 0.13102553 0.14464945 0.12576926 0.13532265 0.10206717 0.14745392
 0.13043519 0.14441418 0.17306912 0.12887238 0.14746618 0.14657123
 0.14802555 0.13003679 0.13903845 0.10255528 0.12394741 0.16435999
 0.11604126 0.1414161  0.11700916 0.15148881 0.16456812 0.14634936
 0.12805554 0.12410838 0.17003048 0.15497295 0.15867076 0.17918379
 0.13833726 0.13589016 0.13911413 0.13071383 0.1571999  0.16183122
 0.17543265 0.11975836 0.12900914 0.13636666 0.14166546 0.10031416
 0.13611904 0.15042251 0.1020982  0.18028479 0.1315898  0.15685679
 0.14918451 0.14262504 0.15000669 0.16001275 0.16185406 0.10333943
 0.13654433 0.14098345 0.13416565 0.15540737 0.16285066 0.14250672
 0.14513402 0.16756918 0.12802863 0.1240303  0.15681372 0.16179181
 0.12813715 0.15442023 0.15427914 0.15860142 0.12899658 0.13320341
 0.1915048  0.1697397  0.12291867 0.15770704 0.12681106 0.16497508
 0.18180916 0.13516535 0.162909   0.12948012 0.16382957 0.17721507
 0.11056719 0.13329575 0.11436215 0.13880209 0.12045709 0.08934204
 0.12073316 0.15419079 0.15568692 0.14170877 0.1413112  0.12527434
 0.11876097 0.14477749 0.15988239 0.14477955 0.13513403 0.14931768
 0.12064523 0.13786409 0.13822221 0.16262567 0.14009869 0.16320885
 0.1149373  0.13901297 0.12478234 0.14338461 0.10291841 0.12353129
 0.13519599 0.17151496 0.14160763 0.1285285  0.10929433 0.12178659
 0.1404759  0.12552327 0.10583253 0.14370897 0.13293098 0.14871604
 0.12544692 0.17268811 0.09837749 0.14812047 0.12458508 0.15909185
 0.14772043 0.16383978]
alpha_repeat-1
BERT
[0.7897324  0.79308563 0.7939723  0.7945064  0.7961399  0.79440933
 0.7989097  0.7925699  0.79860526 0.7949686  0.7975222  0.7976879
 0.79310095 0.7931378  0.7914822  0.788994   0.7998069  0.79260856
 0.78939223 0.7963003  0.7934733  0.79103684 0.7908977  0.7952133
 0.78847104 0.78937846 0.7983329  0.79433656 0.7954004  0.7953118
 0.79665756 0.79100233 0.79725015 0.793162   0.794111   0.79808843
 0.7940438  0.7983601  0.7975162  0.79813373 0.7952318  0.7963953
 0.79857284 0.79850703 0.7911603  0.7951034  0.7921736  0.796091
 0.7948296  0.7926832  0.7896457  0.7908178  0.7912885  0.79192406
 0.7956031  0.79499125 0.7922547  0.7939937  0.79447687 0.7941893
 0.79092956 0.7947534  0.79504144 0.7983872  0.79029393 0.7952582
 0.7923892  0.7909291  0.7896723  0.79450244 0.7925289  0.7979339
 0.79081655 0.7935793  0.7980833  0.7880377  0.7929339  0.7981267
 0.791801   0.79326975 0.79470295 0.788817   0.7959659  0.7912995
 0.7880528  0.7971701  0.7913167  0.7934481  0.7967441  0.7956347
 0.7970886  0.7907443  0.79861414 0.79795957 0.7931429  0.79647976
 0.79522574 0.79556274 0.7915059  0.7951811  0.79833007 0.79356134
 0.7991673  0.79208475 0.7909291  0.7934829  0.79019564 0.79188985
 0.7956415  0.79820085 0.7903016  0.79310787 0.78926796 0.7943036
 0.79214215 0.80248743 0.79831284 0.7971686  0.7969497  0.78848946
 0.79234034 0.793395   0.79120153 0.7985613  0.79610044 0.79888713
 0.7929868  0.8001284  0.7959281  0.7928565  0.7925123  0.79259825
 0.79356104 0.7959123  0.79246324 0.7955389  0.79317904 0.788728
 0.7962253  0.79284763 0.7954589  0.7963365  0.79625684 0.79255927
 0.8006749  0.7924184  0.7979952  0.7902419  0.79742944 0.79405266
 0.789569   0.79287785 0.79513454 0.79354125 0.7906803  0.79073703
 0.79511374 0.7971064  0.79706043 0.7950131  0.79531    0.78995925
 0.7912651  0.79589695 0.7908111  0.79197663 0.7990272  0.79132026
 0.79267037 0.79150474 0.7946353  0.792856   0.7917855  0.7967901
 0.7908738  0.7951648  0.79239726 0.7946666  0.79045707 0.79525197
 0.79568017 0.79469186 0.795436   0.7955924  0.7917787  0.79354894
 0.7928682  0.7882599  0.7930832  0.7930293  0.79613477 0.80096006
 0.79639703 0.79147154 0.79370266 0.7941606  0.7909304  0.79735684
 0.7975832  0.79858   ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
alpha_repeat-2
WER
[-0.21387784 -0.20857612 -0.22496455 -0.23493604 -0.18931106 -0.20298254
 -0.1920761  -0.20650035 -0.19176177 -0.19692134 -0.20082846 -0.17319419
 -0.19365351 -0.20364689 -0.22828423 -0.18922239 -0.17724251 -0.18949636
 -0.21871203 -0.19296721 -0.21565926 -0.21536447 -0.1681689  -0.16509922
 -0.14333126 -0.1992145  -0.21693801 -0.17325854 -0.21914069 -0.20330597
 -0.23558911 -0.2201756  -0.1686998  -0.18578826 -0.19337683 -0.18065526
 -0.18812683 -0.19762953 -0.16447583 -0.198722   -0.21967175 -0.19612737
 -0.17790436 -0.21901207 -0.19967309 -0.17592782 -0.22336454 -0.20572042
 -0.17829132 -0.18008148 -0.21447404 -0.19594969 -0.22391441 -0.20792157
 -0.19586458 -0.15308877 -0.17535268 -0.19431181 -0.1846825  -0.1829548
 -0.19457941 -0.17723575 -0.19285108 -0.18205299 -0.20282389 -0.17094273
 -0.1902637  -0.20150494 -0.16916667 -0.1852879  -0.19609498 -0.19543613
 -0.16571611 -0.16506051 -0.20273954 -0.18688197 -0.19580254 -0.20836816
 -0.17832814 -0.21141982 -0.16066655 -0.21870502 -0.18898551 -0.17990364
 -0.21022221 -0.20580002 -0.20136564 -0.19351838 -0.18639355 -0.2092844
 -0.19945636 -0.16807119 -0.17466663 -0.20979009 -0.21735005 -0.1954979
 -0.1772638  -0.17900631 -0.21874345 -0.20978494 -0.18679911 -0.23451578
 -0.2022713  -0.17006831 -0.22080773 -0.20589183 -0.20235068 -0.19891627
 -0.20262295 -0.17745519 -0.19577568 -0.21859417 -0.19744802 -0.21392364
 -0.18682966 -0.19489788 -0.21670564 -0.2186567  -0.21798735 -0.14948777
 -0.18452774 -0.2337306  -0.20880153 -0.1909612  -0.20532854 -0.1951397
 -0.16142954 -0.20823873 -0.1853798  -0.23520309 -0.20078424 -0.21639097
 -0.18664867 -0.18035843 -0.17228764 -0.18482847 -0.19081448 -0.20569272
 -0.20793422 -0.2193697  -0.22584758 -0.21061281 -0.2003014  -0.16601351
 -0.1868583  -0.2019244  -0.19979272 -0.20034659 -0.18920427 -0.18551999
 -0.19574872 -0.21847332 -0.17757409 -0.18442937 -0.20390314 -0.20166392
 -0.17686282 -0.16854732 -0.20744181 -0.1887967  -0.18179801 -0.17264394
 -0.20491309 -0.19344992 -0.15958894 -0.17914264 -0.19309965 -0.19517604
 -0.18364915 -0.1804823  -0.19574871 -0.17328708 -0.1954176  -0.15484758
 -0.20396181 -0.17473938 -0.19786128 -0.21289266 -0.22449199 -0.21164753
 -0.18938945 -0.20836467 -0.21047294 -0.20401319 -0.20900797 -0.2028005
 -0.18481632 -0.18045589 -0.20558551 -0.14742725 -0.18874163 -0.21097411
 -0.1744466  -0.18462741 -0.20008463 -0.22221325 -0.18755231 -0.20523971
 -0.17110424 -0.20158041]
alpha_repeat-2
BLEU
[0.20635542 0.1599473  0.12848706 0.1276295  0.18159745 0.21195794
 0.17192469 0.13663422 0.18754348 0.18279358 0.12546877 0.17509669
 0.17516467 0.1789594  0.13450364 0.18661784 0.20456986 0.22833242
 0.15704216 0.16081126 0.14193175 0.15361109 0.23723078 0.21469444
 0.22935362 0.1747363  0.12738799 0.20148255 0.13979925 0.15507478
 0.13300622 0.14270518 0.20342367 0.18821196 0.16006778 0.20577598
 0.16372067 0.2194639  0.22664487 0.20244827 0.12814599 0.18270652
 0.22070612 0.14810737 0.18408608 0.22513171 0.15801613 0.19001831
 0.2286356  0.20951073 0.16658947 0.17466446 0.13513075 0.13170748
 0.16391006 0.24233366 0.23163203 0.18989357 0.18627163 0.22606516
 0.18070382 0.22000978 0.16287906 0.19630708 0.16548572 0.21322251
 0.20253469 0.18757008 0.20118638 0.19347614 0.22210872 0.18980764
 0.22185244 0.22336082 0.14318695 0.20273586 0.17071514 0.16662076
 0.19022256 0.14681731 0.20301424 0.12161081 0.15748755 0.17260824
 0.18888461 0.20435105 0.18199502 0.16636191 0.22184558 0.18062785
 0.19045656 0.24188697 0.17230104 0.17627186 0.169679   0.24315775
 0.18458527 0.19610623 0.18712789 0.16879728 0.19533975 0.12313515
 0.20787606 0.20960797 0.14188822 0.20756797 0.1629072  0.18060093
 0.15495279 0.22655928 0.16308395 0.1476822  0.14123841 0.17590487
 0.19204266 0.16614326 0.09655664 0.12094725 0.18774467 0.23255104
 0.17948748 0.13535993 0.16888545 0.18490187 0.19173624 0.17804544
 0.2520108  0.19177843 0.22238088 0.09541327 0.14980808 0.15161912
 0.17493331 0.17268755 0.21622205 0.22298123 0.19860599 0.20168462
 0.19850077 0.19163361 0.13974653 0.19588229 0.17148592 0.20052503
 0.20143212 0.19779654 0.16877922 0.1837031  0.20451057 0.19491529
 0.19017576 0.17813029 0.21307927 0.20816219 0.19338094 0.17938748
 0.21677969 0.18259837 0.20401425 0.17989364 0.16902952 0.21040412
 0.17463741 0.20610016 0.22002191 0.21783035 0.17049483 0.18525219
 0.20644099 0.21740424 0.19213943 0.17444713 0.17017541 0.23787827
 0.19441945 0.2039442  0.21946086 0.16763569 0.11480536 0.17527125
 0.18238745 0.14416044 0.18088803 0.13746611 0.21661194 0.19791725
 0.2378803  0.20444793 0.17759707 0.220729   0.22066499 0.13820008
 0.22308719 0.21451875 0.18433934 0.11595062 0.2336428  0.17373797
 0.18346976 0.18163978]
alpha_repeat-2
METEOR
[0.15886689 0.1322003  0.09748368 0.0957271  0.15130636 0.16524912
 0.13189107 0.11323889 0.14482531 0.13417109 0.09478774 0.11837851
 0.1308111  0.13929356 0.11108865 0.1553037  0.13517388 0.16743143
 0.145083   0.13639034 0.1097357  0.12072131 0.17993673 0.17413636
 0.15646805 0.13633989 0.10169114 0.14535822 0.12597534 0.1196372
 0.12667877 0.14070868 0.15360457 0.15321164 0.14190177 0.14981503
 0.14814272 0.17284346 0.15563608 0.15001414 0.10305808 0.1377364
 0.17217804 0.10957024 0.15407943 0.16175455 0.12508653 0.14527265
 0.18562347 0.16057268 0.12705831 0.134813   0.11325384 0.09757595
 0.12965724 0.20644897 0.16370201 0.12612776 0.13877049 0.16130143
 0.13207379 0.16797099 0.10987341 0.15174676 0.13424539 0.1689286
 0.16082308 0.15355977 0.15392904 0.14151095 0.18300821 0.13960295
 0.15424314 0.15888949 0.12419947 0.14363211 0.12207225 0.12919391
 0.13700607 0.10036807 0.15032651 0.10903329 0.12887591 0.13270967
 0.13829097 0.1546987  0.13883426 0.13776616 0.15636392 0.13065138
 0.16169496 0.1751293  0.11848606 0.15218931 0.11926049 0.17296455
 0.12779765 0.16194901 0.1561224  0.12120865 0.14958039 0.10965934
 0.16096493 0.14771207 0.11845238 0.14855582 0.12763333 0.14863651
 0.12285035 0.16598197 0.13424107 0.12071902 0.11986213 0.14390982
 0.15433757 0.12679617 0.08001522 0.10567568 0.14527046 0.17721934
 0.15030072 0.10919676 0.1314181  0.14923025 0.14622402 0.13255476
 0.19331914 0.14816532 0.15615412 0.08578604 0.10371752 0.12044326
 0.12653788 0.13060019 0.17550071 0.1606038  0.15114059 0.16482139
 0.15446484 0.13962436 0.10670177 0.13715348 0.13384577 0.1498435
 0.16825177 0.13276434 0.13749571 0.14309316 0.14808363 0.14805474
 0.15000044 0.12873567 0.16401785 0.170626   0.15345725 0.15065274
 0.16254366 0.15644068 0.1485317  0.13675397 0.13582222 0.16222334
 0.14761328 0.15515651 0.16504531 0.1686792  0.14372677 0.13599471
 0.1487096  0.15980965 0.14430052 0.1403115  0.1273194  0.19133561
 0.15033036 0.14292133 0.17491239 0.12892788 0.09735646 0.13391543
 0.1454029  0.11141376 0.17051291 0.12047822 0.15952323 0.1366999
 0.18091573 0.16544519 0.13200481 0.15307583 0.15246745 0.10000976
 0.17112818 0.1353108  0.13661439 0.09542838 0.19366755 0.16569997
 0.14467362 0.14919019]
alpha_repeat-2
BERT
[0.7941923  0.7942866  0.79054755 0.7931421  0.7937667  0.7963899
 0.78667367 0.7906696  0.79934824 0.7885273  0.78908145 0.79530185
 0.7919035  0.78914905 0.79494095 0.79033625 0.7923103  0.79465485
 0.79038185 0.7924951  0.79170626 0.79007953 0.79530436 0.7971843
 0.79777074 0.7911541  0.7953047  0.79301804 0.7930276  0.79210484
 0.79389966 0.79433304 0.7998755  0.7958775  0.79699343 0.7922237
 0.7958518  0.80285794 0.798035   0.80262256 0.7930767  0.7934713
 0.79521227 0.7924291  0.7871614  0.7922207  0.79233116 0.8001228
 0.79369855 0.79150254 0.7962088  0.79374915 0.78840506 0.7908377
 0.7894047  0.7991264  0.7985468  0.7917623  0.79775816 0.79453605
 0.79547006 0.79184437 0.7889988  0.7944301  0.7952453  0.7929603
 0.7952672  0.7975476  0.79383796 0.79791456 0.7956506  0.79346454
 0.7950394  0.7967496  0.79033947 0.7955009  0.79520965 0.79410934
 0.7908354  0.79150045 0.8034133  0.79102147 0.7880515  0.7924844
 0.7934685  0.7943299  0.79164475 0.7888324  0.79711133 0.79084444
 0.79472333 0.797236   0.79526037 0.7939005  0.794925   0.8005031
 0.7957052  0.7965104  0.79567224 0.79106903 0.7957584  0.79102063
 0.7968082  0.79920065 0.7912319  0.7989668  0.7940707  0.79446274
 0.7946638  0.8025693  0.78962296 0.7936222  0.7938445  0.79013807
 0.7997962  0.7907053  0.7906256  0.7893914  0.79699725 0.79833686
 0.7943953  0.7931512  0.79610723 0.7955982  0.79395294 0.7922208
 0.7954896  0.79027146 0.79298073 0.7949858  0.7916204  0.78748304
 0.79315203 0.7899303  0.79641587 0.8004334  0.79243654 0.79332715
 0.79142165 0.79489917 0.7948959  0.7957426  0.79019433 0.7955382
 0.7960759  0.79526705 0.7917804  0.79084384 0.79560924 0.79172313
 0.79979956 0.79343367 0.7930602  0.79338783 0.79535145 0.79479563
 0.79440355 0.79596645 0.7937881  0.79443324 0.79681176 0.79878753
 0.7950288  0.79431313 0.7930616  0.79980886 0.7966186  0.7941168
 0.79787445 0.79582644 0.7958945  0.79452896 0.79523253 0.79611105
 0.79535663 0.7952262  0.79245573 0.79333365 0.7884551  0.7955088
 0.791278   0.79121894 0.79387677 0.79219174 0.8002835  0.79482716
 0.79864746 0.79359215 0.7933738  0.79714423 0.79463357 0.79113483
 0.7946361  0.7962351  0.79769915 0.79215175 0.79774356 0.7963482
 0.79327977 0.7894927 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
bravo_repeat-1
WER
[-0.00443709 -0.02865435 -0.00826407 -0.02248976 -0.02081028 -0.01702868
 -0.02827654 -0.02243849  0.00552562 -0.01468619 -0.0237572  -0.01418987
 -0.02496364 -0.01262221 -0.01071921 -0.00252175 -0.01383925 -0.02395495
 -0.021553   -0.03593274 -0.00844326 -0.02607673 -0.01552282  0.00913958
 -0.01368052 -0.02226092 -0.0187419  -0.01484451 -0.01162903 -0.0267147
 -0.01524429 -0.00836207 -0.01039681 -0.00464174 -0.02441984 -0.00485953
 -0.03290969 -0.01477744 -0.0297962   0.00771726 -0.02337283 -0.04074647
 -0.01122807 -0.01795635 -0.02635885 -0.02216186 -0.02589403 -0.0125499
 -0.03227807 -0.02206791 -0.0209566  -0.02378767  0.00429743 -0.01890529
 -0.0126076  -0.01459789 -0.01309949 -0.01645849 -0.01576383 -0.00191572
 -0.01373475 -0.01655775 -0.0277673  -0.00953137 -0.00897071 -0.02641537
 -0.02473214 -0.00594247 -0.02797349 -0.03217092 -0.00189377 -0.02556239
  0.02309397 -0.01519941 -0.0257758  -0.03829937 -0.00080287 -0.00748525
 -0.01413166  0.00700965 -0.0141855  -0.03326465 -0.01330995 -0.03334966
 -0.01476351 -0.01695163 -0.0124914  -0.00513266 -0.03601995 -0.02548574
 -0.01983355 -0.00123424 -0.01998005 -0.00658562 -0.02214854 -0.01947965
 -0.02652469 -0.02125567 -0.02373372 -0.01730893 -0.02807051 -0.01231586
 -0.0268579  -0.02356058 -0.0116023  -0.01769753 -0.03894457 -0.01594155
 -0.00259973 -0.01988413 -0.0050562  -0.01986945 -0.01697481 -0.03082465
 -0.02194025  0.00065127 -0.03133316 -0.02263363 -0.01382669 -0.03845002
 -0.00789038 -0.03306858 -0.01035075 -0.01148248 -0.01835396 -0.02819857
 -0.01768835 -0.02448376 -0.02186602  0.00365464 -0.01969024 -0.01576486
 -0.01889955 -0.0151849  -0.03159537 -0.02297591  0.00081261 -0.02867908
 -0.02383249 -0.01925224 -0.0296213  -0.02799355 -0.02880335 -0.00687883
 -0.03066047 -0.03381049  0.00217626 -0.01869243 -0.01263947 -0.02464968
 -0.02427256 -0.01676266 -0.01099249 -0.02190632 -0.01849299 -0.01097162
 -0.02094779 -0.01440214 -0.02635711 -0.03647341 -0.00185615 -0.01020301
 -0.01769825 -0.01838528 -0.01071527 -0.03075494 -0.004274   -0.00786093
 -0.01770051 -0.03480532  0.00046269 -0.02729445 -0.0238377  -0.02747368
 -0.02364319 -0.02280767 -0.02861653 -0.0260635  -0.00594556  0.0080324
 -0.01678384 -0.02540858 -0.01774891 -0.01074141 -0.00794816 -0.01823594
 -0.02763326 -0.00303105 -0.032203   -0.01068443 -0.02405134 -0.01710043
 -0.01985316 -0.0431252  -0.01053206 -0.01738903 -0.00992095 -0.02108051
 -0.02186026 -0.0088227 ]
bravo_repeat-1
BLEU
[0.2351139  0.204017   0.23219649 0.19216916 0.21659514 0.2263602
 0.17059747 0.23325117 0.25815772 0.23732786 0.24137049 0.25598209
 0.18451816 0.22905874 0.2151369  0.23106868 0.25059663 0.18506514
 0.23369056 0.22309665 0.18406688 0.18561434 0.19736495 0.21129441
 0.21344418 0.2438609  0.21611265 0.22923958 0.22978402 0.21110293
 0.21135773 0.23730065 0.25526881 0.23435756 0.19465504 0.21935771
 0.23712588 0.23114265 0.19281946 0.2429414  0.22388334 0.15896987
 0.21405958 0.16793361 0.13831036 0.24187136 0.21448849 0.23178429
 0.1726093  0.20515127 0.1741493  0.22791581 0.22229784 0.21570111
 0.19034842 0.24297855 0.2388486  0.22004267 0.22027082 0.22841371
 0.18216993 0.26206743 0.2089167  0.19325903 0.21533521 0.21995771
 0.2222446  0.21796537 0.18400369 0.22725134 0.24247815 0.22331374
 0.28155919 0.2325224  0.2037914  0.1752214  0.25543318 0.25027167
 0.2324442  0.22190407 0.21539413 0.15918893 0.24041121 0.17532234
 0.24852229 0.2120962  0.20853487 0.19371049 0.21203442 0.22550772
 0.23814996 0.24812736 0.23126279 0.24118409 0.22475959 0.17326795
 0.2607405  0.21574657 0.17709938 0.24387659 0.19895613 0.22839098
 0.18751488 0.22935244 0.27644246 0.18968367 0.18824352 0.2373034
 0.24382343 0.23577834 0.23152401 0.21888391 0.18407475 0.17237102
 0.15441293 0.24573727 0.21135657 0.19947728 0.20709772 0.22696749
 0.28399359 0.2360432  0.26893471 0.24695001 0.23590357 0.20114961
 0.26197666 0.17029073 0.21044631 0.21297585 0.21092391 0.17672848
 0.19355546 0.24796541 0.21273079 0.24824883 0.16979579 0.13540383
 0.18916292 0.23992824 0.18154837 0.21805249 0.16186287 0.20140692
 0.18739288 0.14597762 0.27683732 0.17263824 0.22720603 0.16876651
 0.17958009 0.18323861 0.26755862 0.22628054 0.23384818 0.21399374
 0.21669851 0.19993605 0.15662551 0.19592945 0.24719984 0.25347045
 0.21137212 0.23395867 0.16224925 0.18881639 0.27591935 0.22962216
 0.20411842 0.19273515 0.26926583 0.19068574 0.21783802 0.16532219
 0.22720098 0.15194807 0.19728744 0.18961431 0.23699803 0.23967661
 0.25573697 0.2268644  0.230041   0.21336544 0.19040384 0.22552875
 0.19599771 0.20808646 0.19072978 0.22466717 0.18536421 0.27072131
 0.25125586 0.12122605 0.22215649 0.25542958 0.25020456 0.24049793
 0.22301403 0.18450127]
bravo_repeat-1
METEOR
[0.1590668  0.1234112  0.15784797 0.11790662 0.15412615 0.13456103
 0.10039164 0.15501835 0.16616362 0.1583633  0.14832273 0.15998978
 0.12456724 0.167159   0.14141433 0.15298466 0.17751984 0.1193984
 0.15618848 0.15984807 0.12028987 0.12337567 0.14067869 0.15572569
 0.14649807 0.16635326 0.15229746 0.15482792 0.16184125 0.14032089
 0.15595    0.17766865 0.16796447 0.14608557 0.11576362 0.15661948
 0.15646568 0.17118551 0.12831301 0.15309716 0.15019839 0.12222433
 0.14617972 0.11480973 0.11034856 0.15147662 0.12400009 0.1646885
 0.11781509 0.13860473 0.11774386 0.14208308 0.14982537 0.14266052
 0.14305175 0.15424635 0.15947334 0.15216333 0.13371757 0.14558094
 0.14126786 0.15779362 0.13894237 0.13520881 0.15334153 0.1490129
 0.13935077 0.14037352 0.13319784 0.14383605 0.14815118 0.16756244
 0.17292354 0.14462946 0.13303186 0.11022164 0.16558484 0.16885175
 0.15275114 0.15283921 0.12792597 0.12055292 0.17155533 0.12646798
 0.15993326 0.15561801 0.13293753 0.12840736 0.13763073 0.14959235
 0.15795469 0.16366163 0.15147753 0.15178826 0.14855883 0.10331899
 0.17374513 0.15245578 0.11495204 0.15705215 0.12627068 0.12684802
 0.13033275 0.16854279 0.17972232 0.11720846 0.13406222 0.16017834
 0.1719775  0.17019117 0.14968248 0.14429225 0.15379789 0.10847349
 0.11431966 0.15049431 0.13271549 0.13630729 0.13429679 0.13944028
 0.18983047 0.15702784 0.18165333 0.17031866 0.16024375 0.14312398
 0.16977369 0.11169331 0.13642427 0.14843071 0.14344545 0.11901019
 0.14757872 0.17320773 0.12651734 0.19067853 0.1220309  0.10287233
 0.12945724 0.15784459 0.1261871  0.15904407 0.11320861 0.1288603
 0.13605222 0.1023989  0.18258057 0.13060128 0.15542422 0.11422388
 0.11860142 0.15582788 0.14979696 0.15006789 0.1480891  0.14719967
 0.15582328 0.13471127 0.11030566 0.13917408 0.17766122 0.16639002
 0.1328899  0.14516257 0.11627048 0.13575891 0.17838189 0.14478601
 0.13135417 0.14422501 0.16741936 0.11762627 0.14947896 0.12011733
 0.14887988 0.10786378 0.13464894 0.13578188 0.15537397 0.16786275
 0.18745401 0.16215247 0.14288779 0.13476805 0.12459472 0.13622126
 0.14670369 0.14997149 0.12414587 0.14859354 0.1321086  0.17522139
 0.15261828 0.08812164 0.14796163 0.17430937 0.17683147 0.15430272
 0.16103135 0.13664708]
bravo_repeat-1
BERT
[0.81099886 0.80488616 0.8085436  0.8032845  0.8039347  0.80262923
 0.8024359  0.80341434 0.80657446 0.8032449  0.79920876 0.8001481
 0.8039349  0.8060518  0.8057077  0.79989773 0.8056619  0.8042092
 0.80981714 0.80427605 0.80667466 0.8008965  0.8051814  0.8020417
 0.80577433 0.80629945 0.8096048  0.8079081  0.8084987  0.8042677
 0.8108195  0.8073805  0.80771565 0.8057311  0.79964876 0.80507034
 0.8082196  0.80642235 0.8054433  0.8045378  0.80438393 0.80368686
 0.8027122  0.8073556  0.80254114 0.80625564 0.80634737 0.8071702
 0.80018646 0.80320656 0.8033812  0.80434215 0.8034587  0.80596805
 0.8008693  0.806739   0.80140084 0.80629313 0.8089308  0.8055923
 0.80571663 0.8047497  0.80176115 0.7997808  0.8086077  0.8053053
 0.800246   0.8074725  0.8088807  0.7989024  0.80485237 0.80289215
 0.8067514  0.80506766 0.8082722  0.8022674  0.8033839  0.8072259
 0.8087142  0.8042291  0.81267583 0.79920316 0.80684894 0.8035587
 0.80802816 0.80591714 0.80406535 0.80496365 0.8057823  0.80644643
 0.8056539  0.80469644 0.8048686  0.80413276 0.80194104 0.80012065
 0.81080276 0.8037708  0.8016645  0.8079481  0.8033854  0.8067382
 0.80447453 0.8127813  0.808572   0.8036272  0.80129856 0.8057286
 0.80396307 0.8100492  0.8025417  0.8068673  0.80104494 0.7984737
 0.80296224 0.8033451  0.8035199  0.79994303 0.79900813 0.8039886
 0.8065284  0.8075985  0.8073526  0.81009245 0.8072919  0.80236846
 0.8084981  0.80365324 0.80372673 0.80811894 0.8071881  0.80515534
 0.8077741  0.8096539  0.8022182  0.8043153  0.8031213  0.80149746
 0.8066939  0.8067186  0.8064376  0.80227757 0.80368394 0.8095624
 0.800017   0.79992497 0.80802286 0.8043361  0.80846703 0.80110306
 0.80249107 0.8057487  0.8049038  0.8085042  0.80648386 0.805333
 0.803785   0.8052917  0.8025148  0.80449307 0.80988395 0.8067088
 0.8044815  0.80929345 0.8013705  0.80639017 0.8067176  0.8051169
 0.80093634 0.8037856  0.8083564  0.80525047 0.8036339  0.800865
 0.80547994 0.80127037 0.8044268  0.80658937 0.80209535 0.8102339
 0.8126859  0.8061444  0.8088901  0.8081622  0.805207   0.8050784
 0.8047245  0.80465746 0.80262953 0.80317354 0.8020582  0.80865294
 0.80687183 0.80411774 0.80720425 0.805213   0.810042   0.80397815
 0.8058221  0.8017512 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
bravo_repeat-2
WER
[-0.06482008 -0.07094681 -0.04639371 -0.03671569 -0.03747196 -0.06029539
 -0.05936081 -0.04621798 -0.05224701 -0.04488195 -0.0563739  -0.03758341
 -0.03660932 -0.05418698 -0.0547011  -0.03879915 -0.05293174 -0.03195717
 -0.05480273 -0.04593034 -0.07644952 -0.04965065 -0.05959206 -0.0727371
 -0.0508866  -0.04853583 -0.0590653  -0.03233188 -0.0446931  -0.05958404
 -0.04630039 -0.04039619 -0.05896451 -0.03765363 -0.04534306 -0.03356353
 -0.05581207 -0.04869865 -0.05178273 -0.04722727 -0.04166444 -0.06372218
 -0.0378043  -0.06247692 -0.06021388 -0.04351003 -0.05238208 -0.05440538
 -0.04481093 -0.04560155 -0.05780337 -0.05963947 -0.06756233 -0.04360716
 -0.04985826 -0.0331764  -0.03761706 -0.04913354 -0.04118187 -0.07354348
 -0.05801764 -0.0615867  -0.05281119 -0.05199389 -0.04780115 -0.0191354
 -0.06124907 -0.06177006 -0.06294955 -0.05894844 -0.04743704 -0.04926967
 -0.07444914 -0.06217174 -0.05991452 -0.0469878  -0.04423411 -0.05516732
 -0.05717898 -0.03840057 -0.04426778 -0.04656989 -0.06537767 -0.06020968
 -0.05316513 -0.03637077 -0.07429087 -0.04181238 -0.0475237  -0.05201975
 -0.0456582  -0.05445668 -0.06371706 -0.04968704 -0.049922   -0.03360658
 -0.04599067 -0.06140822 -0.05695341 -0.07212535 -0.06680404 -0.03144716
 -0.06458329 -0.03859649 -0.02489177 -0.04769588 -0.07448886 -0.05735559
 -0.02897664 -0.03924664 -0.0592839  -0.0480829  -0.04432967 -0.05157744
 -0.04115507 -0.03506185 -0.05386639 -0.05716939 -0.03571743 -0.06051361
 -0.0546474  -0.050141   -0.06605759 -0.05204531 -0.0217579  -0.05427879
 -0.06501516 -0.04760182 -0.05729697 -0.04729298 -0.05785537 -0.03701738
 -0.05729644 -0.04882714 -0.03910377 -0.04424116 -0.05132609 -0.04572873
 -0.05714614 -0.04690381 -0.04948573 -0.0665134  -0.06156585 -0.05670388
 -0.0496928  -0.02734892 -0.04821707 -0.04765264 -0.04445089 -0.05691944
 -0.03711941 -0.05437264 -0.0596885  -0.04243462 -0.04790181 -0.05215303
 -0.04624184 -0.06897639 -0.05462324 -0.06597689 -0.06770677 -0.04686382
 -0.03774852 -0.05397452 -0.05913459 -0.0471515  -0.03577504 -0.0730959
 -0.05696763 -0.04888751 -0.05328227 -0.04266933 -0.05329492 -0.04384586
 -0.05734327 -0.03009878 -0.07303413 -0.04267851 -0.05596294 -0.05539571
 -0.05619161 -0.05153125 -0.03472969 -0.0706078  -0.05178153 -0.05895246
 -0.05945422 -0.03292437 -0.05855771 -0.0266945  -0.05612697 -0.05130158
 -0.05307768 -0.05934659 -0.04695258 -0.04764303 -0.04121852 -0.06383256
 -0.05652503 -0.04844126]
bravo_repeat-2
BLEU
[0.19463184 0.18448835 0.23279884 0.24373716 0.24614769 0.22048799
 0.14523469 0.22569316 0.19949785 0.19190124 0.20600436 0.23939174
 0.2497946  0.20390803 0.20809336 0.23744011 0.22037821 0.23194051
 0.22400113 0.19888298 0.15161084 0.23108269 0.20256268 0.23671352
 0.22663874 0.23401196 0.22775639 0.17758434 0.22254967 0.19540785
 0.23632752 0.25259419 0.18192764 0.22985717 0.21871979 0.24686024
 0.17376223 0.23952852 0.18027386 0.1926169  0.24678849 0.21328412
 0.20390678 0.21588061 0.19439663 0.24348502 0.19639301 0.16647547
 0.25724963 0.21496589 0.1855751  0.2090065  0.19438554 0.24783918
 0.22790463 0.23136007 0.2205965  0.20490127 0.22575499 0.15582397
 0.21211181 0.24022477 0.15830982 0.20451691 0.18774842 0.1610499
 0.16885897 0.20452721 0.23429086 0.24103037 0.1995905  0.22716684
 0.17706679 0.2198278  0.16485873 0.24397224 0.22693038 0.21311281
 0.21813779 0.23091173 0.21850908 0.21110415 0.18892648 0.19438805
 0.23194663 0.23667694 0.13802253 0.2423273  0.19228564 0.18871158
 0.22950433 0.21475499 0.19197178 0.20247075 0.21526517 0.21851043
 0.30044744 0.13058242 0.19132471 0.18533912 0.16249709 0.19966101
 0.22049937 0.2653675  0.22520295 0.23675855 0.16055991 0.22725925
 0.23959417 0.19113511 0.19886281 0.24311377 0.23403659 0.21888982
 0.22919778 0.22027164 0.20618764 0.23624888 0.2376402  0.20284306
 0.20512356 0.22637068 0.22059295 0.20540065 0.23767543 0.15718723
 0.1687879  0.19781457 0.19689003 0.21156364 0.20180127 0.25506416
 0.2086466  0.1970355  0.2286746  0.1926195  0.24326136 0.24494936
 0.25815386 0.23813648 0.25354133 0.16035733 0.18382891 0.22871379
 0.17289644 0.17777704 0.2334549  0.20162152 0.18991453 0.23446931
 0.22503238 0.17548766 0.19570772 0.21552911 0.2489474  0.19025793
 0.18670495 0.1670014  0.2132559  0.17596791 0.14981973 0.18543316
 0.2208021  0.20816741 0.20206949 0.23988342 0.21822544 0.16744475
 0.21068731 0.21449949 0.21225797 0.1872045  0.17652724 0.23074846
 0.18657172 0.26970949 0.18755141 0.22631417 0.20077642 0.21381034
 0.2276363  0.21398406 0.23472473 0.20820321 0.22297584 0.18967572
 0.20348306 0.21582124 0.1643324  0.23893636 0.23615286 0.20312254
 0.22076467 0.20144591 0.22034547 0.25065005 0.22845919 0.18502883
 0.17739311 0.22830082]
bravo_repeat-2
METEOR
[0.1400831  0.13484939 0.19161584 0.15458082 0.17228379 0.14660523
 0.12996183 0.15560212 0.1464269  0.15594661 0.17426933 0.1592569
 0.1711443  0.13917233 0.13338129 0.16732366 0.14912469 0.15343739
 0.14395908 0.13338473 0.11307879 0.13637081 0.13752794 0.15703323
 0.1552424  0.14902881 0.16946099 0.13382513 0.14797031 0.13141183
 0.17959115 0.20152449 0.12210204 0.15642555 0.14967579 0.16396295
 0.11013072 0.16199135 0.12952475 0.13271868 0.16398485 0.14557498
 0.1398677  0.15153433 0.13550499 0.18721972 0.14628582 0.11416985
 0.16953293 0.17012218 0.14555214 0.15752962 0.12672241 0.20159004
 0.15118333 0.17125269 0.15483175 0.14547772 0.15140524 0.0996065
 0.1509352  0.17248774 0.09867363 0.14308551 0.12308958 0.14141139
 0.11430102 0.15433432 0.15286563 0.18232693 0.13926109 0.1580387
 0.1205621  0.1625015  0.12981639 0.18456807 0.14786631 0.14688652
 0.16908492 0.16761541 0.1464322  0.13994525 0.13356093 0.13471043
 0.15607917 0.16579972 0.10625629 0.16934529 0.1353761  0.1403332
 0.17794366 0.15855304 0.13170405 0.14739226 0.13663604 0.17602749
 0.20252341 0.1006596  0.14955806 0.12205513 0.1212231  0.14611662
 0.15440177 0.16561637 0.17070506 0.14893071 0.1138851  0.14318835
 0.17213594 0.14036902 0.14809279 0.21750695 0.15954717 0.14925658
 0.14749767 0.15948245 0.13916447 0.13796918 0.16456388 0.13894104
 0.15119109 0.15463116 0.17861829 0.13872249 0.16399479 0.11072148
 0.11650763 0.14736608 0.14403334 0.1453637  0.13900308 0.17541742
 0.14853994 0.1409249  0.17426452 0.15043793 0.15857232 0.1521701
 0.16554124 0.1510557  0.17308199 0.11439204 0.12100054 0.16233448
 0.13109916 0.12649795 0.1606628  0.15704835 0.14966533 0.15750324
 0.15066926 0.12965046 0.14353618 0.12815576 0.15173625 0.12652501
 0.14394725 0.13163397 0.17116261 0.11550067 0.11745636 0.12542674
 0.15100054 0.15004771 0.16216684 0.14976417 0.13457945 0.1279634
 0.15935356 0.15177154 0.14143489 0.13746938 0.13945731 0.16313566
 0.12366569 0.17054345 0.13633735 0.15520779 0.13686811 0.14048982
 0.15499178 0.13933095 0.17421645 0.14023514 0.14254991 0.12318172
 0.13131761 0.15326841 0.12036826 0.15447969 0.16792743 0.13318054
 0.15727813 0.11702164 0.15171978 0.16739345 0.15305636 0.14452036
 0.12700822 0.1415854 ]
bravo_repeat-2
BERT
[0.804993   0.80074346 0.80855376 0.8072176  0.81030345 0.80766624
 0.8045968  0.80511725 0.8039486  0.8073971  0.80982023 0.8084473
 0.8125398  0.8086964  0.8013445  0.80842274 0.81093067 0.803358
 0.8096027  0.8030977  0.8039048  0.80082583 0.80182564 0.8038378
 0.80935305 0.8089887  0.8060448  0.8047958  0.81207013 0.8027881
 0.8073344  0.80696833 0.80120254 0.8071815  0.806398   0.8035551
 0.8013871  0.806875   0.8039041  0.8060474  0.80768466 0.8022562
 0.80201226 0.80321306 0.80467874 0.8118336  0.8008028  0.8080275
 0.8038342  0.8091308  0.8029835  0.8067756  0.80493116 0.806222
 0.8093643  0.81062496 0.8053179  0.8045178  0.8021454  0.80273706
 0.804001   0.8104888  0.79859024 0.8054626  0.80422175 0.8033199
 0.8033542  0.7992149  0.80696684 0.8072704  0.81015384 0.80315566
 0.80432427 0.80194056 0.7982602  0.8064842  0.8078149  0.8011946
 0.80736405 0.80531377 0.8032101  0.80483264 0.8006837  0.80468357
 0.8058004  0.8108379  0.8013525  0.8090364  0.801241   0.7992697
 0.80783314 0.805235   0.8043528  0.80380666 0.803636   0.80634505
 0.80898845 0.80116135 0.80280685 0.8015871  0.8034852  0.80501455
 0.8048066  0.8119177  0.8045591  0.8070557  0.8031555  0.80513436
 0.80604523 0.80402803 0.80411196 0.8109809  0.8077474  0.8047649
 0.80373746 0.8027772  0.8075892  0.80088276 0.8038566  0.80244374
 0.80384374 0.80341506 0.8054498  0.80441564 0.80659455 0.79881704
 0.797627   0.8072571  0.804046   0.8012588  0.80103254 0.80625546
 0.80301255 0.8076573  0.8102677  0.8016004  0.8058503  0.80491096
 0.8089908  0.8040755  0.80630755 0.81059337 0.8020051  0.8052486
 0.8025601  0.8000264  0.8023801  0.80638003 0.80394965 0.8052292
 0.80982786 0.80530643 0.80574936 0.800552   0.80327976 0.8029656
 0.8016545  0.8024285  0.8068659  0.80512536 0.8007459  0.79907656
 0.8087911  0.8105689  0.80649024 0.80433303 0.8041685  0.8041258
 0.80411947 0.80197906 0.80205536 0.80025494 0.8036542  0.799846
 0.80338836 0.80422723 0.8016153  0.8074472  0.8066424  0.80249226
 0.80453587 0.8046086  0.801821   0.8014609  0.80752116 0.8040578
 0.80862314 0.803797   0.8021025  0.80604684 0.80634147 0.8005817
 0.80822825 0.8030284  0.8124469  0.8132887  0.80645686 0.8082051
 0.80240685 0.8029561 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
charlie_repeat-1
WER
[-0.13072774 -0.13192662 -0.12124984 -0.11772181 -0.13679184 -0.10829742
 -0.11921783 -0.12991877 -0.10721758 -0.13968615 -0.1094281  -0.13660592
 -0.11768735 -0.12461388 -0.14463502 -0.13149322 -0.11719968 -0.12715367
 -0.12631137 -0.10178953 -0.13411194 -0.10517521 -0.11298769 -0.12548553
 -0.13145811 -0.13784055 -0.11462862 -0.13790664 -0.12805279 -0.12852506
 -0.13663331 -0.15017287 -0.12569975 -0.12967524 -0.13438649 -0.14000568
 -0.12713316 -0.12735644 -0.12826275 -0.12881751 -0.12400802 -0.13530072
 -0.12117025 -0.13489394 -0.13995596 -0.1367695  -0.14137021 -0.13012458
 -0.12517785 -0.13426446 -0.12308574 -0.10312982 -0.12569026 -0.10898894
 -0.09890589 -0.12122202 -0.13258368 -0.13151755 -0.11318468 -0.13298154
 -0.11306288 -0.1127551  -0.12761933 -0.11577685 -0.13713323 -0.13815183
 -0.1223641  -0.12790963 -0.1182506  -0.14401133 -0.12711043 -0.12873279
 -0.11814253 -0.11525212 -0.11916615 -0.14556865 -0.13361584 -0.12142251
 -0.13440201 -0.13073723 -0.11761059 -0.12095506 -0.13376736 -0.10077619
 -0.14138921 -0.1320895  -0.10154734 -0.12768554 -0.12644959 -0.13333214
 -0.12546572 -0.11026251 -0.12104716 -0.12282922 -0.13510714 -0.12355716
 -0.14355854 -0.12481496 -0.13960023 -0.1199156  -0.12618353 -0.14181328
 -0.13626868 -0.14808872 -0.13357096 -0.09689744 -0.11467575 -0.13466516
 -0.13102515 -0.13597694 -0.12698806 -0.10949247 -0.13574134 -0.1349472
 -0.11515668 -0.12711565 -0.13333112 -0.12116608 -0.15329488 -0.1507721
 -0.12696148 -0.14056451 -0.13438621 -0.11745965 -0.13841294 -0.13461625
 -0.11900859 -0.15347102 -0.13567883 -0.12943667 -0.10450936 -0.12550496
 -0.13385728 -0.12429819 -0.1170839  -0.10414166 -0.1565022  -0.12804853
 -0.13884993 -0.12416721 -0.13334865 -0.12057489 -0.14416282 -0.12054245
 -0.14039018 -0.13830161 -0.12992351 -0.14135642 -0.13248424 -0.13955253
 -0.10807958 -0.13398794 -0.12975434 -0.11771567 -0.13309271 -0.13738812
 -0.13411271 -0.11768025 -0.12402907 -0.11879403 -0.13522962 -0.12732612
 -0.11672751 -0.12607212 -0.13718038 -0.1252866  -0.15548155 -0.13578281
 -0.14253109 -0.11862586 -0.13701708 -0.12511122 -0.14342207 -0.1296538
 -0.13570858 -0.14818007 -0.13294641 -0.11321905 -0.11087272 -0.13544054
 -0.11806034 -0.11936051 -0.11924517 -0.12177346 -0.13918735 -0.12226721
 -0.12030119 -0.13046625 -0.11494387 -0.14079353 -0.10599447 -0.12426469
 -0.12734888 -0.12937782 -0.13170522 -0.14085749 -0.13405866 -0.11636965
 -0.13971104 -0.1507089 ]
charlie_repeat-1
BLEU
[0.18593245 0.18004846 0.18047396 0.19906046 0.1996065  0.20348654
 0.19782941 0.2010872  0.18358767 0.19641368 0.18593194 0.17831394
 0.15286277 0.20813671 0.14191447 0.17757411 0.20394717 0.18787908
 0.18424485 0.21078444 0.19692093 0.18991346 0.23806888 0.19796722
 0.17279973 0.18290344 0.20645398 0.1763227  0.1839916  0.18539289
 0.15219635 0.1847884  0.17990886 0.18152729 0.19586946 0.17855612
 0.19309662 0.19301648 0.18228819 0.21937761 0.18100642 0.19576005
 0.19567004 0.18455248 0.14532924 0.20198141 0.15791507 0.17780519
 0.19015981 0.2131255  0.20280195 0.22597276 0.20941376 0.19150978
 0.20899368 0.21907733 0.16597733 0.18911669 0.19575829 0.16256658
 0.17410349 0.20149153 0.167393   0.19744632 0.17059075 0.16753032
 0.18915997 0.20875832 0.20124477 0.15482273 0.16162927 0.17517102
 0.18925151 0.18484237 0.21117017 0.16558224 0.20582788 0.17106525
 0.18695356 0.19080546 0.1650011  0.21531572 0.16022936 0.22513269
 0.17087456 0.18335084 0.18732559 0.20475487 0.17331634 0.20101158
 0.21514895 0.20402559 0.21176264 0.21548729 0.18699051 0.19685576
 0.15127948 0.21888548 0.16963817 0.19619047 0.15332154 0.19648172
 0.19299742 0.15614773 0.18957217 0.23495147 0.1785239  0.17910285
 0.14532389 0.18288686 0.20458129 0.20800164 0.17432026 0.19032221
 0.20440717 0.19051221 0.17806999 0.20018774 0.15354582 0.15267118
 0.1941729  0.20761468 0.1876428  0.20800856 0.20399498 0.15992932
 0.22644181 0.16331883 0.17694257 0.19550362 0.19616113 0.19295642
 0.18765604 0.20753602 0.18117076 0.23015818 0.14713118 0.17524437
 0.18525323 0.21209967 0.15519585 0.18580317 0.1878499  0.20285127
 0.1912573  0.16896447 0.18917319 0.19460816 0.21271831 0.16690028
 0.19243987 0.1626524  0.1876929  0.20686127 0.19538022 0.19525819
 0.16606539 0.18907758 0.20084031 0.20712877 0.16301915 0.21118441
 0.20056036 0.18242273 0.14258926 0.1866196  0.13556571 0.19077107
 0.19301384 0.20365946 0.1589612  0.19031104 0.17586671 0.19434446
 0.16197554 0.14438614 0.16739571 0.21002117 0.22396136 0.20284381
 0.18850632 0.1854643  0.19287311 0.20516014 0.1877909  0.18374089
 0.18815732 0.18225803 0.1806732  0.17037436 0.20185551 0.20273014
 0.20718049 0.15868791 0.16725371 0.14760204 0.1834758  0.20247642
 0.15924706 0.18635875]
charlie_repeat-1
METEOR
[0.15374744 0.15459236 0.15455596 0.14918152 0.16232191 0.16900206
 0.17546468 0.1519328  0.15102244 0.13856548 0.145641   0.13909487
 0.14149467 0.15901913 0.11256264 0.12703143 0.17438609 0.15817069
 0.1309487  0.16018937 0.16380722 0.14166774 0.17714401 0.16019399
 0.14199477 0.16049298 0.16223136 0.13284779 0.13555365 0.13945743
 0.12487568 0.14070925 0.13304256 0.12856648 0.17108186 0.15479551
 0.16007755 0.16325356 0.13985778 0.1835945  0.13573673 0.16394607
 0.1556844  0.14856665 0.13472166 0.16029963 0.13225332 0.14795274
 0.15231479 0.15769706 0.16623014 0.17943455 0.152575   0.1526263
 0.17281652 0.18501527 0.14410337 0.13130392 0.17227728 0.12753278
 0.14803473 0.17197895 0.13574428 0.14576316 0.15457669 0.13574675
 0.13559082 0.18145404 0.16619663 0.14113059 0.13606619 0.15933872
 0.16049115 0.15411236 0.17009178 0.14630808 0.18553777 0.13330848
 0.16370498 0.15905535 0.11868304 0.15414186 0.12823592 0.16132505
 0.13429714 0.13464042 0.15934627 0.15411498 0.17004119 0.16211029
 0.18192353 0.15838274 0.1643818  0.17065367 0.13344263 0.15112362
 0.1258278  0.16678436 0.15137833 0.13968173 0.1503963  0.15028049
 0.15082821 0.11340361 0.12408733 0.18519658 0.13447776 0.12623485
 0.12094467 0.16055485 0.15859949 0.18368979 0.13507709 0.1324292
 0.15150359 0.14592755 0.14289175 0.1615958  0.12464235 0.12020165
 0.17233927 0.15305446 0.15052035 0.16031632 0.15711629 0.15181784
 0.16465889 0.14299845 0.16056609 0.15284664 0.16510034 0.14563639
 0.13878854 0.16391701 0.12825862 0.19865197 0.11148877 0.14008316
 0.15203113 0.17144406 0.12876743 0.14557995 0.1694926  0.15836359
 0.15286411 0.14110457 0.14160212 0.18333515 0.17344463 0.13128868
 0.16657464 0.15919125 0.16442619 0.15294824 0.16020016 0.15461161
 0.15483225 0.17073873 0.15236404 0.17279406 0.14882084 0.16735981
 0.16170704 0.14935489 0.1212623  0.15316378 0.10816736 0.13452701
 0.14974119 0.15621112 0.12885397 0.16309842 0.13846533 0.1814133
 0.13805201 0.13723426 0.15407156 0.14441062 0.17522738 0.16063751
 0.1609911  0.14675783 0.14944497 0.17130809 0.13133438 0.15997078
 0.16988181 0.1392194  0.14627283 0.13897193 0.14842315 0.1607455
 0.17464234 0.10642276 0.12615237 0.13879053 0.14370794 0.15598458
 0.11266129 0.15230779]
charlie_repeat-1
BERT
[0.7816189  0.78610444 0.78315    0.7826276  0.78869134 0.7922541
 0.7928794  0.787325   0.7893784  0.7855137  0.78126097 0.78167444
 0.78608036 0.7859257  0.7906454  0.7861893  0.7867163  0.7823697
 0.78254646 0.78986794 0.7853667  0.78365326 0.7884997  0.7847318
 0.78471255 0.78150475 0.785177   0.78312236 0.7820118  0.7812404
 0.7817475  0.7850927  0.78845584 0.783334   0.79098994 0.7871693
 0.78448206 0.79073584 0.78317505 0.7881788  0.78136605 0.7866331
 0.78179365 0.7840858  0.78032047 0.78216285 0.7884961  0.78107905
 0.79111516 0.790043   0.7862831  0.78517777 0.7887993  0.7841997
 0.7871916  0.7869151  0.78313315 0.7814693  0.782338   0.78484076
 0.7889045  0.7839428  0.7788966  0.7855954  0.7803223  0.7837536
 0.78514725 0.78031605 0.78749216 0.78583544 0.7877365  0.7892974
 0.78399277 0.7859117  0.78346485 0.778985   0.783079   0.782648
 0.7868952  0.7834944  0.7824477  0.7875117  0.7849338  0.7907188
 0.7889062  0.7813969  0.78904575 0.7777051  0.78374493 0.7831091
 0.7877989  0.785914   0.7839198  0.7889315  0.7833424  0.79081374
 0.7843466  0.7850682  0.78471875 0.785218   0.7837756  0.7888661
 0.7916403  0.7837719  0.7899341  0.79201293 0.782286   0.7856049
 0.77961147 0.7859258  0.78695744 0.78840107 0.78417045 0.78164256
 0.7818476  0.79273826 0.78392214 0.7843101  0.7875207  0.78315485
 0.7853599  0.78484845 0.78761756 0.7868893  0.7865657  0.7852301
 0.79306805 0.78502953 0.79094696 0.7840908  0.78743726 0.7848285
 0.7843049  0.7894674  0.7809848  0.7875403  0.7802183  0.7883611
 0.7838541  0.782746   0.78317606 0.7803734  0.78359485 0.78676337
 0.7838426  0.7800834  0.78231716 0.7879722  0.7842696  0.78042656
 0.7922337  0.78508544 0.7793593  0.7869141  0.7829882  0.78833103
 0.78414536 0.79198074 0.78577906 0.78716826 0.7794754  0.78992534
 0.78499025 0.7862277  0.78143245 0.7832229  0.7871482  0.7879417
 0.7865943  0.78812057 0.7821644  0.78828764 0.78549355 0.78247
 0.7800344  0.78284955 0.781494   0.7860187  0.7875855  0.7861996
 0.7869016  0.78463495 0.7794633  0.78385854 0.78561085 0.7841944
 0.7859756  0.7822252  0.7868331  0.78062356 0.7890054  0.78664947
 0.78978527 0.78809226 0.7828449  0.78056705 0.7848325  0.7853281
 0.78229916 0.7868591 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
charlie_repeat-2
WER
[-0.1265745  -0.1349015  -0.11412704 -0.14109663 -0.12582051 -0.12765608
 -0.11642409 -0.11627319 -0.12768784 -0.11556949 -0.13504518 -0.11332136
 -0.12775426 -0.12892021 -0.13412174 -0.11517454 -0.12740128 -0.13110284
 -0.12251806 -0.12775321 -0.11019801 -0.11422709 -0.12018583 -0.110658
 -0.12923041 -0.14653873 -0.11486581 -0.12366062 -0.13173562 -0.12848612
 -0.113257   -0.12925523 -0.12628963 -0.11539631 -0.11526682 -0.12047435
 -0.12976139 -0.11588163 -0.11611534 -0.11287692 -0.11406698 -0.13176216
 -0.12300094 -0.12407872 -0.11985498 -0.11102399 -0.13215963 -0.11658086
 -0.12160157 -0.13634908 -0.1113259  -0.12522086 -0.10772951 -0.11410168
 -0.12292937 -0.13062962 -0.11198604 -0.12332383 -0.11963555 -0.11240674
 -0.11456515 -0.13148043 -0.12250415 -0.13147385 -0.11901767 -0.12370412
 -0.11701565 -0.12397214 -0.11150234 -0.13386844 -0.12717481 -0.10252452
 -0.12628014 -0.12994335 -0.1147814  -0.11083218 -0.10430062 -0.13474166
 -0.12532322 -0.10002631 -0.12334874 -0.10978948 -0.11935764 -0.13322405
 -0.09681637 -0.13562011 -0.11276302 -0.11501097 -0.11487182 -0.1336024
 -0.10636117 -0.12606931 -0.10471713 -0.12561087 -0.11530257 -0.15303302
 -0.11193317 -0.12334715 -0.11578319 -0.13231762 -0.10164825 -0.1151886
 -0.12189697 -0.11502801 -0.11896415 -0.12261244 -0.12311031 -0.1160387
 -0.11844583 -0.11435822 -0.12550557 -0.12588086 -0.11553199 -0.09331458
 -0.12646129 -0.13079627 -0.10203871 -0.12128227 -0.13230984 -0.1290092
 -0.10741977 -0.11405704 -0.11466247 -0.12467357 -0.127687   -0.11248524
 -0.11050261 -0.13933068 -0.13709695 -0.10139476 -0.11958067 -0.12510028
 -0.1097195  -0.10747291 -0.12369938 -0.10917582 -0.13017726 -0.12014572
 -0.11677488 -0.11413695 -0.12322171 -0.12805634 -0.12713593 -0.11214408
 -0.13539247 -0.13287069 -0.12389296 -0.11521445 -0.11895828 -0.11936215
 -0.11491987 -0.10751391 -0.13571406 -0.1222584  -0.0875169  -0.11615957
 -0.1326594  -0.11601385 -0.12705108 -0.10539013 -0.14022791 -0.12630094
 -0.12036714 -0.13403264 -0.11809406 -0.12986805 -0.12922392 -0.12201072
 -0.11731468 -0.12864938 -0.11212791 -0.12303959 -0.12731146 -0.12848886
 -0.10251256 -0.13021536 -0.10851132 -0.12861046 -0.12317687 -0.13435043
 -0.12017235 -0.14021996 -0.12461879 -0.13465395 -0.11685912 -0.13177574
 -0.09748948 -0.11400765 -0.12715985 -0.13635894 -0.10965372 -0.12982741
 -0.12830172 -0.12774065 -0.12757123 -0.1230162  -0.1362042  -0.14512543
 -0.12819272 -0.12003416]
charlie_repeat-2
BLEU
[0.14112637 0.17357619 0.20225304 0.15332019 0.1954232  0.16447941
 0.19230443 0.19515236 0.15214904 0.1926815  0.15430386 0.17479569
 0.17673176 0.16739394 0.17356657 0.20649586 0.16132422 0.14515724
 0.1711395  0.18156202 0.193999   0.20339279 0.170273   0.19780426
 0.16406812 0.15130402 0.19492826 0.17467121 0.16007401 0.20164447
 0.17525354 0.15446012 0.16859745 0.18387471 0.20367805 0.17015282
 0.15995418 0.15570693 0.17144244 0.20500763 0.15122694 0.13608408
 0.1620443  0.19334329 0.18612443 0.19855688 0.15447471 0.15762725
 0.16085824 0.16326677 0.19337935 0.1598587  0.17841079 0.15367403
 0.1698761  0.14473289 0.20604497 0.18803453 0.17505568 0.17752154
 0.17292852 0.13682977 0.17796584 0.16331785 0.20410008 0.15712233
 0.17804212 0.17959076 0.17662365 0.14874689 0.16467105 0.20305771
 0.20168278 0.17009506 0.1771873  0.18691795 0.20952228 0.15085554
 0.18374951 0.21438801 0.16911217 0.17501668 0.18554691 0.18125062
 0.17948537 0.16024894 0.22549443 0.18085746 0.20162546 0.15732095
 0.19771309 0.12594782 0.20381916 0.17678421 0.19401014 0.13341562
 0.14656675 0.19657758 0.20752108 0.13595098 0.18417463 0.19416695
 0.15095044 0.1968529  0.20878177 0.1722367  0.18194033 0.18024336
 0.18502854 0.15371892 0.17672745 0.16937702 0.16796565 0.18779865
 0.13698576 0.16099002 0.19349455 0.17805953 0.17495353 0.19806241
 0.18457952 0.19437883 0.1863639  0.15470127 0.19858712 0.22547116
 0.18035306 0.17232002 0.13259417 0.2052364  0.20198701 0.16048457
 0.19279884 0.18170484 0.18159893 0.19914876 0.17544937 0.17735182
 0.1744472  0.18097375 0.20812947 0.13027551 0.17872973 0.18879999
 0.15282956 0.16844984 0.18008463 0.19441937 0.19749167 0.17640124
 0.19820295 0.19510332 0.17493233 0.20598421 0.16856811 0.16645843
 0.16486686 0.19956429 0.14896555 0.18213864 0.15601323 0.17886425
 0.19148133 0.13728576 0.17966546 0.15848382 0.16281747 0.20109989
 0.17305938 0.16136054 0.18790017 0.13680782 0.15313385 0.15967429
 0.16155006 0.17546869 0.19604934 0.16357402 0.20075454 0.16533946
 0.18042194 0.15118409 0.17750704 0.14976839 0.16064155 0.13136737
 0.21787978 0.16623578 0.15318735 0.16971971 0.19432132 0.17945017
 0.15623602 0.17198454 0.16548577 0.18435138 0.15325005 0.14174538
 0.1719583  0.16768861]
charlie_repeat-2
METEOR
[0.10704325 0.1394818  0.1715364  0.12053928 0.14603766 0.14920382
 0.14628537 0.18480897 0.12303475 0.14590342 0.11662142 0.15487134
 0.13643045 0.13581983 0.13069191 0.16368114 0.14391078 0.13539247
 0.15997265 0.14973391 0.14281212 0.18202763 0.16663474 0.15810435
 0.13187732 0.13335062 0.15440067 0.138855   0.11227389 0.1695774
 0.13327057 0.12492622 0.14103137 0.14589367 0.17809675 0.14233009
 0.19733321 0.1686151  0.14576205 0.15299403 0.12374415 0.12128814
 0.15453155 0.15943906 0.15051532 0.15255703 0.11687368 0.15093466
 0.13665701 0.13083724 0.16899193 0.1326496  0.14410584 0.14395447
 0.14064206 0.12498217 0.17726559 0.1448684  0.12807256 0.1399045
 0.14557041 0.1077129  0.1327223  0.1386245  0.15964381 0.1538004
 0.15340963 0.14521358 0.14728038 0.12951112 0.15563541 0.14901172
 0.14946866 0.14431633 0.13343838 0.16397647 0.18156471 0.13163227
 0.1685059  0.16628834 0.13883548 0.13104158 0.13772435 0.15376187
 0.13767904 0.13132606 0.16439371 0.14016331 0.16349667 0.12923864
 0.14661872 0.12255977 0.14398003 0.14321239 0.16535742 0.09876918
 0.11574975 0.15178742 0.1498947  0.12349567 0.14748356 0.18358047
 0.14303936 0.14472725 0.20703479 0.14783135 0.13821837 0.1622382
 0.1391153  0.13908606 0.1367839  0.14369301 0.16737168 0.166179
 0.12987423 0.14304449 0.15484373 0.15028346 0.14653659 0.17092723
 0.13304548 0.14796437 0.14438479 0.12433393 0.15427991 0.16674082
 0.13823738 0.15722159 0.1008362  0.15723268 0.17342127 0.12771465
 0.15908902 0.14365484 0.15669196 0.16102325 0.12745093 0.13903461
 0.13834403 0.15063076 0.17078274 0.1313638  0.1652932  0.16090584
 0.12202948 0.1460892  0.14785506 0.15651688 0.17077488 0.1413409
 0.15968102 0.15441168 0.14341699 0.17374487 0.16689304 0.13581743
 0.12417074 0.15480826 0.1188883  0.14493861 0.11041764 0.12530873
 0.15879029 0.10399985 0.16521894 0.13455995 0.14560109 0.16841695
 0.14142708 0.14586299 0.15209389 0.12155946 0.12592097 0.13724752
 0.15779061 0.12906337 0.16876157 0.14505049 0.1553185  0.12914326
 0.15514955 0.14586324 0.13288067 0.15870532 0.13246278 0.13962546
 0.17206513 0.1328539  0.13294382 0.13366086 0.14125054 0.13795899
 0.13124093 0.1445129  0.14419692 0.13709983 0.13268782 0.10850932
 0.12402094 0.14863134]
charlie_repeat-2
BERT
[0.7849069  0.78375745 0.78506154 0.7817635  0.7835254  0.7840298
 0.79069954 0.7814626  0.7809828  0.7826313  0.78359556 0.78378695
 0.78927743 0.7831618  0.78288734 0.7868388  0.78423786 0.782169
 0.78725106 0.79243237 0.78697324 0.79318523 0.791451   0.7839789
 0.78393734 0.78280324 0.7788405  0.7774918  0.7798448  0.78347266
 0.78412914 0.7808875  0.7832623  0.78169894 0.78436476 0.78274584
 0.78455156 0.7876812  0.7836135  0.7830988  0.7822272  0.7841442
 0.7835522  0.78308517 0.7818287  0.7862635  0.7874273  0.7890522
 0.7857472  0.78411067 0.7833249  0.7810602  0.7863879  0.78085846
 0.7813533  0.78221166 0.7858054  0.7856369  0.7879947  0.78050685
 0.7865858  0.7853775  0.7827538  0.78363454 0.78849113 0.79196036
 0.79250056 0.7886837  0.7822902  0.7806897  0.7836164  0.7827079
 0.7821433  0.78704125 0.78414845 0.78590214 0.78779256 0.7890328
 0.7873389  0.7924665  0.78727937 0.7838221  0.7835563  0.785924
 0.782335   0.7859817  0.78785425 0.78032255 0.7832757  0.78320473
 0.78313196 0.78161323 0.7832096  0.79017293 0.78428054 0.7845553
 0.779241   0.786059   0.78220665 0.78447527 0.78884494 0.7911455
 0.78201824 0.78834516 0.78919154 0.78620994 0.78725505 0.7832223
 0.7870086  0.7843362  0.78149277 0.78946054 0.7867013  0.7824147
 0.78252083 0.7818617  0.78852993 0.7884158  0.78936946 0.78371394
 0.7844689  0.782694   0.78625077 0.7817106  0.78063154 0.7915659
 0.7852291  0.78442067 0.7869526  0.78417045 0.78603595 0.783509
 0.7904559  0.7893504  0.78750014 0.7838549  0.7818882  0.7844952
 0.7876013  0.7863834  0.7804696  0.7893383  0.7838722  0.7829213
 0.7863489  0.7872411  0.78163093 0.7842095  0.7824027  0.7827319
 0.7858877  0.78706175 0.79245955 0.7864156  0.78967714 0.7835942
 0.78254604 0.79068774 0.7780479  0.781327   0.7857332  0.7801147
 0.78616947 0.7832395  0.7932011  0.78342235 0.78170675 0.7827535
 0.7859147  0.7803628  0.7831519  0.78492576 0.78329784 0.7803253
 0.7846282  0.78513503 0.78739834 0.78267473 0.78608686 0.77921134
 0.78006184 0.7834786  0.784681   0.78225017 0.7811523  0.78314877
 0.7910227  0.7844991  0.7824606  0.78594095 0.7832381  0.78294075
 0.7913606  0.78534156 0.78352284 0.7830443  0.78539556 0.7848036
 0.78287303 0.7845862 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
delta_repeat-1
WER
[-0.20753589 -0.21749312 -0.20194671 -0.20424809 -0.19720538 -0.19695468
 -0.21064838 -0.20978455 -0.19963249 -0.22227633 -0.2274788  -0.20284108
 -0.21179993 -0.20704753 -0.21154457 -0.20041788 -0.18931208 -0.18361821
 -0.21137472 -0.2182023  -0.21674543 -0.20477784 -0.20048407 -0.14877218
 -0.21334198 -0.18627586 -0.22958718 -0.2032875  -0.21319841 -0.21356632
 -0.20906945 -0.18725393 -0.21610573 -0.22845341 -0.21969915 -0.18054425
 -0.20795456 -0.21646016 -0.20033576 -0.20070634 -0.22808764 -0.21317244
 -0.21608575 -0.22262589 -0.22433599 -0.19262869 -0.1995277  -0.22610668
 -0.22088925 -0.22334988 -0.20210447 -0.1945153  -0.18898493 -0.23424204
 -0.22718274 -0.19502746 -0.18907355 -0.2127421  -0.20707725 -0.20623996
 -0.23523251 -0.19473462 -0.23473068 -0.20707802 -0.20964265 -0.2162662
 -0.22963537 -0.19458237 -0.19195952 -0.23357666 -0.22158316 -0.2078592
 -0.17962632 -0.21553451 -0.20520474 -0.19045527 -0.18417131 -0.2028917
 -0.21329334 -0.21771346 -0.16417232 -0.2025004  -0.22321223 -0.19798284
 -0.21605549 -0.19548562 -0.20414442 -0.19591191 -0.20850417 -0.20793806
 -0.21029966 -0.20493715 -0.22645009 -0.23796736 -0.19430416 -0.23016605
 -0.20103006 -0.20691553 -0.21441623 -0.20489364 -0.19483766 -0.23102713
 -0.20756235 -0.22104331 -0.18516357 -0.19795315 -0.20530417 -0.20133962
 -0.22942001 -0.236973   -0.22173534 -0.21763304 -0.20439169 -0.20777054
 -0.21032353 -0.21767229 -0.21056795 -0.22545417 -0.21765163 -0.23047699
 -0.21925495 -0.19221904 -0.21206647 -0.21504596 -0.19811756 -0.22043502
 -0.18900408 -0.20154004 -0.19861462 -0.20624778 -0.20018321 -0.23716857
 -0.22171935 -0.1903145  -0.21375267 -0.21799372 -0.20036315 -0.20366983
 -0.22842571 -0.20663642 -0.20228637 -0.21054468 -0.19343077 -0.18609888
 -0.19440056 -0.23350096 -0.18254048 -0.23097385 -0.18215166 -0.21046242
 -0.23373835 -0.20169356 -0.20067888 -0.20040356 -0.19492202 -0.21206415
 -0.20187891 -0.19384883 -0.20706126 -0.2205466  -0.19177992 -0.21237016
 -0.20627569 -0.20484521 -0.22890052 -0.21163001 -0.18435205 -0.20408896
 -0.20671641 -0.20329363 -0.20050826 -0.22499394 -0.19390337 -0.21629653
 -0.20201779 -0.21048971 -0.21161785 -0.21650504 -0.21561362 -0.18241929
 -0.20019804 -0.21264002 -0.20983583 -0.19953226 -0.2083239  -0.2039645
 -0.20792727 -0.18822829 -0.18286804 -0.19758813 -0.21564311 -0.19807058
 -0.21809698 -0.19986757 -0.2357389  -0.21855883 -0.22779785 -0.23235336
 -0.20140545 -0.22050653]
delta_repeat-1
BLEU
[0.1761407  0.16402181 0.19203405 0.19913879 0.16812567 0.14652887
 0.15819167 0.18767466 0.21203239 0.19045668 0.1579721  0.19042274
 0.15937726 0.20356889 0.20701919 0.21906203 0.19681699 0.20841373
 0.14112517 0.20789266 0.1524682  0.18020149 0.19189425 0.22356837
 0.19517649 0.19544373 0.1542058  0.16456215 0.17938675 0.1834486
 0.20573256 0.2211283  0.1714952  0.19893826 0.16639196 0.19451508
 0.18501419 0.20518559 0.18033233 0.1988345  0.12967491 0.16100538
 0.20607439 0.18939645 0.16492826 0.1701526  0.22108214 0.14545593
 0.15198302 0.13801087 0.20859926 0.17801507 0.20453928 0.14608133
 0.15954744 0.19262676 0.20256194 0.19869954 0.21499801 0.16175184
 0.17246473 0.22208643 0.18511081 0.16486737 0.15489585 0.18575004
 0.15355543 0.22012808 0.22091779 0.1739107  0.18235429 0.20647836
 0.19976816 0.17520806 0.20055847 0.23044497 0.17818435 0.17653691
 0.16122556 0.1713701  0.20742923 0.21165185 0.14340716 0.17809871
 0.18819567 0.21903116 0.19832754 0.24592978 0.16557908 0.18004209
 0.13983996 0.19901665 0.1493008  0.14074647 0.1980299  0.12490938
 0.18872034 0.18313304 0.21110947 0.1868903  0.21521104 0.1352477
 0.14233309 0.16891062 0.22181872 0.18348126 0.23596745 0.18014708
 0.17057447 0.16005893 0.18185388 0.16942163 0.22700711 0.20287975
 0.15489529 0.18211128 0.17072595 0.14744422 0.17477924 0.16501169
 0.13322075 0.2234251  0.14350793 0.19253862 0.20277822 0.16743268
 0.20729552 0.18740867 0.18335659 0.20059574 0.19074126 0.13948636
 0.17259267 0.20989969 0.14393053 0.15641354 0.17532233 0.19894592
 0.17787963 0.16631921 0.18999139 0.17320248 0.18585988 0.21606747
 0.18534638 0.17873711 0.22345531 0.18334805 0.21093663 0.15678891
 0.14426314 0.18111614 0.18508942 0.20589094 0.17807207 0.18798074
 0.20123285 0.19874805 0.17394231 0.15387135 0.17739678 0.19489703
 0.22620272 0.19131443 0.15841046 0.17870618 0.2364252  0.17426686
 0.18339816 0.23212176 0.19891185 0.18919711 0.19302447 0.17464448
 0.19591786 0.193585   0.19650133 0.16651773 0.16481101 0.22592136
 0.1885181  0.18548193 0.17038029 0.17775282 0.20593882 0.15761794
 0.14999275 0.20234239 0.18591973 0.21679828 0.1650017  0.18844034
 0.16544636 0.19833094 0.11549042 0.17013106 0.16403272 0.13852411
 0.19162839 0.20232952]
delta_repeat-1
METEOR
[0.16928497 0.14641284 0.15727649 0.16483171 0.14310901 0.15136811
 0.15433861 0.15230033 0.16417219 0.14744115 0.14235909 0.15369702
 0.14814586 0.17652808 0.16655612 0.1827291  0.17240963 0.17474793
 0.14211976 0.162899   0.14638621 0.16626909 0.1496889  0.18221469
 0.15237241 0.17472873 0.14730759 0.16095259 0.15575142 0.16071103
 0.18970403 0.17345049 0.14821229 0.17044545 0.13596511 0.19416489
 0.15551175 0.18215853 0.16685336 0.15977726 0.10497739 0.12824063
 0.17850804 0.16477434 0.13016899 0.14678118 0.18981062 0.13573959
 0.12767835 0.12713936 0.15763477 0.15847439 0.18121872 0.14394433
 0.14755723 0.17293475 0.18023461 0.19353467 0.17057423 0.14746762
 0.13667143 0.19124703 0.13831638 0.13750336 0.15716962 0.16600094
 0.13365382 0.17799479 0.1679345  0.14389483 0.16728069 0.16664519
 0.16645356 0.16030931 0.19241118 0.18560833 0.15217552 0.15068308
 0.15322769 0.16018589 0.16981549 0.18637888 0.13270386 0.15592661
 0.16098824 0.16952898 0.18300307 0.23136823 0.1397619  0.15704096
 0.12023801 0.15240627 0.13139199 0.13938613 0.14719362 0.12418881
 0.15415449 0.17416034 0.17740708 0.15466451 0.16123662 0.13446779
 0.13845704 0.14884921 0.20341919 0.17683938 0.19577221 0.16247619
 0.15341688 0.15083003 0.17034972 0.14485885 0.18387223 0.17597886
 0.14236278 0.14289076 0.14605166 0.13131661 0.1680524  0.15210941
 0.11123591 0.18447497 0.13859982 0.15861812 0.18055014 0.14597764
 0.16496569 0.15310907 0.14058402 0.1792246  0.17428379 0.12046685
 0.13865243 0.18795139 0.12642704 0.12552195 0.16319721 0.17698233
 0.1510276  0.15697358 0.15504954 0.15579784 0.15100252 0.16396862
 0.18596644 0.144538   0.18550892 0.15510745 0.18208876 0.1268921
 0.10369582 0.15808749 0.16925885 0.1833358  0.16261864 0.16716471
 0.15063598 0.19459773 0.1553605  0.15365489 0.1588928  0.16805088
 0.18482086 0.16851309 0.14914709 0.17452193 0.18952607 0.16336491
 0.16620391 0.17693786 0.18666612 0.16135685 0.16007108 0.1371153
 0.16017979 0.1628435  0.15233924 0.13321535 0.14332464 0.18749647
 0.17879616 0.17798025 0.1522972  0.13717617 0.16427269 0.14412005
 0.13862623 0.17865759 0.13396236 0.16998397 0.13276505 0.15588596
 0.12942073 0.16710613 0.11426592 0.13769305 0.13478835 0.13447959
 0.18572086 0.19450196]
delta_repeat-1
BERT
[0.79950976 0.8013606  0.80678296 0.8061195  0.80459046 0.7925859
 0.7969805  0.80243236 0.80704683 0.80215806 0.7977321  0.80600905
 0.7942938  0.80012363 0.8029159  0.80976903 0.8070725  0.79808563
 0.7995129  0.79908085 0.79725283 0.8038308  0.79828876 0.8029017
 0.798315   0.801603   0.7957803  0.79889584 0.8014649  0.8055118
 0.80431056 0.81173867 0.80047256 0.80461305 0.7976116  0.80430585
 0.8055762  0.80478334 0.8010509  0.79989606 0.7957341  0.7979809
 0.7979057  0.80280656 0.7948406  0.7953212  0.8005652  0.7996574
 0.79689735 0.7941573  0.81327987 0.8000777  0.7994664  0.79555607
 0.7958485  0.8073641  0.80956185 0.805256   0.80700797 0.79452145
 0.7958596  0.8061049  0.8036207  0.79225606 0.7978242  0.7930604
 0.7952     0.8046944  0.8088901  0.7945542  0.79500186 0.80128217
 0.8022035  0.8016561  0.80812585 0.80791026 0.7944837  0.79766196
 0.8013953  0.8063061  0.80415124 0.81413287 0.8013128  0.8056419
 0.796827   0.80702543 0.7988874  0.8123471  0.79962426 0.80024457
 0.79819554 0.7979339  0.80096453 0.7944558  0.79859906 0.7957749
 0.80634975 0.80239606 0.79956365 0.80415577 0.8016774  0.79671514
 0.79646397 0.80254334 0.8061041  0.8052115  0.8048922  0.80550134
 0.79505336 0.79786366 0.79444915 0.8075586  0.8072317  0.79703975
 0.797046   0.80342317 0.79900974 0.7957976  0.8046363  0.79563636
 0.7977484  0.8110955  0.7945716  0.803396   0.80760026 0.7960649
 0.80485374 0.7984019  0.79624    0.79768467 0.80146706 0.79146063
 0.80206627 0.8058727  0.79548496 0.7924612  0.79725015 0.8007298
 0.80494046 0.8010594  0.7984553  0.7985948  0.80360377 0.80238855
 0.79668    0.80794555 0.80047596 0.8030227  0.8067463  0.80021846
 0.7921494  0.79513305 0.7995914  0.80507225 0.7937992  0.8034502
 0.7915391  0.80893177 0.8002649  0.7970432  0.7991225  0.8102236
 0.80986094 0.79934925 0.7938471  0.80296606 0.804449   0.8027898
 0.80160606 0.81348747 0.808304   0.79820085 0.8041179  0.8057709
 0.80473846 0.8012811  0.8030158  0.79737103 0.79772466 0.81121194
 0.80398595 0.801488   0.8035752  0.7988902  0.80791694 0.7971945
 0.7961724  0.8078802  0.8028516  0.80603373 0.8045074  0.8036426
 0.7961423  0.8133012  0.7885504  0.79533273 0.8001964  0.795128
 0.7977101  0.8013898 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
delta_repeat-2
WER
[-0.30279743 -0.29338259 -0.29228    -0.31220625 -0.31198623 -0.29240965
 -0.29378968 -0.27939842 -0.31219263 -0.31337535 -0.30017875 -0.2839271
 -0.31960743 -0.31822271 -0.28347167 -0.32648363 -0.30609836 -0.3167657
 -0.31789894 -0.28097035 -0.30009449 -0.31047442 -0.29411783 -0.33213099
 -0.30640587 -0.3141571  -0.33862501 -0.33311777 -0.35032861 -0.28715791
 -0.27928612 -0.28033573 -0.30439241 -0.30162697 -0.29290572 -0.31875026
 -0.32730104 -0.32362425 -0.31602753 -0.30611613 -0.30658783 -0.32675012
 -0.28438026 -0.29744536 -0.30929785 -0.31570131 -0.31985886 -0.29447346
 -0.31195988 -0.32508852 -0.34303056 -0.30998455 -0.31892021 -0.3143041
 -0.29230349 -0.31456742 -0.32090043 -0.3143667  -0.29771157 -0.31762315
 -0.31607154 -0.31082352 -0.32829866 -0.29754248 -0.30922493 -0.31542654
 -0.30841714 -0.30456435 -0.29929785 -0.28723548 -0.30936116 -0.29896438
 -0.31104901 -0.30082714 -0.31071058 -0.29559027 -0.33538675 -0.29037755
 -0.31515635 -0.30538015 -0.31770846 -0.31791704 -0.31567074 -0.31104383
 -0.31364291 -0.32435602 -0.30186359 -0.30829961 -0.28319101 -0.29724075
 -0.31694273 -0.29775533 -0.28631706 -0.29182585 -0.32404397 -0.3114795
 -0.31680892 -0.30670698 -0.3180765  -0.29350077 -0.32959906 -0.31078073
 -0.28020743 -0.30933063 -0.31038538 -0.31232741 -0.30856473 -0.29514163
 -0.3413231  -0.32814226 -0.30794757 -0.29566598 -0.28583778 -0.28231719
 -0.30566113 -0.29732096 -0.33279242 -0.2669123  -0.32163357 -0.33119723
 -0.34544319 -0.33042149 -0.32653036 -0.31430866 -0.30194223 -0.30818156
 -0.32075964 -0.30942828 -0.29997596 -0.28982945 -0.30924936 -0.32852664
 -0.31939443 -0.27243495 -0.30313218 -0.3201682  -0.31326445 -0.31209481
 -0.31918794 -0.31335189 -0.2840651  -0.28847163 -0.30413367 -0.29860428
 -0.30181493 -0.29910388 -0.32422732 -0.28801774 -0.30422196 -0.33598187
 -0.31568569 -0.28166861 -0.29122489 -0.31693245 -0.29231685 -0.32111049
 -0.28948121 -0.26155751 -0.31570913 -0.30506423 -0.28860534 -0.28555223
 -0.31297412 -0.30895198 -0.29189971 -0.29182856 -0.31348506 -0.31960186
 -0.30525583 -0.31842305 -0.3052053  -0.30646503 -0.32453867 -0.30753205
 -0.30263885 -0.29722954 -0.32150868 -0.28019078 -0.33234813 -0.31494479
 -0.32161343 -0.27428156 -0.27863391 -0.29824114 -0.30323733 -0.30341621
 -0.3088191  -0.30705006 -0.31727042 -0.31692834 -0.30269575 -0.31133121
 -0.30559977 -0.32330418 -0.31068943 -0.31807945 -0.31806823 -0.31259036
 -0.31230621 -0.27547906]
delta_repeat-2
BLEU
[0.18492066 0.17571587 0.17120435 0.15223554 0.14919511 0.22013527
 0.19763909 0.20158173 0.1930403  0.15469003 0.17694498 0.17823145
 0.14234188 0.14551364 0.19091577 0.12718517 0.1689018  0.13337352
 0.1730742  0.20585322 0.17199656 0.18604792 0.14333661 0.12832049
 0.20016704 0.18170449 0.14336157 0.1104431  0.11258105 0.20358075
 0.17449809 0.21306302 0.19539897 0.18288634 0.21469266 0.18037986
 0.1563832  0.15549957 0.15996687 0.17529961 0.20248442 0.15353456
 0.20836987 0.2018676  0.21507912 0.16732697 0.17603157 0.1814671
 0.14587546 0.16618721 0.12498197 0.16336929 0.15678604 0.14044184
 0.18245054 0.13425016 0.16840164 0.17174349 0.22303942 0.15517104
 0.16705311 0.17983061 0.14559637 0.1793738  0.14728463 0.19977848
 0.18045729 0.18314577 0.1839176  0.16431711 0.17406295 0.17921
 0.16387164 0.147873   0.17948322 0.18778927 0.13290178 0.17608547
 0.17597249 0.17917428 0.16254427 0.19590156 0.20516475 0.16812632
 0.18357405 0.14932114 0.16909306 0.180721   0.20362794 0.18633098
 0.16751955 0.19237436 0.14900822 0.18527635 0.16808819 0.18914442
 0.16859249 0.15818464 0.18208077 0.17337765 0.13107292 0.15924761
 0.21049022 0.19013865 0.1791405  0.18134579 0.19060293 0.16166519
 0.13336312 0.15356854 0.18769746 0.20179686 0.20179934 0.18429995
 0.18362471 0.18638365 0.12730926 0.19260255 0.17691246 0.12910905
 0.11655752 0.15158232 0.16078069 0.14169304 0.18692056 0.16386745
 0.15554232 0.15613736 0.16394487 0.19665993 0.17440103 0.14572758
 0.16515198 0.18858829 0.16010557 0.17577523 0.15140787 0.16508459
 0.1794911  0.16512004 0.19307589 0.17974971 0.12512651 0.14295349
 0.19775805 0.18168212 0.15663456 0.1956519  0.17754957 0.13138094
 0.16117738 0.17709395 0.19373717 0.1846646  0.21478964 0.15483614
 0.21899312 0.17877154 0.14340843 0.15348938 0.18926227 0.20426833
 0.16623474 0.15106144 0.19867808 0.22106145 0.17097523 0.16402359
 0.20367404 0.20352346 0.19479561 0.18080079 0.13851495 0.15181684
 0.18516448 0.17483997 0.1545783  0.20870835 0.18431506 0.13074992
 0.13685503 0.17380988 0.20334199 0.17451864 0.19407816 0.20073186
 0.18372602 0.18683831 0.15450657 0.18905581 0.17459534 0.16651975
 0.14352654 0.15988262 0.17845655 0.18265518 0.17119322 0.14277596
 0.20252912 0.22771198]
delta_repeat-2
METEOR
[0.18253656 0.16062571 0.16707871 0.14425295 0.13177529 0.17860969
 0.18119721 0.18109025 0.16227606 0.15106235 0.16258753 0.16695425
 0.13663953 0.13828286 0.15934315 0.13432189 0.13524337 0.11893486
 0.15768904 0.19420887 0.1573758  0.17687753 0.14098164 0.1292896
 0.19257025 0.15521522 0.14478272 0.10529378 0.11944341 0.18431687
 0.15789167 0.18402963 0.17540853 0.16490557 0.18800805 0.15921878
 0.14646121 0.14298916 0.14561987 0.15981583 0.16329127 0.1551391
 0.18247885 0.1755761  0.19464376 0.14543244 0.14014195 0.17300495
 0.13693187 0.14917984 0.11998103 0.15299157 0.15299102 0.13392161
 0.15862512 0.11409483 0.1466496  0.14545325 0.19614006 0.13067079
 0.15292599 0.16691195 0.13974324 0.16751559 0.14133307 0.18509372
 0.1623916  0.18288149 0.15553564 0.1462869  0.16847333 0.17052007
 0.17726482 0.13244222 0.16646028 0.19833294 0.13626401 0.15759457
 0.17605812 0.16952121 0.16495021 0.18368928 0.17193052 0.1598946
 0.16858089 0.1471601  0.16312949 0.16150181 0.17928765 0.1546492
 0.15410014 0.17827405 0.13379855 0.16880222 0.1651114  0.16727374
 0.1533895  0.15686355 0.16569455 0.16032675 0.1347975  0.15117574
 0.18910866 0.17995096 0.15714581 0.1717202  0.1592037  0.14047274
 0.13032365 0.1485259  0.16065775 0.19173353 0.16426138 0.16307037
 0.16295908 0.17228086 0.13663294 0.15822644 0.16822085 0.13441433
 0.11552183 0.14047787 0.13592028 0.13930851 0.18333135 0.15143214
 0.15182523 0.14747029 0.14757612 0.16381017 0.16017801 0.12932753
 0.15352784 0.17896639 0.17264362 0.1489835  0.14139647 0.16656272
 0.13718513 0.12947189 0.16994116 0.1667872  0.13694844 0.13993354
 0.18888181 0.15989021 0.14953826 0.18257361 0.15456196 0.12762235
 0.13947832 0.19211986 0.1759417  0.18103373 0.18449545 0.14572867
 0.18872366 0.15682962 0.13145187 0.1605702  0.17193426 0.17947115
 0.1572965  0.13518109 0.18391585 0.21101125 0.16431385 0.13731764
 0.18271965 0.20156422 0.17608567 0.15516933 0.14098078 0.13082121
 0.15554361 0.15330931 0.15272255 0.17610947 0.16215867 0.1477078
 0.13495058 0.16796553 0.17422738 0.17797278 0.17153979 0.18015196
 0.17746384 0.16510469 0.13591421 0.14828846 0.14913426 0.1517615
 0.14223496 0.15119781 0.16781345 0.16761558 0.14499718 0.13630584
 0.16525223 0.20562898]
delta_repeat-2
BERT
[0.8087835  0.7951632  0.8048822  0.7940251  0.7987656  0.8084735
 0.80327046 0.8052595  0.81213295 0.7974069  0.8023462  0.8008605
 0.7985347  0.79318607 0.8042101  0.7994486  0.80480975 0.795398
 0.798025   0.8034487  0.8058201  0.8040818  0.795577   0.8002386
 0.8028208  0.80491203 0.79277664 0.7904952  0.7922746  0.8017577
 0.80710435 0.80545646 0.8033132  0.8016407  0.81227773 0.80998826
 0.7958805  0.8015833  0.7986213  0.80047643 0.79997474 0.8027701
 0.80200976 0.8032301  0.80340314 0.7975834  0.80046976 0.80014503
 0.79791707 0.79663736 0.7941352  0.8021538  0.79345685 0.79436207
 0.8060032  0.78899896 0.7976651  0.79536253 0.80585134 0.7973147
 0.7970865  0.8065034  0.79506534 0.7964527  0.7948965  0.8097058
 0.80827403 0.8094153  0.8077701  0.80011    0.80000585 0.8036401
 0.80184543 0.7940022  0.7997828  0.7989393  0.79505044 0.8005968
 0.80018634 0.80635    0.79697454 0.7988651  0.8118556  0.79837567
 0.80313605 0.7965021  0.80192417 0.81032616 0.801444   0.79724985
 0.80125314 0.80547464 0.7999543  0.7956715  0.7967916  0.8000586
 0.8001691  0.7943172  0.80375105 0.8012974  0.80052567 0.7945426
 0.8122219  0.8004172  0.7967219  0.80469346 0.8002744  0.8039228
 0.7940234  0.7964265  0.80008453 0.80073684 0.8039259  0.8077988
 0.8022383  0.8047111  0.79455334 0.8049731  0.801766   0.79426545
 0.7909561  0.79409593 0.79677397 0.79208773 0.8014442  0.7979415
 0.79969007 0.8020499  0.80170393 0.79981494 0.79841167 0.8004224
 0.7983231  0.80889416 0.8038367  0.797935   0.798407   0.79347825
 0.7994761  0.7991845  0.79802155 0.8087551  0.79401135 0.79418916
 0.81077075 0.8039722  0.79148686 0.80573916 0.80949605 0.7949028
 0.7925989  0.8071712  0.8025128  0.8093331  0.80750114 0.7900012
 0.8043481  0.8051045  0.79674923 0.8116337  0.8036512  0.808104
 0.79951406 0.7927273  0.80674    0.808232   0.8012285  0.79507846
 0.80400234 0.80490094 0.80592597 0.80619234 0.79543847 0.79498386
 0.8058721  0.79999405 0.7969194  0.8037848  0.80360067 0.79253155
 0.79312116 0.80034786 0.8082496  0.79536086 0.80949897 0.798796
 0.8029006  0.8044934  0.8044946  0.79211724 0.80047536 0.80429214
 0.8044608  0.7980755  0.7974611  0.8054893  0.7976705  0.7898113
 0.8060647  0.80753857]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
echo_repeat-1
WER
[-0.07667892 -0.07791741 -0.09236443 -0.06981139 -0.06678766 -0.07334754
 -0.07853831 -0.07340585 -0.06813037 -0.08311437 -0.07582762 -0.06978352
 -0.08550961 -0.06460924 -0.07975687 -0.08424052 -0.05391699 -0.07484521
 -0.07409001 -0.07100732 -0.07271167 -0.07641958 -0.0524905  -0.08706502
 -0.08535414 -0.05731229 -0.07783715 -0.06261829 -0.06839904 -0.06691238
 -0.06265874 -0.07117195 -0.06949685 -0.08382166 -0.08748455 -0.08896075
 -0.07372042 -0.05526555 -0.07853988 -0.07363185 -0.07807049 -0.08752906
 -0.06699782 -0.08204092 -0.08043988 -0.09513257 -0.06297323 -0.09641911
 -0.07465115 -0.08363806 -0.08900451 -0.07471585 -0.06600848 -0.07340159
 -0.0812393  -0.0836625  -0.09732223 -0.07404972 -0.0750372  -0.0880367
 -0.07392491 -0.08418505 -0.06752801 -0.06191593 -0.07460447 -0.08737637
 -0.09079405 -0.08990744 -0.0780446  -0.07545063 -0.07451401 -0.07136418
 -0.04893525 -0.0728702  -0.06367995 -0.05217866 -0.07728281 -0.07938797
 -0.07253254 -0.0764078  -0.06559655 -0.05751142 -0.08574854 -0.07865319
 -0.07781222 -0.08123334 -0.05664904 -0.0710362  -0.07127916 -0.05873389
 -0.05042971 -0.08933791 -0.07847578 -0.05409738 -0.0880923  -0.09043949
 -0.04726132 -0.08675688 -0.03844278 -0.08689304 -0.0786127  -0.06861077
 -0.0815764  -0.0635776  -0.08133052 -0.08394378 -0.08406016 -0.08131915
 -0.05856662 -0.0698339  -0.09073149 -0.07092535 -0.07611041 -0.07003951
 -0.08607493 -0.05277192 -0.07504737 -0.08577787 -0.06366249 -0.06881319
 -0.06704193 -0.07130191 -0.08767892 -0.06656832 -0.06785589 -0.08657676
 -0.08640825 -0.08376235 -0.08270937 -0.07315667 -0.08354836 -0.07102667
 -0.08117732 -0.06640305 -0.0935772  -0.07436536 -0.05908128 -0.06558044
 -0.07700569 -0.08005172 -0.08741528 -0.0716344  -0.08041806 -0.07712452
 -0.07917385 -0.08127516 -0.07503164 -0.08109723 -0.07904799 -0.08582828
 -0.07786916 -0.0611781  -0.07874769 -0.07289298 -0.07818325 -0.0863817
 -0.09513365 -0.07867681 -0.07014061 -0.0849132  -0.06686149 -0.0699434
 -0.08870915 -0.06096073 -0.05755201 -0.07182372 -0.08245066 -0.07714498
 -0.08019306 -0.07683556 -0.06787151 -0.06061316 -0.07025981 -0.0897591
 -0.0758056  -0.06434794 -0.06841545 -0.09948627 -0.06724308 -0.08084268
 -0.08135093 -0.07663271 -0.08733891 -0.07883019 -0.08936377 -0.07919396
 -0.06783263 -0.09046537 -0.08370943 -0.06895154 -0.0893567  -0.06886379
 -0.07429111 -0.07662954 -0.0972991  -0.05334049 -0.07437678 -0.09279941
 -0.07698067 -0.07894454]
echo_repeat-1
BLEU
[0.15803423 0.15948395 0.12219899 0.1857427  0.16427886 0.13193472
 0.13515948 0.15251201 0.13072174 0.14934317 0.15028508 0.18947745
 0.16527131 0.17288846 0.16052872 0.1487082  0.18277453 0.13675846
 0.18626385 0.15156985 0.14638152 0.11270779 0.16773878 0.12249886
 0.14300122 0.17849028 0.18309383 0.20121912 0.14831652 0.14769336
 0.16587994 0.1625842  0.18141552 0.16138772 0.15598782 0.13201442
 0.16327034 0.16974117 0.11715543 0.15876591 0.14017062 0.15713521
 0.15537544 0.15106233 0.17852782 0.10080736 0.18338353 0.11379719
 0.15927149 0.12413284 0.14254298 0.14456506 0.15971082 0.16703184
 0.13899278 0.16803745 0.11670223 0.17884786 0.14369481 0.13662773
 0.17385442 0.13255859 0.18533647 0.15456589 0.16322333 0.13196101
 0.14294905 0.1369575  0.18431922 0.15288858 0.16746939 0.16904537
 0.19129314 0.15284686 0.18850284 0.16548697 0.15583975 0.14324546
 0.16675973 0.15753448 0.20670889 0.16848678 0.19625148 0.1451322
 0.16005465 0.1265577  0.19199044 0.16450628 0.18760447 0.17707928
 0.18302322 0.16824791 0.15700614 0.17692275 0.12703013 0.12724531
 0.19010175 0.14366788 0.21122193 0.13579349 0.15651333 0.15887572
 0.14753931 0.14125325 0.15330821 0.13550813 0.15560002 0.1666447
 0.15317194 0.16267954 0.13024045 0.16201313 0.17742448 0.15739281
 0.12526616 0.21289363 0.17630086 0.14962178 0.16913135 0.16178884
 0.14393091 0.14711204 0.14128796 0.20250348 0.17314537 0.15234989
 0.16084199 0.15817225 0.17647775 0.15934816 0.14719112 0.16959795
 0.13946447 0.19354286 0.12674932 0.13577636 0.15995014 0.16373064
 0.13773083 0.16584575 0.12855594 0.19119484 0.16995327 0.16367946
 0.11730722 0.14255381 0.12823322 0.1465381  0.13554326 0.15149348
 0.18964292 0.1622969  0.19776088 0.15454244 0.14006262 0.14393767
 0.14427551 0.14659892 0.17154737 0.14350737 0.1513129  0.16160247
 0.10293663 0.17491456 0.15607404 0.18120622 0.1279897  0.18209608
 0.15014349 0.15088815 0.18363893 0.18140724 0.16535583 0.16571268
 0.12958179 0.17545552 0.17271798 0.12829313 0.16840402 0.15905326
 0.13531419 0.14650666 0.13030664 0.15268367 0.16938314 0.12924679
 0.16753164 0.15068021 0.10527678 0.1780117  0.12648774 0.20413534
 0.17411461 0.15721778 0.12857328 0.18351332 0.16817529 0.15417471
 0.16535658 0.1514202 ]
echo_repeat-1
METEOR
[0.10356841 0.10269703 0.08696241 0.12083489 0.10533452 0.09875138
 0.08290276 0.10517357 0.08526097 0.11029246 0.10601357 0.11591597
 0.12232227 0.11563396 0.11876404 0.11686796 0.10892838 0.09632824
 0.11557234 0.09538941 0.09454145 0.07920995 0.11809653 0.08397529
 0.09480014 0.13607544 0.10830837 0.13134227 0.09708914 0.10411066
 0.10481923 0.11214522 0.11411646 0.10316418 0.10104355 0.10341956
 0.12692619 0.11697869 0.08794088 0.10484165 0.10393292 0.12000849
 0.10218223 0.09016565 0.11041466 0.07869948 0.1108423  0.07678112
 0.0999637  0.09032464 0.09471175 0.09704578 0.10581877 0.11455281
 0.08449838 0.10915167 0.07916291 0.09862461 0.09847665 0.10259199
 0.12317072 0.08615653 0.113114   0.0925196  0.10597865 0.08362875
 0.0915648  0.09480356 0.1161066  0.10633298 0.10441606 0.11325537
 0.13453508 0.10231795 0.11770628 0.104837   0.10211852 0.09099307
 0.107705   0.09723428 0.14309057 0.11172601 0.15143143 0.10273543
 0.10665038 0.07981959 0.13551891 0.11495998 0.10769463 0.12184798
 0.11850786 0.11250573 0.10607761 0.13165668 0.09382802 0.08302034
 0.12606895 0.08484762 0.1337343  0.10904784 0.09727093 0.12025694
 0.10677282 0.09743295 0.09828594 0.09775215 0.1310808  0.14324006
 0.10760769 0.11016709 0.08959654 0.10109509 0.10276771 0.10703814
 0.09457378 0.14078124 0.11378702 0.11022254 0.10951191 0.11550001
 0.09992123 0.12180967 0.09384431 0.13325507 0.10561291 0.09258125
 0.11000063 0.11762869 0.11752192 0.10262154 0.09636609 0.11221294
 0.09638581 0.12610138 0.09030198 0.08655148 0.11440427 0.10359027
 0.08745852 0.10928365 0.08361752 0.14041646 0.10338553 0.10629867
 0.08753246 0.09670598 0.08706851 0.11129899 0.10787944 0.098706
 0.13471988 0.09884029 0.14763212 0.10953709 0.09348545 0.09231662
 0.0942672  0.10758554 0.11292532 0.11714071 0.10045564 0.11173156
 0.08077794 0.11207506 0.10112241 0.11994954 0.08935953 0.11443183
 0.11690744 0.11078831 0.11542148 0.13909837 0.11347065 0.10212745
 0.09093598 0.11017311 0.12776881 0.08429921 0.11332544 0.11405247
 0.09164412 0.09728432 0.0847254  0.09392874 0.10101686 0.09083736
 0.12691627 0.09887549 0.08149831 0.11281938 0.08439511 0.14311529
 0.11066221 0.10359861 0.07718861 0.11913193 0.10485636 0.10989232
 0.10772128 0.10864509]
echo_repeat-1
BERT
[0.78268915 0.7809427  0.77748406 0.78181976 0.7826658  0.7782334
 0.77827483 0.7825339  0.7801064  0.7765008  0.78100175 0.7826431
 0.7837584  0.784074   0.77960783 0.78190744 0.7811196  0.77702683
 0.7819225  0.78102475 0.7858714  0.78117657 0.7842741  0.7776836
 0.78099674 0.77862513 0.78268546 0.78451824 0.7859134  0.7823319
 0.7824337  0.7805433  0.7814646  0.78550965 0.782988   0.7798515
 0.7839475  0.77843505 0.7777499  0.7832948  0.7751929  0.7814362
 0.7827057  0.7852141  0.7836526  0.77695477 0.78631926 0.77836967
 0.7823473  0.78229225 0.7800258  0.7772566  0.7830252  0.7801262
 0.78476983 0.7826823  0.7774081  0.783327   0.78005445 0.78170824
 0.78168404 0.7826011  0.78889644 0.781051   0.78164625 0.78111863
 0.77731586 0.78058493 0.7819052  0.7774312  0.7840573  0.7783906
 0.7850649  0.78666085 0.78139067 0.77916723 0.78554803 0.7792154
 0.78546274 0.77803904 0.7809428  0.7838193  0.77862453 0.7816579
 0.7791289  0.77716434 0.7893205  0.7817141  0.77876925 0.7835619
 0.78016526 0.78188455 0.7785491  0.778156   0.7809693  0.779304
 0.78090864 0.78631806 0.7860831  0.77464527 0.7802109  0.7820046
 0.78099775 0.78100216 0.78276116 0.7833867  0.7829248  0.7794885
 0.7840287  0.77820325 0.775574   0.7809196  0.7810598  0.77871394
 0.7789692  0.78411174 0.7851421  0.7823427  0.77674645 0.77946615
 0.7800898  0.7855714  0.77829117 0.7833705  0.78130376 0.7811853
 0.7805408  0.7837729  0.7795981  0.7806101  0.77646327 0.7802293
 0.77962905 0.7869872  0.7786884  0.78257173 0.7825581  0.7840304
 0.7792822  0.781902   0.77848005 0.78429747 0.779735   0.7795947
 0.782041   0.780388   0.78143597 0.7833735  0.7819272  0.7776793
 0.7840268  0.78142875 0.78323585 0.78070664 0.77871037 0.78082985
 0.7812928  0.7804527  0.7850114  0.7796941  0.77934283 0.7887865
 0.77309996 0.7855754  0.780506   0.78323317 0.77969295 0.78177243
 0.7860492  0.78294885 0.7854531  0.78428984 0.78167737 0.78266597
 0.78200847 0.78511757 0.7833799  0.7831607  0.782643   0.7840493
 0.78133076 0.78193545 0.77924305 0.77547014 0.7790893  0.784447
 0.78351283 0.7783914  0.7766661  0.7830389  0.7809973  0.78197694
 0.7800535  0.78280205 0.7800707  0.78205657 0.7802429  0.7838656
 0.78356755 0.7807475 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
echo_repeat-2
WER
[-0.06665957 -0.07727954 -0.08541503 -0.06307873 -0.06478114 -0.08370367
 -0.07784853 -0.05511864 -0.06910422 -0.07183815 -0.05332986 -0.05769247
 -0.05546253 -0.0726381  -0.04456829 -0.06660924 -0.07559703 -0.07515331
 -0.07220002 -0.05601137 -0.05965423 -0.07165305 -0.07042001 -0.05797975
 -0.06678717 -0.06435846 -0.07672975 -0.07891672 -0.07212833 -0.06964262
 -0.04103457 -0.06334798 -0.05156481 -0.05372707 -0.06990318 -0.07815749
 -0.07130172 -0.06339756 -0.07116825 -0.06920433 -0.07378242 -0.07960542
 -0.06551906 -0.07301527 -0.06897221 -0.05602094 -0.06141622 -0.05879008
 -0.05980228 -0.05665265 -0.07487651 -0.05826842 -0.07762532 -0.07055764
 -0.0686446  -0.04927932 -0.06719584 -0.07237051 -0.06622662 -0.06755134
 -0.07108003 -0.07531088 -0.0650405  -0.07756777 -0.08103075 -0.07210489
 -0.0488863  -0.07118455 -0.06248519 -0.04019557 -0.06899866 -0.0620124
 -0.05962853 -0.07557505 -0.06562688 -0.06732317 -0.06220333 -0.08384373
 -0.06826403 -0.06503008 -0.06585654 -0.05159036 -0.06641224 -0.05689067
 -0.06669262 -0.06575895 -0.07596781 -0.064171   -0.04946227 -0.07048888
 -0.07294583 -0.06953766 -0.07019391 -0.05258848 -0.07104267 -0.06805873
 -0.05803345 -0.05630622 -0.07279632 -0.05980425 -0.06291048 -0.06761554
 -0.06523844 -0.05744801 -0.05444063 -0.05134623 -0.06867659 -0.0748819
 -0.04002218 -0.05344168 -0.07756928 -0.05744591 -0.0803364  -0.08261053
 -0.05516469 -0.04326592 -0.05877229 -0.05694168 -0.05907622 -0.0716065
 -0.06229842 -0.04925232 -0.07507347 -0.06169399 -0.0595539  -0.0643596
 -0.07476098 -0.06618686 -0.05249786 -0.06852639 -0.06511847 -0.07646416
 -0.06456504 -0.06135054 -0.061205   -0.07140086 -0.07905114 -0.07653664
 -0.04556742 -0.05503927 -0.07434956 -0.04008055 -0.07674906 -0.06456797
 -0.07166896 -0.06407993 -0.06293957 -0.05401572 -0.07646531 -0.08443189
 -0.05710553 -0.07459589 -0.0584038  -0.04747524 -0.06071989 -0.06283338
 -0.05454866 -0.07228058 -0.05289599 -0.06735269 -0.07073079 -0.07042713
 -0.06790363 -0.07437416 -0.06338515 -0.05509438 -0.07480954 -0.06526869
 -0.07621444 -0.06119619 -0.05928066 -0.07311632 -0.06494151 -0.07132117
 -0.08038415 -0.0560884  -0.07476803 -0.07239303 -0.06124171 -0.04658843
 -0.04931553 -0.06020705 -0.05618306 -0.08118873 -0.06052438 -0.06960622
 -0.05567036 -0.07037893 -0.06775561 -0.06492231 -0.07313917 -0.07366529
 -0.05152631 -0.05405609 -0.0597236  -0.07628701 -0.05673722 -0.0663731
 -0.05500395 -0.06707012]
echo_repeat-2
BLEU
[0.17720823 0.14233288 0.13746761 0.16007951 0.15553163 0.13630814
 0.1450261  0.202926   0.16604395 0.15358483 0.13911734 0.15596122
 0.17004953 0.17591304 0.14965227 0.15209103 0.14673374 0.14156468
 0.16670255 0.14281288 0.17525466 0.16189431 0.15143183 0.15811192
 0.15565888 0.16982429 0.17897433 0.16508943 0.17008117 0.1651371
 0.16782275 0.14287567 0.17516443 0.16067016 0.15654238 0.14363444
 0.12879802 0.18149802 0.15690982 0.18531419 0.1664562  0.17217234
 0.14642667 0.12966833 0.1569866  0.18459131 0.18522015 0.19658595
 0.17210771 0.16352842 0.1422075  0.15895129 0.14423121 0.19460653
 0.15983397 0.16960505 0.13514388 0.16703643 0.14540757 0.16642395
 0.13469509 0.14147238 0.16826801 0.12136991 0.1168722  0.16135751
 0.20068588 0.14943137 0.20259542 0.18676657 0.15783504 0.17581276
 0.15506826 0.11789192 0.13787493 0.18429665 0.14606598 0.18109065
 0.14653746 0.18048018 0.15516467 0.15731298 0.1589112  0.15176
 0.16437857 0.14090951 0.154761   0.16357236 0.20071729 0.14456739
 0.17508165 0.15413248 0.1623043  0.17534306 0.18957713 0.16381597
 0.21979112 0.20061094 0.12474638 0.17027804 0.16382434 0.15566363
 0.1186513  0.17550665 0.16534519 0.1950365  0.14284418 0.14822156
 0.19281426 0.1773938  0.17412131 0.17659186 0.13272144 0.13167813
 0.17206086 0.19076911 0.19524036 0.14755444 0.14098804 0.1670927
 0.18641221 0.20932703 0.16724271 0.17498476 0.15502613 0.16609163
 0.15145924 0.15785847 0.17866237 0.16486019 0.1403614  0.17421347
 0.12249521 0.17836057 0.15449112 0.15839095 0.10784642 0.17819784
 0.16292859 0.1649192  0.17334388 0.18149527 0.14156573 0.14877878
 0.15741475 0.20378471 0.16802601 0.1503598  0.13415899 0.13845278
 0.18872897 0.17384622 0.18910095 0.17485643 0.16040694 0.19222623
 0.19186237 0.14928035 0.14352521 0.15496214 0.18039623 0.16520922
 0.15146418 0.16876076 0.13772824 0.1553974  0.14396853 0.16136382
 0.18846373 0.18499933 0.1383041  0.15753973 0.18180581 0.14516494
 0.18755973 0.16597816 0.16446187 0.18414873 0.19837009 0.16641213
 0.17405622 0.16940288 0.19742794 0.13671083 0.13094444 0.15913376
 0.19142548 0.1530267  0.17002664 0.18048563 0.16190115 0.11199897
 0.15768079 0.18051821 0.16568145 0.15927501 0.16538915 0.1675236
 0.18756717 0.16768251]
echo_repeat-2
METEOR
[0.11177309 0.09401796 0.10067769 0.11459    0.10876598 0.08284932
 0.10295141 0.12200963 0.12266094 0.11715841 0.10905644 0.09644909
 0.11647591 0.12123263 0.11214064 0.09963766 0.0864305  0.10031475
 0.10223673 0.11359228 0.12217923 0.10343941 0.10107566 0.11023162
 0.09710794 0.12290067 0.12943109 0.11696776 0.1093086  0.10905184
 0.11200038 0.10523481 0.12466956 0.10888541 0.10236138 0.11462356
 0.09561348 0.110856   0.13083043 0.12155735 0.10406806 0.11513463
 0.09335365 0.0903534  0.10084591 0.10726902 0.12613828 0.12411079
 0.1286761  0.10737128 0.10976076 0.10256218 0.0932496  0.11442809
 0.10242226 0.10991827 0.10645999 0.09665161 0.0931169  0.10997668
 0.08771288 0.08707562 0.12095666 0.089922   0.07848831 0.10726342
 0.1278002  0.10348385 0.14045184 0.11869162 0.10652565 0.10941062
 0.09567823 0.08940319 0.08002452 0.1115601  0.09072676 0.11495554
 0.1013164  0.12296992 0.10153992 0.1047858  0.09658411 0.11239772
 0.11212332 0.10314875 0.10585977 0.10036662 0.14673745 0.10345348
 0.11783721 0.12311773 0.09862872 0.12859455 0.11322967 0.09889787
 0.12952282 0.14626214 0.08627481 0.12372022 0.12124635 0.09980246
 0.09218477 0.10989898 0.11180393 0.13636935 0.11873562 0.10093145
 0.12277861 0.12248462 0.11820664 0.1094435  0.10582118 0.10280282
 0.11136062 0.13925575 0.13514883 0.0942373  0.09860715 0.11438539
 0.12170256 0.14066673 0.11595401 0.12173452 0.10081914 0.10908882
 0.10570603 0.10769008 0.12290837 0.11737317 0.08555794 0.1103516
 0.08323191 0.10864593 0.10146597 0.09927189 0.07499135 0.12184732
 0.10186902 0.10929893 0.13232984 0.12647254 0.08993182 0.09580971
 0.10320888 0.12430619 0.11097197 0.10904111 0.08517411 0.08620135
 0.11923849 0.129403   0.12205299 0.13662026 0.10406348 0.11830575
 0.12335909 0.09826821 0.09323264 0.09349822 0.11563545 0.11155798
 0.10361355 0.1187176  0.10334465 0.0967748  0.08770324 0.10969025
 0.13878206 0.11346789 0.09533836 0.10571593 0.10953518 0.10437534
 0.11856735 0.10901934 0.100577   0.12403348 0.1217447  0.10884365
 0.12820787 0.10780771 0.13305895 0.08844699 0.09511572 0.10382489
 0.1240067  0.09712847 0.11843592 0.11573751 0.10951714 0.08732259
 0.10719371 0.10502576 0.10101849 0.11567225 0.11547893 0.10666698
 0.11937345 0.09957382]
echo_repeat-2
BERT
[0.78661186 0.78010905 0.78130203 0.7850891  0.7825566  0.7798628
 0.7776026  0.78023535 0.78078425 0.7759785  0.7771514  0.78007436
 0.7801148  0.7822795  0.7819514  0.7822214  0.7781653  0.7820561
 0.7787803  0.7822248  0.7853561  0.78259265 0.7819415  0.78463715
 0.7820501  0.78756976 0.78183645 0.7866798  0.78278184 0.78361577
 0.78294593 0.779228   0.7809496  0.78388065 0.78069633 0.7795018
 0.77857816 0.7787277  0.7830086  0.77790654 0.7793526  0.7817041
 0.77960753 0.77793634 0.78102195 0.78247124 0.7863686  0.78162605
 0.78840816 0.77784425 0.78193265 0.7817678  0.7792934  0.78008217
 0.7820212  0.78264433 0.78343    0.780695   0.77849096 0.78002095
 0.78302187 0.78208494 0.7783617  0.7805947  0.7766649  0.7841276
 0.78570586 0.778865   0.78544235 0.7821646  0.7816616  0.78377104
 0.78212124 0.77767926 0.778191   0.78191537 0.7818147  0.7839825
 0.7816079  0.77977055 0.78401035 0.77992946 0.7838893  0.78122765
 0.7829177  0.7826878  0.7788698  0.78119427 0.784705   0.78731674
 0.7809886  0.77927786 0.78470016 0.78026927 0.78282386 0.77852494
 0.7850542  0.7847036  0.78017527 0.78002095 0.78289574 0.78785807
 0.77946806 0.77956355 0.78058624 0.78284967 0.780769   0.7854949
 0.7894498  0.78199184 0.7847857  0.7835139  0.78136015 0.77866286
 0.78284836 0.7841903  0.7827196  0.7788895  0.78456336 0.78168374
 0.78401065 0.7818552  0.78038555 0.7832879  0.77937156 0.7797216
 0.77979755 0.7796011  0.7843888  0.7794256  0.78152484 0.7811833
 0.77640635 0.7825256  0.78196615 0.780869   0.7795838  0.7824082
 0.78170323 0.78195983 0.78516823 0.7790882  0.77935684 0.78003055
 0.78058404 0.78596014 0.7817718  0.7825642  0.7797064  0.7811331
 0.7824614  0.78507066 0.78528816 0.7869059  0.77927387 0.7851551
 0.7820977  0.7783066  0.7819393  0.77808845 0.78055286 0.7797355
 0.7863849  0.7804628  0.7786343  0.7812732  0.7814913  0.7828107
 0.7868712  0.783777   0.7832902  0.78148407 0.7846152  0.78202057
 0.7780615  0.78110284 0.77977973 0.78057694 0.78391844 0.787568
 0.78582877 0.78356797 0.7810263  0.7800389  0.7810638  0.78081346
 0.77970093 0.7762618  0.7852216  0.78099936 0.7801364  0.78041816
 0.78642654 0.78438514 0.78445303 0.7799354  0.7821     0.7826407
 0.77904844 0.7782681 ]
