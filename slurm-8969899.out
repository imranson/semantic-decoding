Job is using 1 GPU(s) with ID(s) 0 and 8 CPU core(s)
torch cuade is available: True
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wheretheressmoke
WER
[0.03292258 0.03597302 0.03908556 0.03564609 0.03669523 0.0298975
 0.03308266 0.03293963 0.0320634  0.03763352 0.03773493 0.03854739
 0.03499334 0.02915528 0.03208259 0.03036658 0.03689787 0.03569534
 0.03639635 0.03483472 0.03701953 0.03347059 0.0312006  0.03404207
 0.03274248 0.03613686 0.03094105 0.0309538  0.02916152 0.03221401
 0.02996114 0.03738456 0.03368744 0.03427584 0.0335878  0.03361964
 0.03305014 0.04341266 0.03304612 0.03386994 0.03407964 0.03285275
 0.03883487 0.03499339 0.03759457 0.03169681 0.03450826 0.02688345
 0.03039381 0.0307822  0.02513891 0.03141233 0.03097403 0.03545013
 0.03402169 0.03142562 0.03544331 0.03061498 0.03147018 0.03294456
 0.03471063 0.0296229  0.03126457 0.03258937 0.0383704  0.03365428
 0.0394876  0.03370837 0.03503292 0.03159738 0.03510709 0.03387923
 0.03130197 0.0368258  0.03467851 0.03038469 0.03538506 0.03092596
 0.03281471 0.03141972 0.03262437 0.03371617 0.0345109  0.02599948
 0.03186663 0.03016487 0.03983706 0.03747931 0.03444425 0.02589082
 0.03256415 0.03102102 0.03255355 0.03353374 0.03666007 0.03650906
 0.03008635 0.03160235 0.03400855 0.03230197 0.0267397  0.03600342
 0.0370217  0.03843808 0.03098957 0.03864099 0.03337518 0.03611964
 0.03869242 0.03049821 0.02700697 0.03279167 0.0319273  0.03989878
 0.02996608 0.03877933 0.03346462 0.03305951 0.03917882 0.03767392
 0.03544934 0.03328619 0.03520074 0.03711353 0.03324869 0.02931822
 0.030309   0.03677332 0.03349626 0.03083211 0.0370191  0.03411328
 0.04032207 0.03123758 0.0337546  0.03226683 0.03646851 0.03397928
 0.03539945 0.03309798 0.03246845 0.02939657 0.03698909 0.0385816
 0.03392263 0.03123295 0.03137606 0.03764459 0.03526569 0.02987795
 0.03399356 0.03558408 0.03353876 0.04023267 0.03645197 0.03303151
 0.0372418  0.03388611 0.03679244 0.02881576 0.03259727 0.03126633
 0.03001764 0.03286164 0.03626557 0.03373611 0.03883938 0.03345491
 0.03813285 0.03538511 0.03170875 0.03001995 0.03532705 0.02889521
 0.03739911 0.02964929 0.03738621 0.03656604 0.03830988 0.03151016
 0.03704592 0.03143828 0.03682817 0.03662705 0.03181038 0.02948218
 0.03542409 0.03794158 0.02848205 0.03368052 0.03480272 0.03146809
 0.03508886 0.03521606 0.03687278 0.02857644 0.03403506 0.04051311
 0.03414184 0.03498887]
wheretheressmoke
BLEU
[0.19255056 0.18597359 0.19120965 0.18527186 0.1968422  0.1812962
 0.18597465 0.18683173 0.18206824 0.17701152 0.20270469 0.19828318
 0.18693449 0.18976781 0.18394191 0.18317529 0.17961403 0.19498349
 0.19556717 0.18299039 0.19593764 0.18903173 0.18066238 0.18437356
 0.1942584  0.19119409 0.19180357 0.19055778 0.17114981 0.18403139
 0.1798085  0.19295194 0.18665725 0.1949467  0.1876398  0.18299706
 0.19047618 0.20246775 0.18838553 0.18692072 0.19198673 0.19189116
 0.19592716 0.19330067 0.1840507  0.18834371 0.18839359 0.18637137
 0.18100769 0.18319096 0.17433582 0.19431555 0.18266161 0.1875165
 0.19558023 0.177636   0.19340253 0.18782338 0.17862275 0.18511884
 0.19899688 0.17983254 0.18669378 0.19258463 0.19017344 0.18364629
 0.19255724 0.17770569 0.18705693 0.19516304 0.1831184  0.19452584
 0.1780973  0.18982126 0.18307516 0.18192987 0.1907017  0.185342
 0.17868898 0.19083727 0.18106678 0.18404743 0.18264055 0.17456706
 0.18712218 0.1810563  0.1913463  0.19954587 0.17739071 0.17870358
 0.18273307 0.19315291 0.19207299 0.18634643 0.19688412 0.19824079
 0.17995545 0.18708682 0.19018557 0.1933929  0.18018069 0.19251432
 0.19265435 0.20251798 0.17609396 0.18836732 0.19203382 0.19022873
 0.20554474 0.17628793 0.18836189 0.19536434 0.18148317 0.19811676
 0.1870607  0.19205694 0.18890394 0.18416562 0.20428099 0.19329807
 0.19689036 0.18800627 0.1882091  0.18839796 0.18178169 0.18006155
 0.18608004 0.19728791 0.18325793 0.17965198 0.19030669 0.19510362
 0.19822689 0.18863003 0.19454321 0.18175806 0.18147862 0.1826941
 0.19445063 0.18542822 0.19005808 0.18339128 0.18889371 0.19064838
 0.18703303 0.19398695 0.17965852 0.19513051 0.18755542 0.18362913
 0.18395371 0.19197058 0.19995292 0.19830733 0.18887792 0.1770099
 0.19296448 0.18152382 0.1937947  0.1763421  0.18760767 0.19255524
 0.1803748  0.19750989 0.19193038 0.19219621 0.1879137  0.18785183
 0.18986534 0.18306492 0.18356108 0.17673639 0.19144701 0.18237422
 0.18629496 0.17845038 0.188046   0.19210177 0.19399701 0.17612724
 0.19065761 0.18768993 0.18471575 0.18984027 0.18021859 0.18368599
 0.18376979 0.18907743 0.17856397 0.19083205 0.18550959 0.17690693
 0.19013631 0.18476835 0.19530454 0.18492282 0.20098577 0.19574572
 0.18622871 0.1791228 ]
wheretheressmoke
METEOR
[0.14148606 0.13909168 0.1409971  0.13398423 0.14361926 0.13658092
 0.14031449 0.13507926 0.1364778  0.12884785 0.14747832 0.14599574
 0.13375203 0.14217333 0.13350527 0.13440113 0.13573078 0.13629722
 0.1401484  0.1337224  0.13523407 0.13795387 0.13493147 0.13519068
 0.14018663 0.13741314 0.14062737 0.13844986 0.12641849 0.13754797
 0.1312171  0.13650654 0.13748548 0.14285617 0.13204743 0.13734099
 0.13745916 0.14639509 0.13897487 0.13553753 0.13667241 0.13684004
 0.13926743 0.14106958 0.13025526 0.14312362 0.1386398  0.13475255
 0.13552703 0.13806962 0.128573   0.13933582 0.1347249  0.14012796
 0.13986729 0.13251148 0.14308209 0.13238905 0.13278132 0.13655489
 0.14740547 0.1272303  0.13569052 0.14083912 0.13547876 0.13862302
 0.14128524 0.12876998 0.13378067 0.14364839 0.13852563 0.13933336
 0.13686422 0.13842502 0.13645505 0.13695067 0.14086358 0.13762531
 0.13228103 0.13835173 0.13136699 0.13723531 0.1353083  0.12710728
 0.14165892 0.1350809  0.14287846 0.14194744 0.13139186 0.13292808
 0.13179662 0.14057509 0.13865196 0.13442394 0.14315555 0.1437202
 0.12955882 0.13547985 0.1397433  0.14178236 0.13740374 0.13840755
 0.14061703 0.14860538 0.12658118 0.14088675 0.14150078 0.13583518
 0.14774411 0.13298745 0.14167447 0.13599843 0.13803698 0.14389839
 0.13628139 0.14358838 0.13737564 0.13564552 0.1456427  0.14204885
 0.13703516 0.13900269 0.13538969 0.13707472 0.13329663 0.1307846
 0.13336553 0.14070027 0.13205494 0.13680679 0.14026078 0.13814186
 0.14235962 0.13941866 0.1379785  0.1341805  0.13335392 0.13711118
 0.14199288 0.13453061 0.14023818 0.13123161 0.14018722 0.14309186
 0.14122458 0.14094889 0.13244074 0.14332884 0.13717757 0.13733019
 0.13686771 0.13969348 0.14793375 0.14476801 0.14063174 0.13405287
 0.13891582 0.13637809 0.14283615 0.13015551 0.1352086  0.14222222
 0.13603614 0.13896796 0.14010667 0.13510962 0.13757161 0.14015982
 0.1367094  0.13548573 0.13510219 0.13188332 0.14250405 0.13621949
 0.13603495 0.12774806 0.13999832 0.14308395 0.1370202  0.13116658
 0.13965942 0.13909355 0.13317299 0.13904918 0.13531411 0.13528035
 0.13292505 0.14000376 0.12963544 0.13764545 0.1382761  0.12973089
 0.13951647 0.1340332  0.14409487 0.13737333 0.14466742 0.14111622
 0.13484038 0.13155949]
wheretheressmoke
BERT
[0.7897218  0.78981084 0.78900295 0.7895995  0.78982234 0.78877234
 0.7917609  0.7886041  0.79056686 0.7886637  0.79120934 0.7907911
 0.7921666  0.79014164 0.79007703 0.78918725 0.79018795 0.7916313
 0.79048663 0.79097056 0.7903874  0.7895074  0.7917924  0.78930587
 0.789832   0.79162127 0.7886755  0.79044074 0.78845906 0.7913436
 0.78856826 0.79109204 0.78916204 0.7899595  0.7886751  0.78886366
 0.7925727  0.79159194 0.7913497  0.7881586  0.78860915 0.78990626
 0.7914007  0.7902773  0.7898107  0.7900365  0.7894715  0.7897678
 0.7881783  0.78908515 0.78808516 0.7907286  0.7888937  0.79099053
 0.791065   0.7887037  0.7926585  0.788786   0.788248   0.7896016
 0.7911129  0.7883744  0.78984106 0.7897567  0.78910744 0.79111665
 0.7896649  0.78934956 0.7891258  0.79126006 0.7892712  0.7888639
 0.788395   0.790158   0.78878456 0.78845584 0.791129   0.78941375
 0.7876085  0.7908643  0.78904116 0.79007685 0.7913124  0.78691626
 0.7887286  0.79030424 0.79008883 0.79126316 0.7876743  0.7892955
 0.78877413 0.78987706 0.78970534 0.7894325  0.7885369  0.7928819
 0.78788507 0.7906385  0.7911647  0.7919317  0.7887266  0.79011
 0.79198295 0.7901041  0.7886963  0.78965247 0.7927895  0.7888071
 0.79250973 0.78827864 0.79115635 0.78887606 0.7912924  0.79012746
 0.79074067 0.79210126 0.7913578  0.7898192  0.78903216 0.7900537
 0.79015493 0.7871147  0.79132044 0.78917223 0.7899685  0.7895442
 0.7904642  0.7893871  0.79013115 0.7893562  0.7893206  0.78953546
 0.7905965  0.78941894 0.7888325  0.7893244  0.7875535  0.7893273
 0.7907032  0.78947043 0.78950256 0.7885479  0.7903039  0.79154587
 0.78982145 0.7903444  0.7890148  0.79115903 0.7915215  0.7879976
 0.790289   0.7919309  0.79257566 0.78848207 0.78984576 0.78800356
 0.7901983  0.78904897 0.79101557 0.7893626  0.7901773  0.7895814
 0.7897205  0.789197   0.79041076 0.7904123  0.7888156  0.7886993
 0.7909726  0.7919044  0.7892968  0.7899872  0.789841   0.7881987
 0.78944427 0.7881708  0.788726   0.7917876  0.7905758  0.79023874
 0.7886848  0.7894069  0.78999555 0.78879946 0.7889096  0.789192
 0.78841865 0.79231566 0.7873213  0.791402   0.79268956 0.79102504
 0.79097193 0.7884903  0.79100186 0.79087496 0.7908549  0.7903197
 0.79003984 0.78966016]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/net/scratch/q12628ct/semantic-decoding/decoding/evaluate_predictions.py", line 50, in <module>
    ref_data = load_transcript(args.experiment, reference)
  File "/net/scratch/q12628ct/semantic-decoding/decoding/utils_eval.py", line 23, in load_transcript
    with open(grid_path) as f: 
FileNotFoundError: [Errno 2] No such file or directory: '/net/scratch/q12628ct/semantic-decoding/data_test/test_stimulus/perceived_movie/sintel.TextGrid'
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
presto
WER
[0.03623052 0.036102   0.03681334 0.04536371 0.02856498 0.02871601
 0.03223597 0.02074578 0.03272194 0.02675518 0.03422884 0.03575691
 0.0249466  0.02844496 0.02665883 0.03171887 0.03728097 0.02826599
 0.03872382 0.03446207 0.03074548 0.03021362 0.03537993 0.02643519
 0.0269837  0.03427137 0.02523066 0.02671113 0.03603735 0.03470277
 0.04154689 0.03451508 0.03515363 0.04076235 0.03747117 0.03716592
 0.03528492 0.03212705 0.02894926 0.0357403  0.03200728 0.02812811
 0.02720779 0.03427821 0.02942229 0.03565917 0.0373147  0.02684958
 0.04184239 0.04208964 0.03521251 0.03426041 0.03354463 0.03603346
 0.03852756 0.02490854 0.03141482 0.03581267 0.02090335 0.03619442
 0.03528014 0.03599158 0.03841632 0.04575195 0.03081798 0.03710953
 0.02824144 0.02997574 0.03440587 0.02736279 0.02576354 0.02880589
 0.0276833  0.03289405 0.02950379 0.03806273 0.04117483 0.03423184
 0.03359529 0.03792999 0.02751698 0.02565381 0.03543478 0.03332524
 0.03388398 0.03566744 0.03190348 0.03325137 0.03466995 0.02302226
 0.04139687 0.02254838 0.03796579 0.03225399 0.04437159 0.03030676
 0.03045504 0.02937934 0.02344669 0.0347604  0.0268401  0.03140893
 0.04203091 0.03441465 0.03117932 0.0318561  0.03339182 0.03449712
 0.03489291 0.03436277 0.02612728 0.03219676 0.0329488  0.03164279
 0.03430546 0.04016569 0.03454702 0.03480687 0.03172953 0.04072802
 0.02927781 0.03796135 0.0360187  0.03240948 0.0445182  0.04081811
 0.03233813 0.03631626 0.02740527 0.02670346 0.03459783 0.02677257
 0.03325383 0.02678313 0.03075254 0.02517916 0.03360446 0.04053862
 0.0410913  0.02709509 0.03091726 0.03623943 0.03048714 0.02637408
 0.02839967 0.04228549 0.02671155 0.02698368 0.03733324 0.03566663
 0.03636008 0.03912816 0.02957541 0.03024683 0.0317432  0.03399755
 0.03766118 0.02973467 0.02487913 0.03393876 0.03133811 0.03379994
 0.03077576 0.0339727  0.02927813 0.03222006 0.02696079 0.03097391
 0.03703801 0.03443809 0.03343306 0.02787774 0.03261679 0.03460783
 0.04175267 0.03465638 0.03273603 0.02842505 0.02772483 0.03715955
 0.04179828 0.03525466 0.03878664 0.03605625 0.02924191 0.03221123
 0.03632579 0.04423129 0.0329194  0.03163411 0.03420289 0.03264024
 0.02855634 0.03080032 0.03210652 0.03473458 0.02941593 0.02891862
 0.02829722 0.04187197]
presto
BLEU
[0.12401638 0.13265324 0.1398617  0.1437843  0.11149722 0.11914972
 0.10952576 0.09842813 0.12112098 0.10885004 0.12791602 0.1139244
 0.10597857 0.11559793 0.11265827 0.1123922  0.112865   0.11220733
 0.11678036 0.13471836 0.10715023 0.12629379 0.12861185 0.10789026
 0.11175808 0.12053166 0.09396026 0.10742427 0.12664526 0.13217319
 0.13606572 0.12217137 0.11653329 0.12598796 0.1185998  0.13751382
 0.1250445  0.1188348  0.11354077 0.13436065 0.12115839 0.11162603
 0.11664943 0.12636717 0.12052001 0.1139177  0.11795573 0.10515537
 0.1291448  0.12371866 0.13151649 0.12332458 0.11084819 0.12247771
 0.11107614 0.10178806 0.11339088 0.11731577 0.09041788 0.11356639
 0.11897639 0.11934419 0.12645258 0.14862271 0.10492707 0.11472079
 0.12291951 0.11016951 0.11858789 0.103882   0.11462266 0.11468109
 0.10693682 0.11989674 0.10629524 0.1194534  0.13215876 0.10952658
 0.1226233  0.12493138 0.11468609 0.10059835 0.12791246 0.12389542
 0.12859631 0.12101576 0.11478809 0.1196782  0.12866249 0.10703827
 0.13020776 0.10841866 0.13719548 0.12706965 0.13263254 0.11893933
 0.10737511 0.10486124 0.10050608 0.12777626 0.11459839 0.11897947
 0.13012095 0.12654943 0.12168754 0.12396287 0.11211052 0.12786495
 0.12601571 0.11811319 0.10894533 0.11882095 0.11318627 0.12769361
 0.12916404 0.12689261 0.12108024 0.12843858 0.13319395 0.13670456
 0.11071551 0.12373426 0.13273342 0.13227034 0.1272168  0.13271275
 0.11514117 0.12968831 0.11163239 0.09788807 0.12139245 0.11131968
 0.11733147 0.10475483 0.10310409 0.10041592 0.11467106 0.1330118
 0.12246649 0.11235452 0.10013518 0.11430851 0.12016431 0.11243198
 0.10659476 0.13906205 0.10638657 0.10342806 0.12465115 0.13342335
 0.11380806 0.11526582 0.10497945 0.12389225 0.12340975 0.12780198
 0.12698995 0.11672855 0.10425307 0.13254214 0.1282702  0.1335762
 0.12099739 0.11577428 0.10807392 0.10673688 0.10970079 0.105576
 0.12525926 0.10839692 0.12472065 0.12431314 0.11443778 0.11534088
 0.12517765 0.12769973 0.11947948 0.1242661  0.10826811 0.11229067
 0.12827149 0.12901942 0.12237694 0.12351364 0.10482531 0.10138977
 0.13213318 0.14022989 0.13283277 0.12062762 0.11950029 0.11074435
 0.11937909 0.11294321 0.10803522 0.12111768 0.10092075 0.11817596
 0.11513922 0.12816487]
presto
METEOR
[0.08421306 0.0892996  0.09045136 0.09043914 0.08166599 0.08713869
 0.07668267 0.07271365 0.08542244 0.07778271 0.09101256 0.08335362
 0.07445424 0.08066784 0.07702337 0.07932313 0.07529991 0.07743201
 0.08257791 0.09064484 0.07432215 0.08643839 0.08953442 0.07851884
 0.07767418 0.08297226 0.07070151 0.07857598 0.08848747 0.09047152
 0.0855213  0.07881241 0.08328404 0.08410541 0.08496908 0.09044069
 0.08129736 0.08151501 0.07785057 0.09188256 0.08782234 0.07685689
 0.08369283 0.08831442 0.0852871  0.08035592 0.07862804 0.07727773
 0.0880346  0.0859458  0.08902152 0.08163395 0.07556343 0.08179414
 0.08172346 0.07397893 0.07735547 0.08273516 0.0707414  0.08487826
 0.0792146  0.07891467 0.08469894 0.09759411 0.07558099 0.07837685
 0.08384994 0.07752313 0.07566545 0.07523647 0.08062734 0.07853965
 0.07537428 0.0828428  0.07373364 0.0856065  0.09090069 0.07811213
 0.08926903 0.08578662 0.0726829  0.07057349 0.084835   0.08491777
 0.08834947 0.08124036 0.08335793 0.0823142  0.0876408  0.07765787
 0.09184594 0.07970154 0.08907357 0.0853657  0.09205156 0.07974507
 0.07617893 0.07617832 0.0716507  0.0853835  0.0787125  0.07988005
 0.08033526 0.08181387 0.0858218  0.08069013 0.08079695 0.08902457
 0.08584058 0.082842   0.07450649 0.07828066 0.08253574 0.09044797
 0.09129394 0.08374828 0.0862244  0.0912358  0.09414883 0.09312171
 0.07978313 0.08534821 0.09007036 0.09225424 0.08710614 0.09052924
 0.08086402 0.08856373 0.07962275 0.07554542 0.07897251 0.07882613
 0.08058429 0.07144306 0.07477457 0.07134873 0.08302733 0.08788957
 0.08738945 0.07656227 0.07277174 0.0839697  0.08451522 0.08086967
 0.07796781 0.09282135 0.07118976 0.0721703  0.0863129  0.08749884
 0.07704402 0.08268158 0.07447957 0.08297038 0.09012387 0.08331479
 0.08873769 0.08507025 0.07011369 0.08639617 0.08731733 0.08729003
 0.08000863 0.0833214  0.07545431 0.07664163 0.07841719 0.07354322
 0.08542907 0.07462173 0.08757557 0.08604025 0.08321278 0.07927012
 0.08498648 0.08721489 0.07958361 0.08738145 0.07540813 0.07343395
 0.08616217 0.08877468 0.08301257 0.08237144 0.07500942 0.07369813
 0.08746665 0.09613033 0.08982586 0.08326117 0.08652141 0.07818959
 0.08446961 0.08155977 0.07781735 0.08230081 0.07661459 0.08448071
 0.08307482 0.0844135 ]
presto
BERT
[0.77428037 0.7732015  0.7749494  0.7733601  0.77194965 0.7705343
 0.7715563  0.770148   0.77146345 0.7707731  0.77102584 0.7738871
 0.7701077  0.77190757 0.7721876  0.7720151  0.7707038  0.77118
 0.7695945  0.77188796 0.7725687  0.7702465  0.7728218  0.77129257
 0.7712367  0.7700269  0.7706916  0.77047116 0.77067304 0.7724746
 0.77193123 0.7726343  0.7713771  0.77176815 0.77144206 0.7723022
 0.7718541  0.772846   0.7736101  0.7727184  0.7740742  0.7686979
 0.77201617 0.77277255 0.7718366  0.7726164  0.77084374 0.7699019
 0.7723695  0.7726233  0.77367866 0.7739202  0.77012354 0.7759198
 0.772192   0.7685059  0.7706572  0.7733512  0.7691444  0.770772
 0.7698245  0.77077365 0.7718815  0.7755455  0.771622   0.77146995
 0.77079195 0.7716474  0.7720491  0.76874167 0.77122265 0.7727039
 0.7696932  0.77158034 0.7702946  0.77562034 0.7712351  0.77245706
 0.77128714 0.77002096 0.77138877 0.7713145  0.77297544 0.7729128
 0.7746046  0.7743246  0.77045053 0.7707488  0.77058095 0.7708542
 0.77434987 0.77140653 0.7739047  0.7720902  0.77280015 0.7733306
 0.7700189  0.76890886 0.7722912  0.77183896 0.77267313 0.7711283
 0.7739698  0.7748682  0.7705999  0.7735954  0.77240604 0.77020353
 0.7725251  0.77044654 0.7687276  0.77304155 0.773427   0.7699001
 0.7758972  0.77229387 0.77240795 0.77316916 0.7738263  0.77409554
 0.7688311  0.7739832  0.775533   0.773691   0.7710391  0.77390695
 0.7704873  0.7735113  0.7714823  0.7697596  0.7707101  0.77130723
 0.7733141  0.7711303  0.7697824  0.77121717 0.7750523  0.77110857
 0.77247447 0.7699801  0.7712009  0.77272105 0.7741037  0.7709032
 0.7697933  0.7756965  0.77101815 0.770798   0.7726055  0.7728382
 0.77057964 0.7733693  0.7733942  0.76906556 0.7733749  0.7749351
 0.7727638  0.77220124 0.770068   0.77220136 0.771869   0.7755531
 0.7706116  0.77048004 0.7702715  0.77090204 0.7724619  0.7727791
 0.77242035 0.7723634  0.77243716 0.77217466 0.77125704 0.77222914
 0.77018714 0.7713675  0.76999164 0.7706489  0.77103066 0.77061343
 0.77254736 0.77675545 0.77341866 0.7730639  0.77057546 0.7688018
 0.77295244 0.77570295 0.7724353  0.7721073  0.7745121  0.76900303
 0.76918113 0.7744537  0.7685734  0.7731222  0.76842874 0.77004355
 0.769214   0.7734105 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
partlycloudy
WER
[-0.13998914 -0.14080498 -0.14124735 -0.13762021 -0.13784617 -0.1310016
 -0.13358465 -0.13531318 -0.1387917  -0.1287151  -0.14808035 -0.13217486
 -0.14309574 -0.14071017 -0.13665188 -0.1376329  -0.13270989 -0.13852879
 -0.12906721 -0.13404719 -0.12910054 -0.12901916 -0.1329473  -0.13238057
 -0.13126144 -0.13923625 -0.14288095 -0.13297664 -0.14049569 -0.13816573
 -0.13466322 -0.14078661 -0.12910676 -0.12998394 -0.1342813  -0.13079246
 -0.15213272 -0.13786273 -0.13981652 -0.12448253 -0.14068565 -0.13679568
 -0.13559814 -0.1371905  -0.1240826  -0.12761622 -0.13207235 -0.14391073
 -0.13949754 -0.1341242  -0.13573768 -0.1421287  -0.14133519 -0.13947539
 -0.13871281 -0.13874575 -0.1380745  -0.13409152 -0.13249121 -0.14329077
 -0.14253876 -0.13331048 -0.13959855 -0.12562262 -0.14190793 -0.13418533
 -0.13077205 -0.14319593 -0.12830665 -0.13386237 -0.13417478 -0.13432602
 -0.13434428 -0.13648213 -0.13820384 -0.12716813 -0.14069274 -0.13335191
 -0.1362683  -0.13247285 -0.13907357 -0.13103621 -0.13300587 -0.13375122
 -0.13448907 -0.13879432 -0.1344709  -0.13326267 -0.134656   -0.13185279
 -0.12755229 -0.13241253 -0.14318581 -0.13632535 -0.12437328 -0.13043792
 -0.13254528 -0.13175553 -0.13612807 -0.14656793 -0.12804397 -0.12769661
 -0.13298099 -0.14217796 -0.13080274 -0.13366453 -0.13591631 -0.13442041
 -0.13890734 -0.13832174 -0.14052451 -0.13842334 -0.13621694 -0.13457106
 -0.13287641 -0.13800149 -0.13587533 -0.13333716 -0.13276469 -0.12734284
 -0.13362739 -0.13757557 -0.1345566  -0.1306836  -0.14613047 -0.13238962
 -0.14261468 -0.13551128 -0.14098322 -0.14032733 -0.13449246 -0.13813455
 -0.13692144 -0.13417865 -0.14240619 -0.13483498 -0.13661816 -0.14220076
 -0.13364827 -0.12960389 -0.13863816 -0.13041689 -0.13643757 -0.13203021
 -0.13624613 -0.13819064 -0.13611246 -0.13140211 -0.1362611  -0.12536873
 -0.14265002 -0.13622669 -0.1320138  -0.13524332 -0.14201724 -0.14359923
 -0.13938226 -0.13970975 -0.13651478 -0.13469853 -0.13320579 -0.13466952
 -0.14534725 -0.13025382 -0.12741908 -0.13453552 -0.12916227 -0.13644495
 -0.14867122 -0.13537906 -0.13821206 -0.12995911 -0.12983091 -0.13560317
 -0.13046409 -0.12776925 -0.13210219 -0.14111609 -0.13368733 -0.13019644
 -0.13539885 -0.1428549  -0.1461639  -0.1362777  -0.13626385 -0.13243832
 -0.13447367 -0.13765773 -0.14235138 -0.13465085 -0.13575873 -0.14116888
 -0.13044567 -0.12408112 -0.12569618 -0.13354209 -0.13693631 -0.13747819
 -0.12457672 -0.13427957]
partlycloudy
BLEU
[0.11954496 0.12048172 0.11391658 0.12811606 0.12138707 0.14090307
 0.14896238 0.13044829 0.13231037 0.13508654 0.11137377 0.12988206
 0.11098679 0.12411601 0.11676743 0.13199394 0.12118189 0.10862938
 0.13456926 0.12364555 0.13639564 0.13713403 0.12829755 0.13313005
 0.13749439 0.12598715 0.12743037 0.11960061 0.11724137 0.12305175
 0.12033924 0.13377317 0.12357843 0.12560904 0.13062902 0.11924673
 0.10815467 0.11952406 0.1171325  0.12440738 0.12534476 0.12454419
 0.12029891 0.11123282 0.13938221 0.13173812 0.12907985 0.13190671
 0.11807706 0.11846228 0.12568737 0.1175426  0.12595832 0.11481091
 0.12770653 0.12844964 0.12468942 0.11465179 0.13491291 0.11875353
 0.12510518 0.11729577 0.1337415  0.1429437  0.11597111 0.12596144
 0.13343081 0.1218598  0.12963355 0.12626194 0.13013484 0.12706001
 0.11650602 0.13404631 0.12110322 0.14377263 0.11665753 0.12558264
 0.12475983 0.13691932 0.11118562 0.12542588 0.12639251 0.11671123
 0.13272223 0.12064716 0.12608963 0.13327365 0.13485    0.14017087
 0.1421309  0.12510055 0.12198984 0.12020495 0.13809051 0.13949583
 0.13864035 0.11922934 0.12608978 0.11841499 0.14565843 0.1484912
 0.13086647 0.10796822 0.12774345 0.12573714 0.12804878 0.130677
 0.12734144 0.11587714 0.11691045 0.1258018  0.12030818 0.13949968
 0.12906832 0.12424671 0.11865383 0.12621331 0.12690544 0.12898721
 0.13421556 0.11619047 0.12768566 0.1336121  0.10423594 0.1276291
 0.11990864 0.12180889 0.11580552 0.1265967  0.11967068 0.12295706
 0.11427559 0.12844594 0.10918839 0.12586165 0.12330752 0.11907798
 0.13607318 0.14160919 0.12227798 0.13468279 0.122813   0.12703588
 0.11946696 0.11669873 0.12475318 0.12186514 0.13348223 0.14505632
 0.13399081 0.13562694 0.12224588 0.13313564 0.10448377 0.11705574
 0.12965015 0.11913889 0.12385846 0.12629718 0.12700204 0.12314306
 0.10762963 0.14728271 0.12772638 0.1236472  0.14768879 0.13114837
 0.10829056 0.12076909 0.12321695 0.12974048 0.13013434 0.11769628
 0.12530346 0.14552718 0.1300368  0.12849217 0.1335607  0.13590103
 0.13628694 0.1110084  0.11454134 0.12696378 0.12628138 0.14107383
 0.12821152 0.12197643 0.10988518 0.14178188 0.1313367  0.11048924
 0.1318326  0.13978585 0.13051366 0.12340002 0.12927238 0.13316559
 0.14029536 0.12266255]
partlycloudy
METEOR
[0.10079433 0.10400682 0.09618678 0.10475628 0.10638663 0.10904325
 0.1227672  0.10351881 0.11003442 0.11005522 0.09360442 0.10433963
 0.09323694 0.10388182 0.09290998 0.10504322 0.10161121 0.09488049
 0.11187989 0.10483964 0.10538288 0.11824762 0.10523494 0.10487074
 0.10327813 0.10028733 0.09419089 0.09543873 0.09721899 0.10507344
 0.10618349 0.10065763 0.10700178 0.09604456 0.10904677 0.10438528
 0.09245466 0.09965871 0.09708464 0.10273703 0.09937504 0.10002579
 0.10095974 0.09997289 0.10924684 0.11216647 0.10300173 0.10299631
 0.09941027 0.10049073 0.09869159 0.09688668 0.10170828 0.09275234
 0.09989148 0.10832142 0.1019198  0.0985166  0.10446012 0.09802065
 0.10234966 0.09346233 0.10577874 0.11059497 0.09351324 0.1036609
 0.1032044  0.10066021 0.10316939 0.10330592 0.10102488 0.10516634
 0.09534849 0.10740113 0.09957372 0.11155031 0.10080583 0.10001138
 0.10075016 0.11354699 0.09175225 0.10155741 0.09994539 0.10647398
 0.10923784 0.09916708 0.09573589 0.10722453 0.10471237 0.11028605
 0.11479988 0.10219119 0.09999316 0.09992304 0.11030322 0.1087961
 0.11177688 0.0955565  0.10053808 0.09575494 0.11199965 0.11378391
 0.10869045 0.09062718 0.09955208 0.10439155 0.10706919 0.10820795
 0.10632113 0.09563035 0.0969157  0.10632573 0.1002619  0.11013733
 0.10587106 0.1002277  0.09234734 0.10529344 0.10652507 0.10527052
 0.11037811 0.10001668 0.10866122 0.10051515 0.0863157  0.10094462
 0.09286834 0.10358036 0.08822707 0.09888104 0.09651187 0.09801908
 0.09999809 0.10019034 0.09475746 0.10635368 0.10196787 0.10365613
 0.10850439 0.11491023 0.10489419 0.10488483 0.10996301 0.10161843
 0.0984114  0.09732874 0.10755487 0.09862243 0.10392945 0.11833188
 0.10164542 0.11187959 0.10030724 0.10584012 0.09058661 0.09881815
 0.10275298 0.10003709 0.10098034 0.10538349 0.10521505 0.10662641
 0.08630025 0.11503304 0.10149563 0.10450621 0.11210039 0.10233218
 0.09179693 0.10269366 0.09816816 0.10728177 0.10395631 0.10287419
 0.10090099 0.1187253  0.10841196 0.103934   0.11079355 0.10722477
 0.10513201 0.0944606  0.09382485 0.11067963 0.10051506 0.10607802
 0.10033484 0.10304624 0.09063339 0.12245185 0.10411391 0.08648911
 0.10597825 0.11427948 0.1067249  0.10036263 0.1047554  0.11052087
 0.11647867 0.10287222]
partlycloudy
BERT
[0.7803079  0.78087616 0.7777653  0.78082454 0.7792032  0.7787555
 0.7813855  0.77830917 0.7766148  0.7815107  0.77748567 0.7790026
 0.7821977  0.7802565  0.77736884 0.7769615  0.78231704 0.7784245
 0.7793576  0.77994746 0.7809929  0.77897745 0.7817366  0.7822917
 0.78186023 0.78210706 0.7811363  0.7788504  0.7795273  0.7815949
 0.77698433 0.7784296  0.7797857  0.7808089  0.7793537  0.7801595
 0.7783165  0.7808131  0.778568   0.78288776 0.77930266 0.7810001
 0.779578   0.779257   0.7787406  0.77808446 0.7818996  0.78035396
 0.7796166  0.7791388  0.77674884 0.78236973 0.7760037  0.77951956
 0.7763456  0.77687216 0.7786736  0.78001523 0.77884746 0.77873766
 0.77771324 0.7799475  0.778641   0.7822331  0.7799273  0.78230417
 0.7791152  0.7802456  0.7793275  0.7788505  0.7814533  0.781662
 0.777971   0.7803429  0.78050745 0.78102875 0.7780796  0.7791634
 0.7805028  0.77872264 0.77905905 0.7818077  0.7787554  0.782661
 0.77951443 0.780024   0.7800766  0.7803385  0.77891105 0.7784831
 0.7797578  0.78077334 0.7764301  0.7804284  0.77907115 0.78049105
 0.7829145  0.7792251  0.77823204 0.7790861  0.78201014 0.7809784
 0.77968216 0.77869165 0.78152716 0.7821544  0.7821556  0.7791895
 0.77851087 0.7774206  0.77974427 0.7818302  0.77791256 0.7797102
 0.78081    0.77990526 0.7784135  0.7796908  0.7789778  0.780578
 0.78059256 0.7810772  0.7804997  0.78172505 0.7762135  0.7803159
 0.78144974 0.77908677 0.7798425  0.7810133  0.77939945 0.779229
 0.77837497 0.7809323  0.7786152  0.77982116 0.7777674  0.7803875
 0.78534955 0.77889854 0.7776428  0.7804957  0.77999556 0.7788216
 0.7791028  0.7810745  0.7804983  0.77899843 0.7799593  0.7798182
 0.779789   0.7811275  0.780182   0.7819916  0.7756867  0.77716017
 0.7805383  0.7798597  0.7790637  0.77977693 0.78208244 0.7809209
 0.780139   0.77867496 0.7788293  0.7782904  0.7803013  0.7760225
 0.7773332  0.7781809  0.77715313 0.77826715 0.7806688  0.7781564
 0.7797534  0.77774537 0.7803788  0.77635175 0.78160936 0.77998066
 0.78240085 0.7769058  0.7794852  0.78178436 0.77901566 0.78335667
 0.77971107 0.77712464 0.77767134 0.78325343 0.77950406 0.7811535
 0.7786255  0.77944404 0.778845   0.77860725 0.7802037  0.7813304
 0.78067654 0.781438  ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
laluna
WER
[-0.06674345 -0.05371551 -0.05834546 -0.05247546 -0.05024176 -0.05253868
 -0.0495122  -0.05652416 -0.05510822 -0.05623237 -0.05698022 -0.04960657
 -0.05477217 -0.0548974  -0.05402281 -0.06428562 -0.05236443 -0.05279595
 -0.04924627 -0.06177944 -0.06079939 -0.05383347 -0.04975093 -0.06048987
 -0.0510603  -0.05578595 -0.05126862 -0.05439478 -0.05956295 -0.04851895
 -0.05170239 -0.05272874 -0.05833096 -0.05695972 -0.04658845 -0.06554139
 -0.05415862 -0.0518028  -0.05420728 -0.05051681 -0.05695638 -0.05027156
 -0.05235933 -0.05811721 -0.05822713 -0.05742836 -0.05400196 -0.05273968
 -0.05523183 -0.06163287 -0.05556294 -0.05585563 -0.05894941 -0.0530141
 -0.05392194 -0.05043434 -0.05844688 -0.05808379 -0.05516332 -0.06395297
 -0.06335263 -0.04667687 -0.05643661 -0.06346701 -0.05326804 -0.05333476
 -0.05946137 -0.05223251 -0.05778272 -0.06223372 -0.05948423 -0.05280626
 -0.05542349 -0.05587558 -0.05991669 -0.04775599 -0.05940255 -0.04982767
 -0.05825367 -0.05862997 -0.05148588 -0.06139922 -0.04642363 -0.05549473
 -0.06020557 -0.05178538 -0.05401883 -0.04917757 -0.05456364 -0.06084761
 -0.04605997 -0.05513617 -0.05260804 -0.04959594 -0.063269   -0.05928672
 -0.04629703 -0.05333264 -0.05592279 -0.0560578  -0.05909326 -0.05519698
 -0.05643577 -0.0553666  -0.05532398 -0.05456166 -0.05833119 -0.05358402
 -0.04575704 -0.05500723 -0.05825061 -0.05674256 -0.0543097  -0.05635959
 -0.06479634 -0.05441213 -0.05586109 -0.0615888  -0.05500119 -0.06211202
 -0.05685392 -0.04970378 -0.05882503 -0.06115886 -0.05683996 -0.06142904
 -0.05698447 -0.06069493 -0.05338881 -0.04698186 -0.05628638 -0.06156981
 -0.05748361 -0.06133054 -0.04767035 -0.05714651 -0.05682601 -0.05202797
 -0.05161615 -0.05009653 -0.05031785 -0.05400838 -0.05669436 -0.05772216
 -0.04998102 -0.05228011 -0.04781434 -0.05297492 -0.06160777 -0.05269296
 -0.0560644  -0.05738368 -0.04714874 -0.0479974  -0.05103477 -0.05986188
 -0.0539929  -0.05293141 -0.06346358 -0.05219344 -0.0592385  -0.05847138
 -0.05645334 -0.0536098  -0.04948805 -0.05633145 -0.05315773 -0.05049586
 -0.05524581 -0.05730615 -0.05272653 -0.05202011 -0.05359286 -0.0513613
 -0.05661292 -0.0476889  -0.04531357 -0.05665079 -0.058529   -0.04733097
 -0.05658487 -0.04775185 -0.05226604 -0.06034127 -0.06129848 -0.04615953
 -0.05573762 -0.05529131 -0.05435209 -0.06169425 -0.05443716 -0.065021
 -0.05575315 -0.03960658 -0.0561862  -0.05204082 -0.04667347 -0.05086328
 -0.05262741 -0.05707624]
laluna
BLEU
[0.11212223 0.13170261 0.11034786 0.13001053 0.12820911 0.12746597
 0.13595506 0.11707197 0.12126211 0.12140402 0.12463335 0.12669342
 0.12750483 0.12302042 0.12813509 0.11495133 0.13605839 0.11097132
 0.12852817 0.10834315 0.11055071 0.12872879 0.13892282 0.1186821
 0.12233646 0.13037301 0.13125281 0.12085966 0.12051906 0.14133067
 0.11907322 0.11313503 0.12920082 0.12234947 0.14798047 0.10626877
 0.11911923 0.12179849 0.13325338 0.13332552 0.13017519 0.13072907
 0.1440688  0.11732408 0.11325271 0.1166186  0.13919263 0.10984108
 0.11584464 0.11061207 0.12660046 0.12253572 0.11987062 0.12549962
 0.12290072 0.12587668 0.12790871 0.12766033 0.12857649 0.10851979
 0.10397851 0.1246195  0.12780979 0.10345024 0.1229381  0.12058588
 0.11003514 0.13353352 0.11608752 0.10739117 0.10900644 0.13465212
 0.12971966 0.11314498 0.13052837 0.14287915 0.11633675 0.12730796
 0.11535998 0.12984831 0.12690278 0.11972633 0.1330207  0.11930546
 0.10881736 0.12430608 0.11457323 0.13628182 0.12357937 0.12212371
 0.14171135 0.1273888  0.12059232 0.13639966 0.1161874  0.11431245
 0.13578527 0.12910445 0.11940215 0.12839446 0.12385367 0.11338037
 0.12430849 0.1334251  0.11952551 0.12308316 0.10935443 0.12879185
 0.14356514 0.12349482 0.11966208 0.1215337  0.14141055 0.11616159
 0.12049133 0.12863926 0.12728207 0.11967101 0.12536749 0.11418983
 0.1234196  0.1373097  0.1283756  0.10833056 0.12125825 0.10915528
 0.11884283 0.1133688  0.11982866 0.13804689 0.11005609 0.11401604
 0.1220299  0.11673984 0.1400754  0.11968759 0.13433121 0.12980076
 0.12421841 0.12398584 0.13421837 0.12696222 0.11323349 0.11594928
 0.13309115 0.14298186 0.13386174 0.12482157 0.12107372 0.13001571
 0.1321324  0.11924084 0.12935653 0.13519783 0.12599284 0.11984184
 0.13082745 0.13402641 0.11240847 0.13302863 0.12217987 0.12155445
 0.11586851 0.11699711 0.12904176 0.11808284 0.13239643 0.12608217
 0.12379525 0.11645306 0.11988886 0.14122636 0.12949833 0.12830813
 0.12200097 0.12758926 0.13363047 0.12802831 0.12052403 0.13842818
 0.11945005 0.13833728 0.1280861  0.12304078 0.11986967 0.14120365
 0.11953363 0.12746685 0.13203065 0.10944269 0.12362515 0.10854516
 0.11143361 0.15958049 0.12200866 0.12459457 0.13925083 0.12885329
 0.13566436 0.13324906]
laluna
METEOR
[0.08601282 0.10060551 0.08641337 0.09875564 0.09670308 0.09299544
 0.1007382  0.0873268  0.08950649 0.08876116 0.09546283 0.09280316
 0.09420055 0.09522373 0.09315389 0.08440227 0.09857186 0.08777364
 0.09457049 0.07994937 0.0837691  0.09292058 0.10167005 0.08858119
 0.09302014 0.09237337 0.10259065 0.09002854 0.09219263 0.10098715
 0.09002078 0.08682409 0.09277903 0.09797259 0.10573194 0.07761095
 0.08522047 0.09479657 0.0996777  0.0995177  0.08914066 0.09774637
 0.10525388 0.08673883 0.08643719 0.08764369 0.10557071 0.08719789
 0.08922613 0.08315096 0.09334802 0.0942354  0.09058687 0.09198636
 0.09373452 0.09454935 0.09215112 0.09937846 0.09565152 0.08291943
 0.07643853 0.09098367 0.09506605 0.08374739 0.09069184 0.09280267
 0.08416728 0.09800661 0.09051383 0.08676057 0.08507969 0.09787855
 0.09743502 0.0818222  0.09311675 0.10419209 0.08813889 0.09560729
 0.08622709 0.09644001 0.09737109 0.09392248 0.1013664  0.08769981
 0.08333665 0.09343698 0.08879424 0.10299297 0.0943919  0.08975569
 0.09900515 0.09358968 0.09117662 0.1002876  0.09287016 0.08891919
 0.10128069 0.09778711 0.08966059 0.09526267 0.09605892 0.0817276
 0.09476544 0.09828882 0.09240592 0.08712826 0.08496024 0.09719561
 0.10284008 0.09443651 0.09434699 0.08991807 0.10063099 0.08779539
 0.08747732 0.09326828 0.09298226 0.08897238 0.09581585 0.08591235
 0.09444138 0.1019124  0.09563448 0.07890815 0.09493389 0.0881927
 0.08916717 0.08162264 0.08994406 0.09896288 0.08608109 0.09182065
 0.09561197 0.09004865 0.10350837 0.08713678 0.10469105 0.09415646
 0.09179229 0.09464083 0.09641679 0.0929288  0.08770634 0.09062227
 0.09543347 0.10453091 0.09813237 0.09218656 0.08935292 0.09924298
 0.09690323 0.09184639 0.09726231 0.09686002 0.09623465 0.09243626
 0.09080572 0.09714915 0.08440739 0.0950696  0.09389975 0.08590058
 0.08895264 0.08973645 0.09848115 0.09310088 0.09855357 0.08958437
 0.09099996 0.08863402 0.09341921 0.1053563  0.09685181 0.09098962
 0.0898732  0.09962789 0.09812882 0.09326397 0.09105415 0.10121438
 0.08680019 0.10135509 0.0987548  0.09124823 0.08620815 0.10287364
 0.08918907 0.09074666 0.09751845 0.08339988 0.0925556  0.08821714
 0.08554476 0.10849148 0.08697862 0.09169601 0.10125669 0.09934471
 0.10030098 0.09890437]
laluna
BERT
[0.783625   0.78523296 0.7811257  0.7848826  0.7844481  0.7839964
 0.78539705 0.7819384  0.7841443  0.7834321  0.78080416 0.7865879
 0.7872313  0.78320247 0.7873574  0.782694   0.78594667 0.78154814
 0.7827674  0.7804407  0.7809286  0.7837025  0.7846499  0.78347605
 0.7852301  0.784482   0.7805941  0.782165   0.7848993  0.78665173
 0.78430855 0.78396654 0.7839258  0.785144   0.78707105 0.78071755
 0.78552115 0.78319585 0.7878025  0.7833785  0.7823881  0.78503406
 0.7851603  0.7833412  0.78243667 0.78753513 0.78163093 0.7811667
 0.7856321  0.784148   0.78554535 0.78360254 0.7832466  0.78524494
 0.7833335  0.7862031  0.7850647  0.7844525  0.78526515 0.78488076
 0.7814458  0.78434473 0.78499913 0.7797968  0.7849785  0.7841273
 0.78111213 0.7834661  0.7815927  0.7810868  0.7825393  0.78640866
 0.78376085 0.7814449  0.7844616  0.79000276 0.7844473  0.7835963
 0.78241646 0.78273225 0.7860904  0.782715   0.7859681  0.7836789
 0.7817794  0.7848005  0.7831488  0.7871933  0.7836105  0.78170604
 0.7859447  0.7826088  0.7835948  0.78658366 0.7852768  0.7831583
 0.7844206  0.7829754  0.7834956  0.78323466 0.78060204 0.78219724
 0.78469884 0.7820406  0.7838273  0.7841664  0.78255934 0.7859993
 0.7882438  0.78426164 0.78374195 0.78449535 0.7844919  0.78323656
 0.7813374  0.7861977  0.78322804 0.7840672  0.78421944 0.7840025
 0.78105664 0.7849501  0.78169125 0.7811529  0.781223   0.78297925
 0.7858812  0.78442246 0.78554034 0.78528863 0.78491765 0.78382796
 0.7814704  0.78213805 0.78490937 0.78293437 0.7860657  0.781335
 0.781725   0.7816171  0.7847821  0.78334177 0.7800787  0.7824088
 0.78571    0.7858887  0.78600484 0.7850413  0.78384805 0.7826391
 0.7827755  0.78487265 0.7839904  0.78376883 0.78324306 0.7839916
 0.7820942  0.78682745 0.7823525  0.784703   0.7817024  0.7852926
 0.7855958  0.78190774 0.78512055 0.7851367  0.7807818  0.78511995
 0.7822352  0.782251   0.7820035  0.7853041  0.7874547  0.7857251
 0.7818035  0.78286934 0.7871573  0.784295   0.7816936  0.7852726
 0.78403604 0.78687465 0.7847692  0.7843212  0.78235555 0.78457826
 0.78502643 0.784611   0.78400636 0.78334343 0.7861706  0.78288615
 0.7827803  0.7868123  0.7830295  0.78199106 0.7834859  0.7859001
 0.78629875 0.7863508 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
attend-M
WER
[0.05109712 0.04513745 0.04835644 0.04581041 0.04513633 0.04976595
 0.04294234 0.0501032  0.04192527 0.04370449 0.04983671 0.0507113
 0.04722909 0.05190779 0.04969642 0.03899366 0.04827428 0.04458631
 0.04744808 0.04795077 0.0444052  0.0478527  0.04581284 0.05336218
 0.04137791 0.05442716 0.04489556 0.04667447 0.05093587 0.04642292
 0.05306018 0.04901329 0.04454162 0.05467503 0.04506115 0.04945182
 0.04876428 0.0513262  0.05102627 0.04539273 0.04767977 0.04269668
 0.04507894 0.0497106  0.03741709 0.03850258 0.04964705 0.05157686
 0.05063495 0.05100453 0.04749174 0.0522334  0.0528644  0.05139753
 0.04931624 0.04833686 0.05090598 0.04542523 0.05532577 0.05206606
 0.047273   0.04638385 0.0469828  0.05321932 0.04468719 0.0472266
 0.05006517 0.0464988  0.05067505 0.04508419 0.05208011 0.0500066
 0.04884054 0.05051131 0.05291271 0.04647033 0.04536465 0.04580503
 0.04849555 0.05634751 0.04541963 0.05086757 0.05479872 0.04824377
 0.05144806 0.04750249 0.03378514 0.05444958 0.0462245  0.04611531
 0.04723106 0.05327619 0.05376061 0.04414932 0.04449763 0.05834497
 0.04342539 0.0461833  0.04930199 0.04912247 0.05007192 0.04777412
 0.04458166 0.03733399 0.04701564 0.05496354 0.04979803 0.05274228
 0.03187671 0.04624641 0.05816614 0.04870683 0.04637143 0.04416618
 0.04022048 0.04619155 0.04311373 0.04116332 0.05140744 0.04661633
 0.05328639 0.04300493 0.05082328 0.04370104 0.0480494  0.04722506
 0.04191676 0.05271679 0.04770031 0.04283636 0.05010272 0.0516723
 0.04859704 0.04801753 0.05193363 0.04240564 0.0459704  0.04703937
 0.04306795 0.04179033 0.04765999 0.04884949 0.05125001 0.04598737
 0.04758916 0.04593375 0.05304887 0.04774485 0.04776585 0.05013692
 0.04373464 0.05252703 0.05386205 0.05289062 0.04493242 0.04673488
 0.04079951 0.05327408 0.0531063  0.05130919 0.03907994 0.05552883
 0.05197237 0.04508392 0.05105442 0.04417682 0.0446833  0.04715892
 0.04602564 0.04691175 0.04063663 0.04999768 0.0473776  0.04950021
 0.04887408 0.0479532  0.04446515 0.04781877 0.04215045 0.04815892
 0.04832831 0.04559469 0.04093663 0.04094109 0.05666898 0.0528321
 0.0463452  0.04711657 0.04681409 0.04400407 0.04648829 0.04420799
 0.04219354 0.05000773 0.04543221 0.05502167 0.05459576 0.04934285
 0.05045402 0.04553587]
attend-M
BLEU
[0.20071522 0.17289086 0.20266864 0.18661412 0.19330518 0.18291755
 0.19743614 0.19762358 0.18632815 0.17778944 0.19739531 0.20370425
 0.18612313 0.18993237 0.19175854 0.1780833  0.1781556  0.1980705
 0.18790372 0.1999683  0.19357643 0.19237998 0.18789383 0.20766675
 0.17985524 0.19940433 0.18548284 0.20366881 0.19821989 0.18496524
 0.20109135 0.18590527 0.18681416 0.19736961 0.18926393 0.1797645
 0.19663213 0.19599777 0.20200474 0.19147115 0.18463156 0.17832465
 0.19644231 0.19355123 0.16457618 0.18186027 0.18548243 0.20184136
 0.19256621 0.19110015 0.20366797 0.19930219 0.19820015 0.19731268
 0.19352277 0.19755991 0.20174658 0.19107323 0.20301069 0.1989451
 0.20084439 0.19865942 0.1737523  0.19962033 0.19667304 0.18854698
 0.19667155 0.18707352 0.19468079 0.20128988 0.19739078 0.1914707
 0.19755844 0.19951565 0.19860802 0.17832171 0.17932607 0.19337834
 0.19739756 0.19837137 0.18303029 0.18471944 0.21131561 0.18917479
 0.19922371 0.18800114 0.16465481 0.19676059 0.19321971 0.1884016
 0.18356764 0.20105065 0.19879635 0.18931627 0.19012372 0.19958319
 0.18543563 0.18972305 0.18979193 0.2008604  0.19146008 0.19530768
 0.18221272 0.17652493 0.19938377 0.20424817 0.19911724 0.19284169
 0.16190034 0.19627903 0.20205804 0.18909433 0.20317263 0.18550316
 0.1809822  0.20812389 0.18599181 0.17681486 0.19723104 0.18735231
 0.18696258 0.17027071 0.187795   0.18601303 0.18273581 0.1820852
 0.18556214 0.19798938 0.18760124 0.19007101 0.19369338 0.21122573
 0.19752104 0.18990955 0.20167906 0.18167855 0.18919101 0.19992592
 0.19053825 0.16761906 0.19409339 0.20274985 0.19532549 0.18319261
 0.18610815 0.18950881 0.19645334 0.20030882 0.19175004 0.20185772
 0.17508874 0.19638107 0.20408272 0.18829073 0.19409309 0.18408767
 0.17645164 0.18573093 0.20367215 0.20158394 0.1712824  0.20804484
 0.19581782 0.19369586 0.20713678 0.18272405 0.1870263  0.18020157
 0.18837116 0.19698719 0.17449108 0.18449942 0.19249456 0.19885068
 0.19209997 0.19622426 0.17648718 0.21313811 0.1754277  0.19669321
 0.19127249 0.17794848 0.17785463 0.17763364 0.20297677 0.20476012
 0.18720577 0.18618734 0.18752429 0.1883769  0.17898083 0.19047344
 0.18127298 0.1922942  0.18430273 0.19746836 0.19430002 0.20225084
 0.19618666 0.19663501]
attend-M
METEOR
[0.13990814 0.12428164 0.13583673 0.12974034 0.14132328 0.13295106
 0.1415824  0.1392863  0.12797035 0.12670972 0.14015262 0.14278708
 0.13485197 0.13253578 0.13578352 0.11715598 0.12671882 0.1385396
 0.13661632 0.14278321 0.13463118 0.13730029 0.13308622 0.14210019
 0.13426262 0.13725374 0.13175374 0.14446797 0.1439823  0.12739563
 0.14029915 0.13006196 0.13692636 0.1399593  0.13721219 0.1344412
 0.14244213 0.14103588 0.14365645 0.13925881 0.13867066 0.12850661
 0.14174121 0.13812821 0.11561216 0.13535657 0.1300495  0.14564131
 0.13790078 0.13515074 0.13743118 0.14081067 0.13960409 0.13631149
 0.1354593  0.13787701 0.13983684 0.13540291 0.13908224 0.1367455
 0.1385686  0.14317546 0.1216819  0.13109403 0.13787038 0.13810767
 0.14143026 0.13889177 0.14702386 0.14213649 0.13563974 0.13518447
 0.14023082 0.14610777 0.14017739 0.12534037 0.12679591 0.13843331
 0.13980563 0.13758547 0.1304539  0.14039993 0.14489454 0.13494595
 0.13625488 0.13878801 0.12464673 0.1372755  0.1318261  0.13819191
 0.13256157 0.14292778 0.13388531 0.13666464 0.13286801 0.14841869
 0.13577425 0.1385875  0.13344893 0.13932231 0.13615225 0.13890076
 0.13081486 0.12643361 0.13334432 0.14426415 0.14088227 0.13873367
 0.12017799 0.13592352 0.13549563 0.13088145 0.14305416 0.13161128
 0.13084701 0.1410732  0.13471402 0.12508156 0.14100534 0.13546945
 0.13431285 0.12553312 0.13850768 0.13409921 0.13057007 0.12627294
 0.12737186 0.14134475 0.13550749 0.13289098 0.13666991 0.14931309
 0.13812203 0.1324477  0.15170949 0.12280418 0.13023521 0.14158667
 0.13414    0.1247928  0.13871986 0.14148284 0.1407309  0.12947846
 0.13368919 0.13511572 0.13206313 0.146174   0.14104506 0.14292474
 0.12308525 0.13812977 0.1430838  0.13448414 0.13211122 0.1266938
 0.12565053 0.13354339 0.14237546 0.14090525 0.12912048 0.14260615
 0.14168126 0.13702248 0.14278577 0.1302532  0.13222899 0.13309527
 0.13217682 0.13452566 0.1248206  0.13899295 0.13963256 0.13924354
 0.13054968 0.13626167 0.13214814 0.14627599 0.12846073 0.14122988
 0.14337636 0.12882269 0.12892519 0.12887747 0.14055658 0.14444497
 0.13785962 0.1346845  0.12942461 0.13447074 0.12854713 0.14111704
 0.13422917 0.13545983 0.13276315 0.13393795 0.13573253 0.14233023
 0.13957556 0.14311706]
attend-M
BERT
[0.7904125  0.78608537 0.7883752  0.78800523 0.78990936 0.78747183
 0.7898114  0.791262   0.789719   0.78789926 0.7895622  0.78886443
 0.78749794 0.78902674 0.79095364 0.78709805 0.78772783 0.7890695
 0.7873682  0.78935933 0.78674656 0.7907186  0.7905492  0.7881274
 0.7878562  0.78702366 0.78697276 0.7902361  0.7887437  0.78556347
 0.79013866 0.789098   0.789915   0.7882971  0.7885217  0.78717864
 0.7882693  0.7887612  0.7888217  0.78788614 0.7908456  0.7860448
 0.788474   0.78900784 0.7858575  0.7856269  0.78794736 0.7907934
 0.7890543  0.7880084  0.78843457 0.7889253  0.7907097  0.78866094
 0.7892812  0.7884414  0.7890698  0.79020107 0.7924033  0.7898094
 0.78706896 0.78967637 0.78489166 0.78769046 0.78710884 0.78790945
 0.7877077  0.78796995 0.78888464 0.78985494 0.79129577 0.7884218
 0.7893399  0.79158616 0.7908166  0.78803223 0.7863886  0.7889905
 0.78832394 0.7902935  0.78737026 0.79089785 0.79063714 0.78874964
 0.7897749  0.7875841  0.78821874 0.7909324  0.7876085  0.7870754
 0.7887707  0.79088515 0.7899486  0.78717256 0.7892763  0.7887909
 0.7887363  0.7868108  0.7882203  0.79048306 0.7887507  0.7893071
 0.7878496  0.78523874 0.7903517  0.78972524 0.7898967  0.788949
 0.78513724 0.790252   0.7890225  0.7869491  0.7913777  0.78813195
 0.78803974 0.7919032  0.78957444 0.7857349  0.78851235 0.7889351
 0.78939307 0.7866546  0.7885532  0.7891623  0.7883582  0.78522855
 0.78558534 0.7893799  0.79081583 0.78744924 0.7890791  0.7889905
 0.791197   0.7899285  0.7911315  0.7859117  0.7882861  0.7922485
 0.78818965 0.7884226  0.79125416 0.7882755  0.7886548  0.78830636
 0.7881163  0.79004496 0.7895681  0.7894639  0.78895116 0.7913877
 0.78744596 0.78962934 0.7880789  0.78501534 0.7891926  0.78848493
 0.78760785 0.78942055 0.7873038  0.7909787  0.7862602  0.7904019
 0.7877838  0.79108304 0.790258   0.78897154 0.78836536 0.7864786
 0.7869056  0.7913251  0.78835803 0.79006904 0.7870431  0.78964245
 0.7893008  0.7888676  0.78668433 0.7922802  0.78710747 0.7879097
 0.7911021  0.7871672  0.7874435  0.78601044 0.78766984 0.78850514
 0.7891038  0.78837734 0.7884089  0.79043174 0.7876213  0.79092884
 0.788578   0.78838944 0.7881835  0.7908108  0.78821445 0.78891444
 0.7893823  0.79111356]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
attend-F
WER
[-0.20566555 -0.21647737 -0.20361282 -0.20385411 -0.21371992 -0.20424801
 -0.21821687 -0.21198653 -0.21602159 -0.22348652 -0.21570253 -0.21762384
 -0.21467321 -0.2163627  -0.21491821 -0.22189394 -0.21129524 -0.21112324
 -0.21808759 -0.21526395 -0.22544972 -0.21359761 -0.21313451 -0.20587418
 -0.20376952 -0.21487163 -0.22399096 -0.21298549 -0.21104637 -0.20367228
 -0.21904675 -0.20864391 -0.21350124 -0.21167395 -0.21765264 -0.20868346
 -0.21116301 -0.21505145 -0.21703299 -0.22090063 -0.21483984 -0.21913731
 -0.21003216 -0.20200924 -0.20446239 -0.20450066 -0.22079068 -0.22401877
 -0.21458622 -0.21700368 -0.21569951 -0.21335059 -0.20168835 -0.20670102
 -0.2055007  -0.21119691 -0.21953112 -0.21267528 -0.21555478 -0.22939895
 -0.19989821 -0.21360328 -0.20507324 -0.21119991 -0.21558351 -0.22002925
 -0.21393097 -0.2131196  -0.20593901 -0.21935944 -0.20925861 -0.21193956
 -0.22523337 -0.20893625 -0.22519021 -0.23036054 -0.21692964 -0.22307549
 -0.2111851  -0.20960396 -0.21122626 -0.21123438 -0.22015992 -0.2163247
 -0.21912335 -0.2203034  -0.22027746 -0.20096023 -0.21416343 -0.20644745
 -0.21180105 -0.21549956 -0.21419989 -0.21143101 -0.21354686 -0.2152876
 -0.22358668 -0.21727538 -0.21559868 -0.2077615  -0.2111836  -0.20294104
 -0.22043718 -0.21681763 -0.21159497 -0.20326346 -0.21346468 -0.20625015
 -0.19544884 -0.21424434 -0.21141827 -0.20256636 -0.21564342 -0.21371973
 -0.20923984 -0.21821265 -0.21881856 -0.20913396 -0.23071301 -0.21906892
 -0.22135041 -0.21765259 -0.2061011  -0.20855007 -0.22029425 -0.21191863
 -0.22271355 -0.21112494 -0.21193772 -0.2024635  -0.20643186 -0.2169462
 -0.21908695 -0.21548733 -0.2111833  -0.22315695 -0.21252994 -0.20930379
 -0.21463024 -0.2109491  -0.21632598 -0.21680765 -0.20998193 -0.22115419
 -0.20051806 -0.20973984 -0.20801359 -0.21331851 -0.20270311 -0.22284065
 -0.22189494 -0.20405982 -0.19832272 -0.20637975 -0.21750546 -0.22094294
 -0.21874102 -0.20992585 -0.22139918 -0.22545831 -0.22119908 -0.2076041
 -0.2162201  -0.22957111 -0.21197762 -0.21493171 -0.2144535  -0.20846785
 -0.21272695 -0.2301413  -0.21468277 -0.21839813 -0.21193027 -0.22621924
 -0.21504917 -0.21296932 -0.21724607 -0.19801676 -0.21341619 -0.20358615
 -0.20670086 -0.20733998 -0.22382115 -0.21105462 -0.21533031 -0.20841127
 -0.21170494 -0.2155882  -0.21818226 -0.22307661 -0.20079498 -0.21198356
 -0.21236942 -0.20880692 -0.22227716 -0.22013942 -0.21228963 -0.20543778
 -0.20788227 -0.20801123]
attend-F
BLEU
[0.21165166 0.1734937  0.1996592  0.1915016  0.19885847 0.20469329
 0.18577924 0.17938701 0.18420042 0.17254443 0.17103243 0.18565895
 0.1878164  0.17704503 0.16934966 0.15820606 0.19097174 0.18778985
 0.17552981 0.17589005 0.15735839 0.18733673 0.18705177 0.19455885
 0.19675166 0.16615218 0.14773999 0.17940861 0.19502578 0.19580485
 0.18170131 0.18617109 0.1900228  0.1880719  0.16962287 0.18750663
 0.19005489 0.18798233 0.19452392 0.176301   0.17557275 0.1731449
 0.19050024 0.18298083 0.18744339 0.1943404  0.1748638  0.18207096
 0.18636889 0.18096516 0.19030283 0.19356677 0.19704127 0.18941314
 0.18819251 0.18555329 0.1682338  0.17372169 0.18264761 0.16438001
 0.20239206 0.19266835 0.18163532 0.19055248 0.1788286  0.18635089
 0.18814342 0.20133522 0.20140028 0.1751693  0.18890892 0.1856033
 0.15148388 0.19776019 0.15280255 0.1776784  0.17435786 0.15749429
 0.18821217 0.1877348  0.17358196 0.18982948 0.18530193 0.17752202
 0.1781008  0.17099779 0.18039322 0.18831774 0.20208921 0.19686395
 0.19362318 0.18535042 0.17294715 0.19129734 0.19840051 0.16972262
 0.1685678  0.18116867 0.18914248 0.18577479 0.17688944 0.2001288
 0.17586223 0.18351646 0.19934413 0.19162255 0.18796384 0.19309547
 0.19826484 0.18000029 0.17813291 0.20127954 0.17944378 0.18287161
 0.19579057 0.16660916 0.17773546 0.17444917 0.16233609 0.18007018
 0.18088059 0.17253485 0.19027829 0.18091778 0.17614849 0.17832488
 0.17356591 0.18532053 0.17001521 0.18968528 0.17460007 0.17208197
 0.1643748  0.17393927 0.20150836 0.1727985  0.18353395 0.19874569
 0.18318477 0.18929478 0.16741998 0.18804389 0.18712207 0.18659928
 0.19727737 0.18510124 0.20171636 0.18478969 0.18588246 0.16915916
 0.18466282 0.20230288 0.21249577 0.18142946 0.16963468 0.18479291
 0.17620464 0.19821879 0.17525933 0.16500876 0.17295956 0.17985612
 0.18657603 0.16276257 0.1814935  0.17484685 0.16035288 0.18889072
 0.18196181 0.16973551 0.18647676 0.17400152 0.18502235 0.16813264
 0.17451036 0.1989854  0.17652305 0.22550849 0.17062125 0.18988116
 0.20088126 0.18192053 0.16774322 0.1835195  0.18568141 0.18221801
 0.18769692 0.17025129 0.16413515 0.15247975 0.19694932 0.18982278
 0.19179778 0.20395785 0.1627966  0.1750214  0.1970455  0.20612407
 0.18505735 0.19081085]
attend-F
METEOR
[0.15186781 0.12916412 0.15219907 0.1496777  0.1462575  0.14785054
 0.13442063 0.13768733 0.13125383 0.13023203 0.1251958  0.14389737
 0.14380891 0.13397004 0.12288381 0.12160612 0.14440124 0.13875286
 0.13337398 0.12943964 0.12291936 0.13857304 0.14599277 0.14583512
 0.1460498  0.12565761 0.11569074 0.13480108 0.14211523 0.14667725
 0.13886634 0.13794784 0.14379666 0.14074952 0.12797816 0.14606248
 0.13977124 0.13900389 0.14552459 0.13944481 0.13712941 0.12720297
 0.14272557 0.13596307 0.14116004 0.14717736 0.13415829 0.13558807
 0.13837671 0.13713704 0.14034163 0.14895277 0.15216155 0.14053268
 0.14825513 0.14122177 0.12807719 0.12653139 0.13884758 0.12090759
 0.14905832 0.14956554 0.13794099 0.14170279 0.13557195 0.1398985
 0.13802362 0.15192348 0.14649996 0.13216247 0.14287745 0.14077801
 0.11807181 0.14885615 0.11540907 0.13185019 0.12544447 0.1225261
 0.13468089 0.13521405 0.12834205 0.14052555 0.14295507 0.12967474
 0.14140576 0.13436326 0.12922827 0.14228072 0.14540665 0.14882319
 0.13908741 0.13796761 0.1310288  0.14151395 0.15114625 0.12645724
 0.1273123  0.13597787 0.14327691 0.13977611 0.13408034 0.1508157
 0.13437618 0.13683387 0.14801362 0.14360273 0.13685607 0.14194235
 0.15110793 0.13270694 0.13086443 0.14668205 0.13399514 0.13067275
 0.15239343 0.13010996 0.13589374 0.13139842 0.12862207 0.13754515
 0.13207241 0.12690341 0.14362017 0.14277263 0.13030597 0.1399292
 0.12465996 0.13446345 0.12800481 0.14171742 0.12888898 0.13098259
 0.12725025 0.1339785  0.1512306  0.13555138 0.14590891 0.14969959
 0.13724573 0.14156887 0.1299732  0.13800762 0.14287651 0.14005304
 0.14814049 0.12762483 0.14518209 0.1328604  0.14236129 0.12427048
 0.13967898 0.15223714 0.15806232 0.13726581 0.12487055 0.13522451
 0.13119504 0.14596913 0.12604814 0.12406194 0.13326365 0.13737527
 0.13809549 0.12331188 0.13599845 0.13569525 0.1302112  0.1422425
 0.13411166 0.12168677 0.13580269 0.13104695 0.13837946 0.12504512
 0.12769974 0.13907219 0.13272696 0.17226011 0.12653884 0.14432768
 0.14324516 0.13306128 0.1305742  0.13109867 0.13808986 0.14090196
 0.14006558 0.12634864 0.12309562 0.12124503 0.14587466 0.13994863
 0.14296165 0.15177774 0.12431056 0.1312497  0.1461072  0.15119946
 0.13838805 0.14150359]
attend-F
BERT
[0.79047966 0.78764856 0.79160106 0.79107666 0.79153323 0.7906234
 0.78955704 0.7906607  0.7908235  0.78766567 0.7876213  0.78906435
 0.7896561  0.7892681  0.78735846 0.78727984 0.7909024  0.7879376
 0.78645396 0.78807634 0.78771496 0.7909983  0.78761065 0.7901553
 0.7892072  0.78738534 0.7860221  0.7880307  0.7917474  0.78890735
 0.7877436  0.7877228  0.7898954  0.79001683 0.78894573 0.78817296
 0.79046553 0.7893065  0.7908862  0.78856957 0.7882393  0.78965926
 0.7893867  0.7896711  0.7915909  0.7893189  0.78697735 0.7904005
 0.7892805  0.7891889  0.78770566 0.78804415 0.79012895 0.7889806
 0.7891416  0.78818333 0.78796506 0.78798056 0.7897852  0.78832
 0.7873332  0.7895175  0.7879213  0.78849894 0.78652555 0.7892111
 0.79040277 0.7900916  0.78869027 0.7886941  0.79078174 0.79002035
 0.78485173 0.79133415 0.78601253 0.7863282  0.7885303  0.78535604
 0.7900804  0.7918865  0.78898376 0.792035   0.7895292  0.78719383
 0.78766316 0.7908208  0.787699   0.7883596  0.7926164  0.7872868
 0.7873979  0.7897903  0.7876937  0.78835976 0.7889558  0.7873657
 0.78501403 0.7904319  0.78894085 0.79025054 0.79093754 0.79123574
 0.7875206  0.789513   0.79107314 0.7911668  0.7909979  0.7900189
 0.79190415 0.7876385  0.7865617  0.7885443  0.7878785  0.78715813
 0.7884984  0.7884533  0.7897465  0.78783464 0.7873757  0.7895726
 0.7881613  0.7873637  0.7860673  0.7881177  0.7872753  0.7886462
 0.79099524 0.78883153 0.7863233  0.7906186  0.78722763 0.7905761
 0.78830254 0.78792    0.7914396  0.78717303 0.78905696 0.79194844
 0.790341   0.7872569  0.78672856 0.7910279  0.78936356 0.7906682
 0.78956556 0.78834075 0.78938115 0.7901975  0.78827757 0.7885958
 0.7893     0.7904211  0.7891658  0.78933465 0.7889277  0.7881427
 0.7908678  0.7912752  0.7881823  0.7891861  0.78921187 0.78886276
 0.78910154 0.7861074  0.7892516  0.78730273 0.78713405 0.79000795
 0.7879998  0.7879723  0.7876376  0.7891146  0.7896417  0.78793985
 0.7856958  0.7893677  0.7874228  0.7927136  0.78742784 0.78837156
 0.79103655 0.7891155  0.7864037  0.78924    0.78987235 0.7870699
 0.7891835  0.78652215 0.7854924  0.7867871  0.78881097 0.78953373
 0.7901439  0.79031074 0.7866773  0.78828895 0.78992265 0.78947175
 0.7884608  0.7897881 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
alpha_repeat-1
WER
[-0.25300654 -0.23440319 -0.21439079 -0.23238348 -0.24353537 -0.22803021
 -0.23336798 -0.25789907 -0.23334442 -0.25110599 -0.19456593 -0.22173097
 -0.23371476 -0.23484354 -0.2547289  -0.20670043 -0.25723291 -0.22294566
 -0.19530558 -0.2152853  -0.23174038 -0.23824691 -0.25087134 -0.24715919
 -0.22457698 -0.24765105 -0.2741634  -0.20879643 -0.21391671 -0.25183763
 -0.23008529 -0.2165763  -0.24849098 -0.23463155 -0.19783329 -0.24039579
 -0.24604092 -0.26287547 -0.24427631 -0.23486013 -0.22669698 -0.26470305
 -0.20434649 -0.23223267 -0.26876512 -0.2218326  -0.23128564 -0.21496276
 -0.22720203 -0.23938653 -0.20246913 -0.21765135 -0.25711396 -0.22802145
 -0.2317442  -0.26444942 -0.23653662 -0.26430955 -0.28165118 -0.21270283
 -0.22171393 -0.21867597 -0.26087171 -0.23554964 -0.20718333 -0.24755948
 -0.20249281 -0.25713742 -0.23811966 -0.22018988 -0.27825995 -0.22672041
 -0.22637614 -0.24794116 -0.2515522  -0.22337414 -0.24910684 -0.22438273
 -0.24171978 -0.22808918 -0.24804557 -0.23651694 -0.22257957 -0.23252841
 -0.21540619 -0.24738492 -0.250312   -0.23227772 -0.22516869 -0.19385787
 -0.24078273 -0.22407243 -0.18799289 -0.21704112 -0.23268993 -0.21751011
 -0.20997969 -0.26378778 -0.22487942 -0.21566711 -0.24117565 -0.26987528
 -0.24937571 -0.23815279 -0.24075842 -0.24940479 -0.27366546 -0.22041964
 -0.23158205 -0.22548263 -0.21990892 -0.2403624  -0.25514943 -0.24447147
 -0.26692167 -0.23740433 -0.22517351 -0.2581444  -0.2187548  -0.24263296
 -0.22457666 -0.23549285 -0.19401289 -0.210364   -0.23441566 -0.21798285
 -0.21760244 -0.25538318 -0.2603845  -0.21800795 -0.24386361 -0.26447374
 -0.20871798 -0.23416976 -0.241081   -0.24116646 -0.21451029 -0.24703444
 -0.22485568 -0.23823329 -0.2040566  -0.22650656 -0.21865325 -0.24417277
 -0.21436906 -0.24237948 -0.23061176 -0.24292505 -0.23577021 -0.20095575
 -0.2372749  -0.21527581 -0.24021312 -0.25023907 -0.19284529 -0.2081058
 -0.20157938 -0.27184959 -0.24907246 -0.2189515  -0.19957515 -0.22450077
 -0.23919129 -0.23410928 -0.25071474 -0.21600081 -0.22307497 -0.24900581
 -0.21791524 -0.23169697 -0.23382425 -0.24031746 -0.22957036 -0.23586757
 -0.25694249 -0.25068009 -0.22219288 -0.20575206 -0.26414458 -0.25636224
 -0.24893155 -0.21842984 -0.22769007 -0.19658124 -0.25522154 -0.23864077
 -0.23218355 -0.24874802 -0.21555095 -0.23989238 -0.2580059  -0.20979759
 -0.23127974 -0.19307252 -0.25011671 -0.23417187 -0.21533943 -0.22347315
 -0.26179344 -0.22903165]
alpha_repeat-1
BLEU
[0.19790812 0.17869817 0.17226309 0.13770363 0.12731434 0.15368197
 0.15801499 0.14372359 0.18380672 0.12925933 0.21774358 0.19234892
 0.1537685  0.15468839 0.15100265 0.16832691 0.16239899 0.20524998
 0.2191498  0.20822252 0.21194454 0.16418596 0.18338487 0.19637889
 0.17195697 0.12655488 0.13227596 0.23191184 0.1874702  0.12227207
 0.16256121 0.21403717 0.18117308 0.19571679 0.21110547 0.13413562
 0.19772775 0.15637494 0.17715875 0.18657974 0.17519038 0.13110421
 0.17697032 0.17724205 0.11731178 0.17145633 0.17707981 0.21089329
 0.20641932 0.1765784  0.20656127 0.16265014 0.14085541 0.18898041
 0.17071403 0.12991425 0.16246922 0.15040522 0.10280805 0.20519535
 0.20879984 0.19391813 0.18370251 0.21414883 0.19836661 0.18371507
 0.21218282 0.15024969 0.12394089 0.22226699 0.13459494 0.22802967
 0.19597189 0.19117633 0.20314766 0.19426685 0.17963924 0.18564639
 0.16443257 0.13777467 0.16799718 0.1754714  0.19328555 0.16586187
 0.17875319 0.19862608 0.15234496 0.21036688 0.18980635 0.2140947
 0.20993057 0.19246685 0.18680168 0.20481534 0.22248293 0.17252663
 0.22026426 0.13571027 0.1935858  0.20557786 0.14905544 0.12759121
 0.1611802  0.16645245 0.16714738 0.15056156 0.10854591 0.20601452
 0.15487361 0.17705266 0.25439239 0.17363959 0.16050313 0.13229448
 0.13767409 0.1886683  0.19242443 0.16948129 0.19720356 0.13700128
 0.19095627 0.18069839 0.23364063 0.20463067 0.181416   0.17259877
 0.21700568 0.18252522 0.12960809 0.18974016 0.12747542 0.11420199
 0.22734093 0.1728832  0.18028887 0.20153603 0.21513397 0.17201315
 0.19727919 0.19962784 0.19704522 0.17506281 0.19594265 0.13673902
 0.19728982 0.210184   0.17489105 0.17009817 0.18679232 0.22147461
 0.16617557 0.19494018 0.17899473 0.12092859 0.21658974 0.1752091
 0.1943505  0.10807091 0.14930713 0.17004307 0.22553315 0.21142954
 0.14527848 0.1856918  0.19513766 0.23431997 0.17904854 0.18656238
 0.19524054 0.17526316 0.18498586 0.15559226 0.1815101  0.1694991
 0.15544566 0.18825544 0.19125513 0.20857046 0.1232507  0.16972439
 0.17167349 0.19340321 0.16912363 0.22987747 0.15998238 0.19031735
 0.18997674 0.17806675 0.1861412  0.15175616 0.1683009  0.22263494
 0.19141695 0.23257055 0.15581378 0.17359423 0.19881998 0.21481816
 0.16013317 0.19494294]
alpha_repeat-1
METEOR
[0.17464194 0.13379883 0.14086397 0.11358984 0.10166586 0.1271583
 0.13089726 0.12730619 0.14733719 0.10742376 0.17267562 0.13673417
 0.14861715 0.14328193 0.11585976 0.14225734 0.11980595 0.16664524
 0.16935435 0.16520193 0.17755533 0.13690667 0.15443412 0.15512395
 0.12972993 0.1029486  0.10513864 0.18337876 0.18064981 0.11982851
 0.12313365 0.17210593 0.14116053 0.13890204 0.1844554  0.10767473
 0.15230788 0.12345446 0.14465175 0.14225961 0.14713252 0.09374271
 0.13270141 0.13359771 0.08926251 0.12960013 0.11896105 0.15542423
 0.16267498 0.15416048 0.14679831 0.13500208 0.12793366 0.16821875
 0.13126491 0.10670181 0.1282105  0.11644068 0.08013863 0.1610579
 0.16704912 0.16579698 0.14027328 0.167369   0.16433346 0.12970061
 0.16610582 0.13542021 0.09170311 0.1608302  0.10413701 0.16732817
 0.14506933 0.14906526 0.1427185  0.14331638 0.14053131 0.13251793
 0.12968972 0.11515601 0.13689898 0.13575301 0.16270248 0.12421689
 0.13621026 0.14475879 0.12618601 0.16025028 0.13407128 0.17372666
 0.15430918 0.14362276 0.15160068 0.17424277 0.15964613 0.12743825
 0.17572783 0.09558826 0.15136933 0.15820011 0.11398687 0.11143661
 0.11499017 0.14946446 0.1297477  0.1318913  0.0808102  0.15462182
 0.13964811 0.14641699 0.20469247 0.13953177 0.14869605 0.11446812
 0.10583028 0.14703145 0.1442349  0.13081637 0.1820135  0.11454226
 0.14668558 0.13454821 0.16964862 0.16010738 0.15188902 0.12452786
 0.15984623 0.12786033 0.10483106 0.13996174 0.10792158 0.09244799
 0.1816772  0.12702805 0.13146937 0.15060243 0.1760693  0.14283109
 0.14611564 0.15126387 0.14486082 0.14577176 0.16002477 0.10676064
 0.18890707 0.16037995 0.13563817 0.1378452  0.13972861 0.16046387
 0.12076808 0.14493558 0.14842824 0.09564895 0.18560909 0.1471481
 0.15023542 0.09703403 0.11927035 0.11680358 0.17144056 0.15652106
 0.1144631  0.1415379  0.14640396 0.17834897 0.13522177 0.15042578
 0.1734927  0.14098467 0.14116036 0.12802946 0.14294781 0.1458812
 0.12836986 0.14400834 0.13631278 0.18733096 0.11605255 0.11690974
 0.13315192 0.16494813 0.125203   0.17827101 0.13919018 0.15638982
 0.15920593 0.13881516 0.12857492 0.11428978 0.1228853  0.17773478
 0.13950925 0.17571628 0.12146139 0.13186546 0.16242285 0.17200831
 0.13107123 0.15623552]
alpha_repeat-1
BERT
[0.798238   0.7956463  0.79818416 0.789093   0.78805643 0.7898953
 0.7954764  0.79196596 0.7978164  0.7924121  0.7954109  0.79548925
 0.7936742  0.79376084 0.7927029  0.79293007 0.7914269  0.7965763
 0.79435384 0.79649466 0.7993128  0.7951859  0.79619485 0.7955968
 0.7943292  0.7911276  0.7893833  0.7927022  0.79627144 0.79353696
 0.7924184  0.7963507  0.79276407 0.7927797  0.7974139  0.79404163
 0.79522777 0.7957767  0.7956408  0.79465437 0.79445106 0.79003704
 0.7920746  0.7928841  0.7882599  0.7974032  0.79667926 0.7996476
 0.79487014 0.7960374  0.7965637  0.79697347 0.7900222  0.79439
 0.79602677 0.7956103  0.7960878  0.7873401  0.79046875 0.8001555
 0.7967106  0.7937473  0.79433924 0.7959185  0.79944676 0.7885256
 0.7969251  0.7921504  0.79711694 0.7982833  0.7892348  0.7974679
 0.7980187  0.79366463 0.79350483 0.7958505  0.79554933 0.7910331
 0.79117405 0.8007409  0.797938   0.7944943  0.7963567  0.79395956
 0.79369193 0.79371625 0.7911858  0.79691064 0.79231626 0.7972777
 0.7938689  0.7973671  0.79281664 0.79403734 0.79386383 0.7918503
 0.7964336  0.7878526  0.79677564 0.79391277 0.78906566 0.79302883
 0.791281   0.7931429  0.79662305 0.79365903 0.7927001  0.79954624
 0.7910417  0.80039096 0.7952401  0.78809327 0.7994329  0.79351157
 0.7905166  0.7947531  0.7913972  0.7924669  0.8043755  0.79459715
 0.7938169  0.7924808  0.7917206  0.79634976 0.7937862  0.78996193
 0.8001443  0.7910856  0.7964468  0.7938669  0.7903113  0.7912316
 0.8039601  0.7905839  0.7906     0.79338056 0.80151117 0.791352
 0.79523563 0.79650676 0.7987659  0.79474694 0.79648256 0.7885764
 0.8005481  0.7943171  0.7953481  0.7913138  0.79230917 0.79343134
 0.792098   0.79392403 0.79421544 0.7966743  0.8006681  0.7954111
 0.78894144 0.794114   0.79120785 0.799022   0.7944923  0.7960535
 0.7893139  0.79683584 0.793663   0.7977593  0.7877851  0.79906726
 0.79652834 0.79253256 0.79522043 0.7943253  0.79483616 0.79627126
 0.78969693 0.7957624  0.79304206 0.79606    0.7923971  0.7948934
 0.7939773  0.7938133  0.79046184 0.79482234 0.78928536 0.7977887
 0.7963389  0.79302514 0.7946033  0.7916911  0.7931885  0.80015665
 0.7949962  0.795807   0.79627365 0.79182255 0.7943162  0.7956351
 0.7894987  0.7954135 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
alpha_repeat-2
WER
[-0.17110872 -0.13893212 -0.19639645 -0.20439641 -0.18515336 -0.20820687
 -0.19347912 -0.17526097 -0.1745637  -0.20138661 -0.19205773 -0.15273092
 -0.18378616 -0.18383903 -0.18852721 -0.13030348 -0.2066972  -0.18369056
 -0.17309048 -0.14008482 -0.17544103 -0.15221319 -0.19746163 -0.17835351
 -0.18855554 -0.20878225 -0.17075547 -0.201335   -0.16887893 -0.17730637
 -0.15626505 -0.21006553 -0.18001634 -0.17927111 -0.17710568 -0.1535764
 -0.19216703 -0.19535245 -0.17180986 -0.16915644 -0.20543437 -0.18339441
 -0.18370513 -0.17768739 -0.19120816 -0.18111708 -0.14464045 -0.17965406
 -0.16985471 -0.17607323 -0.19210171 -0.14142276 -0.20092683 -0.20755809
 -0.17845974 -0.17366412 -0.19564396 -0.19245882 -0.18134972 -0.17958151
 -0.19583371 -0.17539669 -0.18241452 -0.18509229 -0.18917134 -0.18946691
 -0.15883175 -0.19432368 -0.19515695 -0.17374282 -0.15607524 -0.17932853
 -0.16877615 -0.18240956 -0.20084083 -0.17257527 -0.20276178 -0.17187086
 -0.17637615 -0.14630446 -0.1736797  -0.16944956 -0.18412304 -0.14315831
 -0.19376463 -0.184366   -0.17438548 -0.13874958 -0.18404313 -0.16153587
 -0.14220742 -0.16980447 -0.16885408 -0.16056316 -0.17254487 -0.14571943
 -0.18350666 -0.19947216 -0.14372547 -0.17722208 -0.12773167 -0.18716322
 -0.18778701 -0.16609081 -0.21006057 -0.16798756 -0.18653313 -0.16272438
 -0.20311532 -0.17423748 -0.15603708 -0.18258746 -0.20410773 -0.18544318
 -0.20771076 -0.15467507 -0.17959982 -0.20298106 -0.19187121 -0.18063844
 -0.13973624 -0.20572183 -0.15199032 -0.16357217 -0.20787402 -0.180059
 -0.17789738 -0.1788583  -0.19566839 -0.1928559  -0.20261731 -0.18050706
 -0.16854775 -0.16944169 -0.1804219  -0.17752353 -0.17122266 -0.16893257
 -0.17787001 -0.16355317 -0.17639401 -0.15047681 -0.17167869 -0.20093214
 -0.16765972 -0.14858409 -0.18959203 -0.20367186 -0.16907596 -0.17728724
 -0.16785881 -0.17075818 -0.15782079 -0.16690749 -0.16318113 -0.18136302
 -0.14376098 -0.15670853 -0.19861258 -0.16856643 -0.17731525 -0.17994383
 -0.17671628 -0.17287669 -0.17181658 -0.19342772 -0.1505811  -0.19429713
 -0.17034552 -0.14801973 -0.18541121 -0.15611337 -0.19354725 -0.16586535
 -0.16758722 -0.17598848 -0.16586176 -0.1914362  -0.18687054 -0.14946223
 -0.17727787 -0.14326527 -0.1699232  -0.17293096 -0.18600447 -0.20798265
 -0.18102379 -0.19317251 -0.19186266 -0.20039757 -0.13166106 -0.16408798
 -0.15958791 -0.19276654 -0.16867761 -0.20242592 -0.16514446 -0.17767664
 -0.18422917 -0.1468368 ]
alpha_repeat-2
BLEU
[0.18058146 0.21523017 0.13033509 0.15395097 0.13376028 0.15461561
 0.19471514 0.1599776  0.22652529 0.14076771 0.16794395 0.23481862
 0.18661914 0.14803362 0.15956941 0.24435322 0.1948892  0.18244255
 0.20404013 0.26673007 0.19899324 0.21295051 0.18003222 0.19422352
 0.18746793 0.08964    0.27666872 0.14203153 0.22568702 0.19649248
 0.23327652 0.13225514 0.21896697 0.16590608 0.20569804 0.23396629
 0.1750844  0.144887   0.21433865 0.1986962  0.15195891 0.18614618
 0.19373172 0.20360266 0.16890789 0.17769842 0.23926011 0.20772534
 0.2027116  0.20650955 0.17427498 0.2449351  0.15264392 0.19406793
 0.19441965 0.13857566 0.14754596 0.1448742  0.2024983  0.1580198
 0.16061394 0.19304727 0.16489417 0.14539945 0.20394865 0.13601051
 0.18383045 0.15929815 0.17232459 0.19361    0.18590927 0.16456947
 0.17342645 0.1904529  0.17406004 0.19474763 0.12485186 0.17978952
 0.18621278 0.20475508 0.18488572 0.24437098 0.19885273 0.21335144
 0.14820311 0.13703422 0.19618874 0.21003572 0.17883095 0.2127897
 0.19709019 0.20640733 0.22518939 0.21300251 0.17784363 0.18355957
 0.19151403 0.18038159 0.21680804 0.22182177 0.21015804 0.21808619
 0.12015145 0.17816232 0.1418585  0.21045561 0.18955066 0.22115076
 0.17421187 0.16316883 0.19942114 0.12778262 0.15460945 0.15791401
 0.13439954 0.2049638  0.15133542 0.12102336 0.16347478 0.18876973
 0.20360481 0.14078754 0.22968344 0.19994323 0.14915474 0.1929527
 0.20132003 0.1677295  0.14997613 0.18175773 0.12249479 0.1812167
 0.19547643 0.20067958 0.15463913 0.17853367 0.17933589 0.21456216
 0.14255788 0.18323178 0.16484815 0.19771999 0.2144416  0.15170588
 0.19599489 0.27374422 0.22369837 0.14966293 0.19717932 0.17437829
 0.20583194 0.22363929 0.1798247  0.19408411 0.20543393 0.17607413
 0.22475936 0.17467592 0.14504322 0.16932057 0.19296213 0.21948738
 0.18247498 0.23776674 0.1895335  0.17860355 0.19285477 0.15908441
 0.18916884 0.22791926 0.20752819 0.19396801 0.17614513 0.17007086
 0.24825803 0.17739797 0.21076216 0.14848298 0.19750363 0.21925175
 0.17227198 0.23513842 0.17698132 0.22806677 0.16526196 0.15827012
 0.18427159 0.15045155 0.16586522 0.17420858 0.1934037  0.18333066
 0.16333778 0.17823031 0.22563195 0.14920919 0.22519718 0.20005894
 0.17487308 0.23653597]
alpha_repeat-2
METEOR
[0.12714663 0.16714938 0.12480419 0.11830918 0.11434035 0.13861541
 0.13711941 0.12124751 0.17178309 0.12281854 0.13669335 0.18185199
 0.12896454 0.10626057 0.11284347 0.17074596 0.14440696 0.12527309
 0.1561714  0.18223497 0.13584726 0.15709947 0.14732032 0.14094817
 0.13898585 0.07635896 0.18527203 0.11020496 0.17033609 0.14719314
 0.1706755  0.10561416 0.15036493 0.12060906 0.17268255 0.15596479
 0.12291547 0.11659576 0.16557981 0.15019208 0.10908625 0.14550044
 0.14317006 0.14755102 0.15206145 0.14183971 0.18521089 0.15323587
 0.16733871 0.1453805  0.14001939 0.19041565 0.12937764 0.15244519
 0.12812311 0.1022996  0.10448965 0.10849398 0.16848374 0.11468094
 0.11651086 0.12865038 0.12993375 0.10232201 0.143171   0.10510418
 0.15787471 0.13806947 0.12480755 0.16213312 0.16036664 0.12106434
 0.12774442 0.15080665 0.12312015 0.13785864 0.09889277 0.12643443
 0.14346466 0.15251548 0.13167337 0.18593926 0.15420937 0.17054227
 0.10236117 0.10103106 0.14115154 0.16007198 0.14398586 0.1617656
 0.14864201 0.15035826 0.19061662 0.14920534 0.13922271 0.14626854
 0.1452756  0.13041351 0.15566959 0.15384626 0.14897973 0.1561923
 0.10982856 0.11611425 0.09794998 0.14758037 0.13996068 0.16582182
 0.14055837 0.11761222 0.14324824 0.11791581 0.11940646 0.13426947
 0.10186419 0.15369324 0.1222681  0.1068869  0.12414222 0.13077225
 0.16742691 0.102595   0.16203983 0.15409261 0.10787491 0.12811707
 0.14514942 0.13054278 0.11163874 0.12454851 0.0972636  0.13228342
 0.14889457 0.15670926 0.12681561 0.14244614 0.13643933 0.16335502
 0.11328689 0.13811001 0.12504715 0.13314682 0.16195601 0.10842695
 0.13800638 0.21445954 0.18110246 0.10560011 0.13965332 0.13151819
 0.14045527 0.15509569 0.14515228 0.14111168 0.15814399 0.1183802
 0.16247259 0.13440572 0.10213015 0.15190701 0.14788551 0.14453587
 0.14080717 0.16641246 0.15104481 0.140887   0.15774651 0.11056784
 0.1561904  0.16342729 0.17123058 0.1427292  0.12815963 0.13546646
 0.17498411 0.14079103 0.15891719 0.11851876 0.15230939 0.1654754
 0.12343282 0.20356254 0.14220041 0.15202532 0.13106058 0.12599841
 0.13284517 0.12200778 0.12713303 0.15099159 0.17196738 0.14434182
 0.13701331 0.13753509 0.16770799 0.1145881  0.14989172 0.15112598
 0.12701356 0.16519828]
alpha_repeat-2
BERT
[0.7895322  0.7917364  0.79179513 0.79793453 0.78852516 0.79234093
 0.7932662  0.7931603  0.8033547  0.7887633  0.7940342  0.79926884
 0.7917578  0.7890416  0.78567237 0.8039264  0.7929215  0.78944534
 0.79290074 0.79784256 0.7969433  0.7962232  0.79211974 0.7944941
 0.797467   0.7892682  0.7957475  0.79291767 0.7937785  0.79505855
 0.79320574 0.792536   0.7965662  0.78548545 0.79678017 0.79646397
 0.7934963  0.7905859  0.79527974 0.7990451  0.792475   0.79175323
 0.7976953  0.79804665 0.79591006 0.79308516 0.7990321  0.79547304
 0.79452264 0.79302377 0.78880674 0.7992401  0.7930112  0.79362637
 0.7972627  0.7905588  0.7921353  0.79585284 0.7912344  0.78988016
 0.7881745  0.7986546  0.79656935 0.79172915 0.7892401  0.79483247
 0.7958626  0.7971652  0.79578304 0.7912006  0.79422706 0.7886441
 0.7918988  0.7957266  0.7944618  0.800346   0.7904128  0.79618853
 0.7943837  0.79157585 0.79408675 0.79715514 0.7919967  0.7956453
 0.78871834 0.78779835 0.79611206 0.7928293  0.79212224 0.79899114
 0.7957603  0.7932174  0.7939612  0.79614955 0.7944837  0.79354423
 0.79581714 0.7930134  0.7953722  0.7976855  0.791377   0.79061705
 0.7916208  0.79344493 0.786916   0.79925084 0.7941618  0.7935531
 0.7934214  0.7937411  0.7976724  0.7920005  0.796292   0.7955461
 0.7909038  0.7945456  0.79113483 0.7958299  0.792258   0.7945555
 0.7905317  0.7926779  0.7989771  0.79533833 0.7919099  0.7911904
 0.8006036  0.79157424 0.7940914  0.7942115  0.79178035 0.79576427
 0.79260284 0.8009758  0.79126656 0.79361296 0.7936341  0.80224127
 0.7947028  0.7980712  0.7918011  0.79061013 0.8030407  0.7912501
 0.7947071  0.79565495 0.7972148  0.79302436 0.7912458  0.79550624
 0.7950378  0.7982113  0.79618084 0.7967722  0.79718876 0.79097545
 0.7925683  0.7910547  0.79304683 0.79885286 0.7918346  0.7991517
 0.7944176  0.7940004  0.79165846 0.7940387  0.7932879  0.79315025
 0.79367054 0.7933374  0.79706156 0.7932424  0.796258   0.79286265
 0.798794   0.7909556  0.7940969  0.790981   0.7974414  0.7938367
 0.7945967  0.8043963  0.791589   0.7960313  0.79766214 0.79003555
 0.7946702  0.7934603  0.7934429  0.79589343 0.79520637 0.7914548
 0.7931777  0.7924539  0.7924113  0.7938111  0.79569316 0.79774696
 0.7919799  0.79320395]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
bravo_repeat-1
WER
[-0.08881205 -0.09214577 -0.07440569 -0.09148225 -0.06926812 -0.07669193
 -0.10799249 -0.08990561 -0.09547277 -0.0873658  -0.09223173 -0.08484124
 -0.09506497 -0.08808405 -0.09316059 -0.08757749 -0.08722968 -0.07799941
 -0.08934692 -0.06596531 -0.10081893 -0.09325614 -0.08002527 -0.08071416
 -0.07028909 -0.09615183 -0.09578245 -0.07704966 -0.088914   -0.06769669
 -0.08291797 -0.07459822 -0.08614335 -0.05091183 -0.08970825 -0.08745363
 -0.09900969 -0.10547404 -0.08235273 -0.10601306 -0.10097439 -0.08185899
 -0.09369076 -0.09650592 -0.07415245 -0.07907386 -0.10188345 -0.09188407
 -0.09037389 -0.11048663 -0.0802905  -0.07895904 -0.10179288 -0.07836874
 -0.09919816 -0.08339248 -0.05944265 -0.06404477 -0.10125528 -0.08860525
 -0.08211312 -0.1020317  -0.09942859 -0.07335156 -0.0837832  -0.07750042
 -0.09987934 -0.08000942 -0.10572803 -0.07575256 -0.08607276 -0.0912057
 -0.09699189 -0.08909771 -0.1096548  -0.07782889 -0.08445238 -0.09290274
 -0.0791614  -0.08506546 -0.09816938 -0.08699614 -0.09953912 -0.09093844
 -0.07334005 -0.08015321 -0.07539235 -0.07168495 -0.11071996 -0.11278246
 -0.0869226  -0.0568193  -0.08992562 -0.08321807 -0.09859537 -0.08397976
 -0.09466204 -0.08560729 -0.08043006 -0.09705159 -0.08555861 -0.09431479
 -0.09169616 -0.08409763 -0.09903385 -0.09542737 -0.09973737 -0.06872006
 -0.08714816 -0.10687324 -0.09048399 -0.09845976 -0.0757875  -0.08244754
 -0.09205374 -0.08181766 -0.09505571 -0.08942048 -0.09934412 -0.09593586
 -0.07709513 -0.08054291 -0.09265211 -0.09012509 -0.08653688 -0.08079584
 -0.06064558 -0.1021962  -0.09091111 -0.10295121 -0.05906502 -0.09192014
 -0.07707921 -0.06139163 -0.1191544  -0.07386356 -0.06066367 -0.07982689
 -0.09103691 -0.07897415 -0.08317955 -0.08393943 -0.11075465 -0.07930614
 -0.09028507 -0.0797984  -0.1021881  -0.07648856 -0.10533417 -0.08767565
 -0.08416726 -0.09872411 -0.09946686 -0.09899919 -0.09031628 -0.0917165
 -0.07187097 -0.08488347 -0.06830477 -0.09136551 -0.09556441 -0.08032103
 -0.07405109 -0.09426163 -0.07599017 -0.08636652 -0.09815374 -0.08862617
 -0.08299813 -0.09601784 -0.11060193 -0.10758348 -0.09045308 -0.10040661
 -0.07632067 -0.0726398  -0.09092274 -0.09065244 -0.09559752 -0.06622371
 -0.10177742 -0.09037947 -0.06740604 -0.07453104 -0.11183146 -0.08391832
 -0.08754623 -0.08403344 -0.10121383 -0.10543972 -0.10930242 -0.09386941
 -0.06663403 -0.09273905 -0.0863764  -0.1017391  -0.0968642  -0.08281023
 -0.09706278 -0.08923239]
bravo_repeat-1
BLEU
[0.24505548 0.22549692 0.24755218 0.25001646 0.26992528 0.24421092
 0.19783384 0.23400647 0.21722538 0.2104441  0.22962195 0.22623903
 0.18695574 0.21703238 0.19912381 0.22513713 0.2187066  0.23203045
 0.22404804 0.22827572 0.2407176  0.22881295 0.17845075 0.23749658
 0.23979639 0.20452341 0.18907164 0.20557666 0.20553573 0.24404322
 0.21783563 0.23984077 0.16995229 0.22299101 0.21443519 0.24340885
 0.21647844 0.20254318 0.18401102 0.13224902 0.21070196 0.21988155
 0.21620042 0.17745946 0.24524864 0.24376992 0.1711938  0.2026061
 0.17707356 0.21622784 0.18833076 0.21606378 0.21414784 0.22455029
 0.23840763 0.23139484 0.26140543 0.21235233 0.18366861 0.22594745
 0.20742564 0.17993099 0.20964745 0.237858   0.2463406  0.21880916
 0.21941478 0.2206956  0.21751804 0.23416439 0.21530649 0.21932091
 0.21400322 0.24048156 0.13396933 0.22589306 0.20766771 0.21246219
 0.21369759 0.24020735 0.20916539 0.1891296  0.1813819  0.1841419
 0.27220081 0.27115833 0.27189818 0.24471756 0.17965949 0.22267181
 0.22382822 0.22479892 0.20459628 0.23194548 0.2180473  0.22594335
 0.25349583 0.22500964 0.22240884 0.14725414 0.27885407 0.19102357
 0.20910663 0.19995125 0.17444207 0.17358936 0.20906483 0.22630363
 0.23601522 0.21840975 0.21425008 0.14558134 0.23986296 0.24142997
 0.21573838 0.23111371 0.17730776 0.23473365 0.17539838 0.22594153
 0.24733226 0.22710367 0.194966   0.15039227 0.26599372 0.23751105
 0.22565058 0.19788854 0.23954254 0.20745103 0.25104112 0.19796593
 0.23049222 0.23220181 0.1500089  0.28500598 0.23085451 0.23082839
 0.22894607 0.22616365 0.25335107 0.23983136 0.13971989 0.23618385
 0.22849565 0.23001692 0.20790621 0.22841936 0.19827595 0.18999344
 0.1954005  0.17287561 0.18521296 0.25543356 0.20612283 0.17320598
 0.26255709 0.26187259 0.23521886 0.20011649 0.2347651  0.20044275
 0.22316392 0.23167229 0.25832815 0.20089268 0.16144484 0.22244572
 0.23844705 0.21809629 0.17361263 0.14976308 0.18661085 0.17390018
 0.24830404 0.22574944 0.20545725 0.21998045 0.23648248 0.23055148
 0.17578628 0.18838987 0.26941376 0.22656581 0.22446006 0.21600148
 0.20472524 0.22715688 0.20589703 0.16169683 0.1872308  0.21223001
 0.20216175 0.23131525 0.19110443 0.18360588 0.2076195  0.20522397
 0.2020966  0.21559199]
bravo_repeat-1
METEOR
[0.16378235 0.15667608 0.18536427 0.16761875 0.18634231 0.18099372
 0.14359271 0.15798398 0.15154285 0.16820981 0.17523776 0.15330186
 0.13235393 0.14100187 0.14746684 0.16015855 0.15853245 0.15696873
 0.15931824 0.16344617 0.15395359 0.17382964 0.12218106 0.14677965
 0.16683692 0.14850144 0.13222964 0.14860099 0.13647533 0.19547066
 0.14336252 0.1693826  0.11229605 0.17058743 0.13703934 0.16213787
 0.14153332 0.16192312 0.13098281 0.1007567  0.16222163 0.17082152
 0.14608802 0.11764054 0.164068   0.17405813 0.11654032 0.14477073
 0.11939219 0.14388394 0.14252147 0.15337628 0.15751024 0.16160499
 0.17472064 0.16417675 0.17409699 0.1513467  0.1387264  0.14703893
 0.15817524 0.11783437 0.14305447 0.17896066 0.16631539 0.16686528
 0.1629351  0.15450079 0.15469854 0.15342379 0.15896945 0.13214997
 0.15932506 0.16335924 0.11055446 0.15838725 0.16948203 0.14586672
 0.15998516 0.14826048 0.15069914 0.12996468 0.13495069 0.1456938
 0.16711992 0.18880932 0.19233026 0.17823785 0.12946742 0.16108662
 0.16034847 0.15949957 0.14159782 0.17006403 0.15024582 0.15939116
 0.18660304 0.1643206  0.17227509 0.10624486 0.19532673 0.13172146
 0.14989989 0.15112905 0.13793245 0.12787269 0.15368413 0.15334437
 0.1739458  0.14204137 0.1569475  0.12097533 0.17018108 0.17676465
 0.16108799 0.18363031 0.14059472 0.16803009 0.15616663 0.16439543
 0.17243768 0.1577981  0.13762175 0.1223651  0.17942678 0.16737965
 0.14979177 0.13135705 0.17256119 0.14521334 0.17033935 0.13757269
 0.17866957 0.16110533 0.1111518  0.18812157 0.16305866 0.17205041
 0.17012034 0.1761586  0.17740864 0.17239316 0.10036102 0.16519772
 0.15824953 0.16163734 0.14667601 0.16735219 0.13930846 0.13299937
 0.15330577 0.12220408 0.15323865 0.16371113 0.14602501 0.1413321
 0.17300001 0.18896043 0.16540544 0.13451143 0.16542149 0.14634847
 0.14237118 0.1667488  0.16845073 0.14978633 0.12167368 0.17142825
 0.14834825 0.13435446 0.11897608 0.10146698 0.15627051 0.11757784
 0.16853758 0.16804565 0.15422019 0.16945539 0.17351443 0.15835888
 0.12140528 0.13996038 0.18782532 0.15695786 0.16442887 0.15358849
 0.1355222  0.16768381 0.14972386 0.11869808 0.1361554  0.14390508
 0.14067025 0.15764894 0.13845668 0.12712472 0.13976537 0.14662928
 0.14231778 0.15150894]
bravo_repeat-1
BERT
[0.80911887 0.8048923  0.80946535 0.80830246 0.80729955 0.80972403
 0.8048641  0.8043279  0.80211306 0.807717   0.8097247  0.80407715
 0.8034686  0.8044778  0.8086586  0.80994713 0.8020923  0.8078691
 0.80644953 0.8063824  0.80376166 0.8030719  0.7983137  0.8038592
 0.8004716  0.8034638  0.8029279  0.8083921  0.80551314 0.80445874
 0.8066328  0.8081275  0.8016459  0.80543154 0.8016878  0.80378205
 0.80537564 0.8074469  0.8006857  0.80181    0.8075085  0.80847794
 0.8044089  0.80133736 0.810799   0.812999   0.80432034 0.80693966
 0.79836124 0.8030341  0.8041289  0.80700904 0.80742496 0.8061333
 0.8085067  0.80809075 0.80473655 0.8072039  0.7990548  0.8066406
 0.80523235 0.8028141  0.8059314  0.8057634  0.80638057 0.8039825
 0.80303055 0.8042942  0.8047588  0.80331963 0.809293   0.805464
 0.8042546  0.8082493  0.8034036  0.8065005  0.80445665 0.8078199
 0.80722713 0.8026088  0.80901504 0.802029   0.8019048  0.80316573
 0.80302143 0.8110307  0.8021529  0.80965054 0.80376315 0.807157
 0.806057   0.7999364  0.80381477 0.8087817  0.8067382  0.80345225
 0.8077658  0.8056796  0.8058024  0.8030601  0.80751234 0.8032991
 0.8035438  0.79975575 0.80035913 0.8030003  0.8022766  0.8072303
 0.81305945 0.8027747  0.801557   0.7974782  0.8068645  0.8063228
 0.80469    0.80425    0.80041814 0.8078197  0.80476    0.8074219
 0.8056023  0.8087024  0.806709   0.7991988  0.80889845 0.81047904
 0.80256665 0.8040304  0.8078552  0.8036782  0.8064917  0.8045885
 0.8066204  0.80887973 0.80039436 0.80710965 0.8103877  0.8074107
 0.80363774 0.8143116  0.805781   0.80606693 0.8011103  0.80462164
 0.8107563  0.8061654  0.8018585  0.80635995 0.80708796 0.7974875
 0.8059218  0.8052597  0.8027007  0.8019716  0.8049348  0.8009088
 0.8116376  0.80623674 0.80763346 0.80351937 0.80573636 0.80083996
 0.80194956 0.8128118  0.8048394  0.8075583  0.80345803 0.8095035
 0.8089464  0.80207324 0.8039506  0.79925674 0.80493236 0.8039431
 0.8094225  0.8109365  0.8033794  0.8065871  0.8036744  0.8072533
 0.80698216 0.80240196 0.80773616 0.80710685 0.80522466 0.8067842
 0.803559   0.8086828  0.8047293  0.8028407  0.8039507  0.81113106
 0.8023557  0.80520666 0.8109849  0.80074424 0.80314803 0.80173975
 0.80402535 0.80547297]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
bravo_repeat-2
WER
[-0.05243762 -0.04532016 -0.04503277 -0.04791569 -0.03862865 -0.01950089
 -0.02840822 -0.04431069 -0.06061045 -0.0400438  -0.03777025 -0.02189103
 -0.03459496 -0.02484501 -0.04141091 -0.03228801 -0.0473168  -0.01357489
 -0.04727196 -0.03060878 -0.03680864 -0.04068481 -0.05518031 -0.03437047
 -0.05540266 -0.0425039  -0.03251926 -0.02575556 -0.04251915 -0.0216304
 -0.05378385 -0.02033983 -0.03918493 -0.04175345 -0.04037007 -0.03095125
 -0.03257945 -0.02813252 -0.03968512 -0.03709753 -0.02017095 -0.02464067
 -0.02666222 -0.0522583  -0.05955033 -0.03636954 -0.03780906 -0.0113419
 -0.06951981 -0.0104496  -0.04131098 -0.05372323 -0.01270232 -0.0417798
 -0.02874473 -0.03103368 -0.03003084 -0.01255877 -0.04418414 -0.04629482
 -0.05015791 -0.04055881 -0.03702465 -0.04487573 -0.0342196  -0.02686588
 -0.03203215 -0.05391389 -0.03642719 -0.03395139 -0.0294939  -0.03883202
 -0.02364329 -0.04929395 -0.0195627  -0.03520352 -0.03544929 -0.02987328
 -0.04658746 -0.03644598 -0.04762807 -0.04164224 -0.04300772 -0.01219336
 -0.04598898 -0.03242486 -0.04605524 -0.04022843 -0.03937057 -0.03341382
 -0.02987787 -0.04013775 -0.04225596 -0.03584627 -0.0337765  -0.03750144
 -0.04824083 -0.04091483 -0.02956243 -0.01712265 -0.05523129 -0.04908125
 -0.00969002 -0.04103768 -0.04031896 -0.04163727 -0.05279324 -0.04116007
 -0.06647843 -0.02918995 -0.05200344 -0.02840864 -0.04174845 -0.05517327
 -0.04290449 -0.0257204  -0.02368463 -0.03296282 -0.03706501 -0.04761319
 -0.02300285 -0.01131182 -0.03077606 -0.03625753 -0.04811243 -0.03391505
 -0.04498036 -0.03680569 -0.04290661 -0.04205826 -0.04330307 -0.05042286
 -0.03482415 -0.03242015 -0.04274325 -0.04431635 -0.0523562  -0.01153163
 -0.0395599  -0.01671171 -0.04931898 -0.03112131 -0.04538132 -0.03539364
 -0.03255002 -0.03603902 -0.03281228 -0.02662666 -0.04330313 -0.03844887
 -0.0326357  -0.04425218 -0.03818592 -0.03236154 -0.05640997 -0.02998847
 -0.03782433 -0.03632812 -0.03666777 -0.03885975 -0.03895965 -0.01666137
 -0.03969631 -0.03987796 -0.03797099 -0.0352819  -0.04700535 -0.02963027
 -0.00917197 -0.03576226 -0.03893215 -0.0310037  -0.03136507 -0.05073271
 -0.03096996 -0.01097988 -0.04457707 -0.04546462 -0.02923327 -0.00675002
 -0.0438943  -0.03579327 -0.03666807 -0.00342367 -0.04530718 -0.05661929
 -0.04722972 -0.05524951 -0.04077364 -0.02341098 -0.05095141 -0.0399744
 -0.04890369 -0.0498273  -0.0550492  -0.04371125 -0.04912138 -0.0301707
 -0.03344594 -0.03111611]
bravo_repeat-2
BLEU
[0.20249536 0.22813939 0.20473109 0.21195977 0.22055714 0.26617973
 0.24555834 0.19248037 0.16089822 0.17405277 0.22829722 0.25142393
 0.22507906 0.24526197 0.19387948 0.23373234 0.1843081  0.30403011
 0.22542371 0.22049258 0.19552582 0.25244601 0.15600091 0.21336587
 0.1949831  0.18966034 0.24763728 0.25874129 0.22806929 0.26752302
 0.17318649 0.21886166 0.23796113 0.26428505 0.21826649 0.24574585
 0.17900366 0.1908733  0.21949525 0.21896032 0.23560777 0.2399873
 0.24849012 0.23549906 0.23324687 0.20467937 0.23005276 0.26039824
 0.16644106 0.2123667  0.21098688 0.17722047 0.25653082 0.21709534
 0.22968367 0.26358948 0.23985143 0.20979117 0.20598126 0.19119068
 0.22972913 0.21688527 0.21064274 0.17054706 0.19420322 0.19276526
 0.18048184 0.22372317 0.20145148 0.19737699 0.2558543  0.23167357
 0.25421968 0.21617939 0.2381036  0.23035908 0.21719019 0.22293431
 0.23512271 0.27052248 0.20307662 0.20652474 0.2713257  0.23825674
 0.23657178 0.25503909 0.23766349 0.20436668 0.22389249 0.18223779
 0.23172295 0.19963313 0.18791595 0.20956971 0.21630267 0.23636424
 0.18131545 0.2302798  0.25389687 0.21344724 0.23443667 0.1927595
 0.24534453 0.20333749 0.22843109 0.21873891 0.21313889 0.18617009
 0.12951516 0.23243471 0.17735733 0.19327919 0.1720741  0.19238158
 0.24668193 0.2424762  0.20859545 0.23912305 0.22575672 0.17212293
 0.2257175  0.22798497 0.23239111 0.22302042 0.19212879 0.2430471
 0.24676538 0.23786928 0.22046498 0.2385639  0.23143882 0.19408156
 0.23669172 0.23151427 0.15605487 0.19168957 0.17662174 0.27275757
 0.23177633 0.25175458 0.17827765 0.19920814 0.1644489  0.19862642
 0.25462847 0.20973074 0.22454501 0.24384702 0.16923569 0.18663278
 0.22960363 0.20632917 0.21343663 0.21831298 0.15587694 0.23685645
 0.20320795 0.22611727 0.25120732 0.22183253 0.24947371 0.26543295
 0.21202441 0.24712953 0.27011438 0.23339685 0.16233483 0.21095433
 0.2770355  0.20918042 0.21517412 0.2218271  0.22393423 0.20198155
 0.24901834 0.25114372 0.22569182 0.2509669  0.26160256 0.24356212
 0.22743509 0.23237222 0.17523248 0.25529999 0.17218011 0.17802436
 0.2157004  0.16275612 0.21237689 0.24651183 0.18546335 0.20602017
 0.22636114 0.19245129 0.17588074 0.24207972 0.21161406 0.24708981
 0.20384907 0.22610833]
bravo_repeat-2
METEOR
[0.14100825 0.15213988 0.13220915 0.1438257  0.13454377 0.17283932
 0.15731095 0.12589789 0.11389673 0.12129514 0.18034014 0.17619103
 0.13664261 0.1695171  0.11972264 0.1541852  0.11315552 0.19448954
 0.172557   0.15646591 0.12301026 0.16411691 0.11675062 0.14387483
 0.13690178 0.13980766 0.16020708 0.17873949 0.15676688 0.16183611
 0.1240351  0.14735147 0.1733918  0.17184745 0.14830094 0.16299246
 0.11818448 0.13601833 0.13405686 0.14607115 0.15237819 0.16037783
 0.16833731 0.14677523 0.17047061 0.13339496 0.1522076  0.18872399
 0.1063779  0.1362672  0.16375384 0.1291763  0.1712622  0.14523889
 0.15352471 0.17610986 0.1634969  0.14194411 0.1361676  0.1311517
 0.14820606 0.14638718 0.13052994 0.13717824 0.14015643 0.1350524
 0.12947181 0.16611672 0.13645056 0.12094904 0.1733593  0.14547429
 0.1672171  0.13283686 0.16621186 0.17298831 0.13712212 0.15746087
 0.14308832 0.18099656 0.13550168 0.14675972 0.18599173 0.16754255
 0.14671429 0.17449883 0.14633057 0.13496973 0.1504829  0.13904272
 0.16254684 0.13851184 0.13414449 0.14061082 0.14621305 0.17512363
 0.12552741 0.1528839  0.16100529 0.13541486 0.16230432 0.13470073
 0.1554115  0.14843769 0.16717648 0.14561742 0.13681366 0.11984639
 0.10780729 0.15305039 0.12435759 0.11975639 0.11786414 0.14384789
 0.17142676 0.14900133 0.14876636 0.15415776 0.1528723  0.11293137
 0.1496255  0.19218476 0.1677644  0.13595792 0.13579065 0.15364891
 0.16117621 0.13718182 0.14831652 0.1465611  0.15883535 0.13256324
 0.18061738 0.16045092 0.10771686 0.12143801 0.11701068 0.1882468
 0.15534228 0.16152496 0.11939635 0.14580881 0.11494264 0.13407896
 0.18331675 0.15184459 0.17556244 0.15610291 0.12151163 0.13274828
 0.15493287 0.14087683 0.15668609 0.13400781 0.10994441 0.16352372
 0.13981972 0.14601127 0.19202269 0.15440386 0.151835   0.17973799
 0.13542464 0.17483053 0.18458902 0.15564719 0.10770254 0.14393658
 0.19286794 0.15007233 0.1359459  0.13165788 0.17683628 0.13785302
 0.16525041 0.17415499 0.16113143 0.14994422 0.16056109 0.14999578
 0.14097027 0.1571119  0.11415834 0.18111341 0.12082825 0.13112641
 0.13663874 0.10539005 0.14447669 0.1573235  0.13605303 0.12946735
 0.14252408 0.12910717 0.12005148 0.17440783 0.15474182 0.17612824
 0.13696021 0.16879888]
bravo_repeat-2
BERT
[0.80173475 0.8049548  0.81253165 0.80386287 0.8035214  0.80904025
 0.8026873  0.80321074 0.8028951  0.8014556  0.80536175 0.80628896
 0.80278707 0.80168605 0.8032958  0.80947083 0.80166084 0.81077874
 0.80715364 0.8082644  0.8010098  0.80983853 0.8019295  0.80707186
 0.80876    0.80334145 0.8086808  0.80632514 0.8034323  0.8040894
 0.8060366  0.8047891  0.8067798  0.80809534 0.80687237 0.81148046
 0.8061179  0.80130225 0.80236    0.80829334 0.8089052  0.8055887
 0.80769485 0.8074404  0.8082473  0.80331904 0.80498123 0.81167024
 0.79776764 0.8086022  0.80294883 0.79839456 0.8081018  0.8058437
 0.8003963  0.8115495  0.8055576  0.80337596 0.8041101  0.8046774
 0.8018572  0.8088744  0.80643195 0.8043331  0.80415547 0.806507
 0.80474174 0.8098965  0.80284595 0.80175984 0.8111539  0.80620956
 0.80821216 0.8034913  0.80279636 0.806502   0.8055065  0.8063044
 0.8050759  0.8052416  0.8002479  0.8089797  0.80835223 0.80156493
 0.8038801  0.8105387  0.8050926  0.8012112  0.80914867 0.8068219
 0.8121847  0.80857074 0.80218095 0.8024795  0.80705243 0.8040032
 0.80421925 0.803091   0.8057485  0.8021854  0.8057467  0.8014686
 0.80540735 0.8045417  0.8070151  0.80814475 0.80348176 0.7963925
 0.8003251  0.80714583 0.80446553 0.8010766  0.8048205  0.80461
 0.814845   0.80435216 0.8058318  0.8046024  0.8074416  0.80685556
 0.8121937  0.80824465 0.8060606  0.8019718  0.8029053  0.8007832
 0.80100465 0.80286205 0.80555457 0.80620354 0.8067509  0.80396366
 0.80931294 0.8022693  0.80397266 0.8041294  0.8037584  0.80859774
 0.8029158  0.8088138  0.8062349  0.8092242  0.8022443  0.80213964
 0.8065076  0.8111495  0.80604243 0.80613065 0.8016742  0.8033157
 0.8016583  0.80903196 0.8049082  0.80422693 0.8016726  0.8028611
 0.80389607 0.80261517 0.8050979  0.8048771  0.80544776 0.8076636
 0.8067171  0.8063699  0.80845577 0.8068397  0.79940385 0.80638105
 0.8043142  0.80656713 0.802139   0.8106421  0.8057826  0.80798984
 0.8097882  0.81057626 0.8024251  0.8063671  0.8083188  0.80761117
 0.8053372  0.7992417  0.8045608  0.8073547  0.8022498  0.80327094
 0.8023546  0.8003714  0.8030447  0.80554926 0.8056669  0.8029439
 0.8014294  0.8017114  0.8023002  0.8106854  0.80230814 0.8071883
 0.8077792  0.8141542 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
charlie_repeat-1
WER
[-0.08096098 -0.09156296 -0.09461211 -0.09031558 -0.1048293  -0.09363354
 -0.08851941 -0.09075597 -0.08572455 -0.08619491 -0.09539331 -0.09319708
 -0.09057943 -0.0819046  -0.08525371 -0.07634614 -0.10150123 -0.09046637
 -0.10020291 -0.08809175 -0.10826114 -0.09130495 -0.09569413 -0.10047005
 -0.10676877 -0.10272136 -0.0936922  -0.10683153 -0.09009981 -0.09203896
 -0.101301   -0.10015216 -0.09124943 -0.06791794 -0.09410343 -0.08680945
 -0.09257711 -0.0972996  -0.08177653 -0.08449399 -0.07470562 -0.07581964
 -0.09356893 -0.09095887 -0.07959477 -0.09618127 -0.08297222 -0.08479835
 -0.09156975 -0.10122909 -0.08002139 -0.09335531 -0.06742265 -0.07543192
 -0.10750223 -0.08365281 -0.10293718 -0.10136063 -0.09197742 -0.09508949
 -0.08720272 -0.08433291 -0.10478801 -0.09609757 -0.08536838 -0.11115396
 -0.11527234 -0.07897605 -0.1050159  -0.09733111 -0.10293563 -0.09651336
 -0.0899289  -0.0823355  -0.0816328  -0.09494654 -0.07202216 -0.0772392
 -0.08934305 -0.0902952  -0.09613697 -0.09334456 -0.09593056 -0.09811017
 -0.06766216 -0.09356097 -0.10926705 -0.08604109 -0.1168639  -0.0911815
 -0.10160089 -0.08640603 -0.10797402 -0.0806699  -0.08470874 -0.09334242
 -0.07667419 -0.08019556 -0.094077   -0.09378993 -0.09380818 -0.06838471
 -0.09307859 -0.09777303 -0.08845505 -0.09624432 -0.0895931  -0.10245331
 -0.07976624 -0.08756496 -0.09278255 -0.09007236 -0.0793237  -0.0939382
 -0.09156331 -0.0875717  -0.07653634 -0.08565195 -0.09422968 -0.08593717
 -0.1024123  -0.06935527 -0.08946727 -0.09051116 -0.08377159 -0.08309762
 -0.091355   -0.09802738 -0.07805012 -0.09409438 -0.0882099  -0.08385809
 -0.08986747 -0.10269989 -0.09187824 -0.08974269 -0.09911663 -0.08856913
 -0.08836643 -0.09767951 -0.10006241 -0.0928031  -0.08124852 -0.09857225
 -0.10609696 -0.08415939 -0.08835078 -0.09965088 -0.08872857 -0.08933101
 -0.08122167 -0.10359432 -0.10402735 -0.09184371 -0.09527033 -0.09577786
 -0.1057021  -0.09726992 -0.09418252 -0.08284164 -0.07090379 -0.08510304
 -0.08881972 -0.09123955 -0.09562992 -0.10120027 -0.08759156 -0.10180015
 -0.07930769 -0.08190144 -0.08119124 -0.08912662 -0.10025521 -0.08310798
 -0.09042703 -0.08779727 -0.07978385 -0.07928997 -0.09446682 -0.0881777
 -0.0995138  -0.09773279 -0.09019612 -0.08365925 -0.08654108 -0.09622094
 -0.07593347 -0.10307487 -0.1155911  -0.09639336 -0.07651406 -0.09936981
 -0.0881512  -0.0786484  -0.09144919 -0.10351211 -0.07880959 -0.06945399
 -0.08348014 -0.09002819]
charlie_repeat-1
BLEU
[0.20368486 0.19837521 0.18324795 0.16273832 0.17256701 0.22177983
 0.16624515 0.17340999 0.20665382 0.19953387 0.17460469 0.20680696
 0.20924925 0.21176338 0.19280891 0.18431523 0.16454879 0.1807973
 0.18994287 0.17300376 0.21806949 0.2158755  0.17554555 0.18685845
 0.1566399  0.16747504 0.17194506 0.17416068 0.21837564 0.18857525
 0.19289405 0.16881224 0.18566093 0.20944127 0.18865741 0.18731124
 0.18605241 0.21059593 0.20497971 0.19523209 0.19694481 0.17775999
 0.17555568 0.19930983 0.2117565  0.22450235 0.22867629 0.16733711
 0.18038341 0.17327015 0.19161539 0.19643156 0.18452168 0.18636316
 0.17933834 0.20021263 0.15185854 0.17756702 0.17679965 0.19539582
 0.18721054 0.19576021 0.19890786 0.18735097 0.20969887 0.15495484
 0.17226808 0.23324046 0.15965125 0.20182029 0.13727634 0.14784927
 0.17621707 0.18561387 0.21509503 0.18722652 0.18957024 0.21024595
 0.18030496 0.18895593 0.16750132 0.23805225 0.20191463 0.16260198
 0.22232436 0.22326505 0.15543372 0.17729949 0.1360157  0.18823523
 0.17524953 0.20155911 0.18805726 0.18342564 0.18971632 0.15086142
 0.23615488 0.17964403 0.20099559 0.17770292 0.2042455  0.24266552
 0.16136826 0.16239059 0.18076358 0.17537656 0.19041828 0.19212691
 0.22777536 0.14682731 0.13446451 0.20803265 0.16385761 0.19091806
 0.20353583 0.18146923 0.21471226 0.17245114 0.17626411 0.20477355
 0.15868452 0.21965799 0.19929129 0.19532766 0.21396903 0.2047651
 0.18481399 0.16268311 0.20677904 0.16381766 0.17300004 0.19325853
 0.17472647 0.18847618 0.19716093 0.15271499 0.16675446 0.18422599
 0.19281116 0.16575738 0.15211057 0.19755965 0.21909629 0.1803993
 0.16302303 0.21401943 0.2053614  0.178267   0.19817217 0.20436662
 0.18545452 0.14284434 0.15937296 0.18020057 0.18931845 0.19235778
 0.18671999 0.19470605 0.15332061 0.19576177 0.21898082 0.20067345
 0.19432329 0.17951123 0.1952555  0.18722303 0.19011278 0.21631344
 0.18128194 0.21045241 0.21043987 0.18037438 0.19078292 0.19335989
 0.17529211 0.19779909 0.19170742 0.19762535 0.19048965 0.19277159
 0.17758225 0.18013777 0.17506587 0.17147911 0.17724078 0.20658098
 0.17810859 0.14208896 0.18335223 0.15157966 0.18954594 0.17934347
 0.20856347 0.18234723 0.17505611 0.18123336 0.21221292 0.21551009
 0.18428862 0.19226787]
charlie_repeat-1
METEOR
[0.15874765 0.17827153 0.1487074  0.13394602 0.12476051 0.15255213
 0.12367625 0.14467258 0.15109268 0.15812225 0.14857118 0.17116781
 0.14784903 0.15332046 0.13927164 0.15420337 0.14380947 0.14489099
 0.13006401 0.12843426 0.15666359 0.15751349 0.13935318 0.12465373
 0.13119262 0.14454097 0.12798066 0.13242589 0.1590212  0.16303901
 0.14246934 0.14121694 0.17068757 0.14355182 0.17816701 0.1427104
 0.15099279 0.15034478 0.13808859 0.16889581 0.16144542 0.14358718
 0.14679096 0.13501528 0.16522115 0.15384541 0.17102006 0.15714001
 0.14162359 0.14170091 0.14598811 0.16019564 0.14671689 0.15478415
 0.14334123 0.13860106 0.14600153 0.12715401 0.12208708 0.15957314
 0.16479192 0.15933827 0.13603319 0.15209847 0.15083319 0.1192354
 0.13130012 0.18446724 0.13766161 0.17484619 0.11557622 0.10662153
 0.1230535  0.14899109 0.16355451 0.14708173 0.12486841 0.16207414
 0.14186248 0.13965633 0.1246152  0.17785449 0.14582252 0.14535304
 0.18960714 0.15102336 0.13550129 0.14274251 0.1155145  0.15145641
 0.12969457 0.14793795 0.13743406 0.14912248 0.1409513  0.12884446
 0.15751055 0.14091234 0.1646629  0.12602033 0.14762383 0.15965751
 0.12165297 0.1324326  0.14909035 0.13653863 0.15795836 0.13771149
 0.14749992 0.13233156 0.12607826 0.14108971 0.12306892 0.14720484
 0.14385654 0.15697501 0.1739671  0.15104499 0.12461833 0.1497611
 0.12989556 0.18500556 0.16176991 0.15833158 0.16065024 0.19441984
 0.15860706 0.12176601 0.1741635  0.11309995 0.1444675  0.1485671
 0.13339561 0.143738   0.15589605 0.13805839 0.13475836 0.13571032
 0.14958413 0.1408222  0.11602371 0.14891205 0.15202122 0.15058051
 0.12457171 0.15901457 0.13966513 0.15517465 0.14870719 0.16369898
 0.13725463 0.12843786 0.12353718 0.14019266 0.1457565  0.13366187
 0.1284973  0.13235148 0.11248138 0.14834408 0.16145347 0.16477885
 0.13174397 0.15376988 0.13433611 0.13253323 0.15521447 0.17328636
 0.15715685 0.17300911 0.14929761 0.14018672 0.14131339 0.15060503
 0.12771643 0.14655202 0.15134898 0.17612132 0.17731378 0.15528599
 0.12371513 0.1575324  0.14723529 0.14897713 0.1639752  0.16066435
 0.16210849 0.13376908 0.13525173 0.14821198 0.1366872  0.13138365
 0.15828049 0.13494649 0.13193712 0.15526257 0.18672304 0.18683743
 0.15260105 0.15912465]
charlie_repeat-1
BERT
[0.78030944 0.78531146 0.78184974 0.7887439  0.7824861  0.7827228
 0.7824168  0.7802549  0.78323716 0.77986366 0.7843038  0.79179454
 0.7811191  0.7850536  0.7887801  0.7846932  0.7837485  0.7887685
 0.7845819  0.78345704 0.7868596  0.78643984 0.7863529  0.78242314
 0.7869105  0.78355277 0.7807403  0.78134555 0.7866302  0.78550106
 0.78744954 0.7839908  0.79082054 0.7798625  0.7836862  0.78408945
 0.7825188  0.7835753  0.78241146 0.78598684 0.7869035  0.7860459
 0.78757596 0.78258896 0.7835012  0.7865324  0.7878078  0.784496
 0.7764094  0.7833592  0.78489673 0.78908736 0.78072876 0.78047985
 0.7845273  0.7836511  0.77850354 0.78831387 0.7870679  0.7886331
 0.78680706 0.7847003  0.78338426 0.7909397  0.78603035 0.786565
 0.7868593  0.7871374  0.7800534  0.7897175  0.7799754  0.78604686
 0.7794257  0.78330183 0.7842333  0.79062635 0.78635013 0.7902137
 0.782789   0.7826472  0.7884434  0.78711236 0.78104603 0.7815701
 0.78360695 0.7873223  0.78272814 0.78366935 0.7835163  0.78545266
 0.7826729  0.7888615  0.7843326  0.7823398  0.7877139  0.7819484
 0.78479415 0.78338856 0.7845486  0.78268594 0.78448796 0.78761756
 0.7832547  0.7837651  0.7920162  0.7858809  0.787493   0.78455424
 0.7827512  0.7849569  0.7838358  0.7844214  0.78172016 0.78813607
 0.7836878  0.7858765  0.7930941  0.78384084 0.7822035  0.78221923
 0.7881263  0.78702766 0.77941656 0.79014504 0.79442644 0.78898484
 0.7854259  0.7845194  0.7886044  0.78038675 0.78626364 0.7875464
 0.78251487 0.7856334  0.7826676  0.78212637 0.78229254 0.783324
 0.78196263 0.78206325 0.78192705 0.78068274 0.79118776 0.7829676
 0.7887376  0.7839284  0.78344774 0.78286344 0.78178924 0.7908139
 0.7816532  0.7838201  0.79121816 0.78875965 0.7843422  0.78357816
 0.782852   0.78162414 0.7805697  0.7843006  0.7828677  0.7897378
 0.7885008  0.79107106 0.7815074  0.78692514 0.7878191  0.7839938
 0.78260285 0.7859265  0.78804684 0.7890844  0.7872373  0.7905832
 0.78710824 0.7857251  0.7853523  0.78658223 0.78149265 0.7817678
 0.7823598  0.7814515  0.78414553 0.7821695  0.7894392  0.78746575
 0.78944427 0.7820979  0.78428257 0.78027767 0.79155266 0.78518665
 0.79128635 0.78561836 0.7870582  0.78594786 0.7924306  0.79156196
 0.7870616  0.7818074 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
charlie_repeat-2
WER
[-0.12932432 -0.1235599  -0.12082333 -0.12954683 -0.1329769  -0.12863674
 -0.12356462 -0.12822697 -0.13540296 -0.12092744 -0.12436213 -0.11747071
 -0.12282801 -0.12645481 -0.13462389 -0.13206388 -0.13330783 -0.11793618
 -0.12683384 -0.11383555 -0.14012115 -0.13693995 -0.11939064 -0.12457828
 -0.1137867  -0.12713512 -0.13381529 -0.11046777 -0.13342278 -0.11551405
 -0.1367356  -0.10951672 -0.13776696 -0.11564859 -0.14939753 -0.12952863
 -0.128803   -0.12734841 -0.13185173 -0.12619446 -0.14686088 -0.13324223
 -0.12423352 -0.11090617 -0.11467754 -0.14081394 -0.11718327 -0.12224916
 -0.12900587 -0.12499024 -0.12354117 -0.11190057 -0.13939486 -0.11644127
 -0.13942031 -0.14660647 -0.12124714 -0.11883714 -0.11633058 -0.10645893
 -0.12501928 -0.13237117 -0.11944194 -0.1169537  -0.13283719 -0.12553366
 -0.14021357 -0.12014282 -0.13504406 -0.12574746 -0.14264946 -0.1442649
 -0.12848752 -0.11338338 -0.13695675 -0.12628028 -0.13567122 -0.14778022
 -0.12898238 -0.12524012 -0.1345418  -0.13045594 -0.1359782  -0.13571525
 -0.11791319 -0.14942485 -0.11719122 -0.13871168 -0.14509216 -0.12118721
 -0.11216826 -0.11709878 -0.11476576 -0.11635831 -0.12860767 -0.13003945
 -0.12487161 -0.12351123 -0.1215924  -0.11491499 -0.13960366 -0.14005524
 -0.1060047  -0.11916494 -0.13013138 -0.12762411 -0.13626409 -0.14550215
 -0.14689645 -0.12182561 -0.11697856 -0.13552675 -0.12569887 -0.0797503
 -0.13234709 -0.12324613 -0.11068974 -0.12310303 -0.11914595 -0.13462762
 -0.12994908 -0.12889239 -0.11837174 -0.11983372 -0.14132319 -0.10579106
 -0.13683658 -0.13943163 -0.13331318 -0.10946098 -0.11881522 -0.10916844
 -0.12328124 -0.12999975 -0.11534256 -0.12569318 -0.11818775 -0.11854499
 -0.13322744 -0.1345031  -0.1222507  -0.12916179 -0.13339255 -0.13125041
 -0.13282635 -0.13121722 -0.13315677 -0.1348083  -0.14028992 -0.11724586
 -0.13056082 -0.11813169 -0.11862073 -0.13853319 -0.09444329 -0.12054261
 -0.10040132 -0.13297589 -0.11494102 -0.12121296 -0.12549735 -0.11614412
 -0.11486517 -0.12708836 -0.1132032  -0.12717872 -0.13714618 -0.13503538
 -0.09180401 -0.13264267 -0.14041039 -0.1102549  -0.10979543 -0.11931708
 -0.14534544 -0.14219636 -0.12966823 -0.1246272  -0.12696742 -0.10613039
 -0.10527115 -0.12070919 -0.12568852 -0.13150483 -0.12928356 -0.14376674
 -0.13950685 -0.12044985 -0.12295799 -0.12159949 -0.13453701 -0.13124256
 -0.14390846 -0.11370466 -0.12549241 -0.13252215 -0.12987308 -0.11405676
 -0.12844219 -0.12282676]
charlie_repeat-2
BLEU
[0.18382904 0.15406728 0.20020083 0.18203467 0.18630364 0.19073319
 0.22078158 0.15765866 0.14300714 0.20254349 0.21611825 0.18178696
 0.18482916 0.18194441 0.16863295 0.17659349 0.18812179 0.20483632
 0.19770335 0.19953862 0.1814206  0.1888062  0.22381762 0.21349687
 0.17006082 0.19509833 0.19755877 0.19471307 0.19081567 0.2266795
 0.17634234 0.23591547 0.14097782 0.21171782 0.17569934 0.17431608
 0.23721524 0.1933564  0.17942273 0.2057265  0.16753822 0.19922095
 0.18828292 0.2124902  0.16224521 0.15267304 0.20898673 0.18538323
 0.19419962 0.15949425 0.23751639 0.20103792 0.14584629 0.18192885
 0.19321834 0.17169411 0.2142077  0.18022402 0.20269172 0.20141854
 0.17008871 0.18027174 0.19748396 0.21641819 0.17278971 0.20443008
 0.16305322 0.19968641 0.18911981 0.18800752 0.13300194 0.18068668
 0.19061527 0.18919777 0.1722595  0.15605938 0.18259863 0.18501139
 0.15039025 0.18501543 0.21631778 0.16067592 0.16373741 0.20683057
 0.19812302 0.14995721 0.20706332 0.17119663 0.15563705 0.20231114
 0.19806174 0.19379742 0.20602811 0.18003515 0.18107462 0.17682282
 0.18291987 0.23339827 0.18368227 0.21066489 0.19998101 0.15559015
 0.16492134 0.20990326 0.19109637 0.20915139 0.16321796 0.19637404
 0.13513425 0.18690135 0.15255984 0.18141307 0.18578107 0.2539699
 0.19829353 0.16996315 0.23751399 0.19372925 0.21801863 0.18086689
 0.20106738 0.20342745 0.18502386 0.19202562 0.17548791 0.2194609
 0.16070269 0.17928121 0.20051579 0.20757169 0.19038014 0.22380358
 0.18306507 0.18445447 0.1908246  0.19744067 0.18831546 0.21031868
 0.18373538 0.16829036 0.17804445 0.17186836 0.18735785 0.18558971
 0.17476458 0.18113597 0.19689195 0.14817129 0.15119234 0.20004382
 0.18423139 0.16247614 0.20565785 0.16583923 0.19220708 0.20561637
 0.18480444 0.16168063 0.21019692 0.18982941 0.22899108 0.21193331
 0.21057695 0.21142969 0.19241816 0.20197027 0.15634012 0.16531921
 0.22912891 0.19379654 0.18760599 0.19949362 0.20732142 0.2152401
 0.14174224 0.1930438  0.18454875 0.20004681 0.17444925 0.20190006
 0.2307177  0.20892543 0.22676324 0.18929253 0.1730107  0.14081935
 0.21711613 0.22540622 0.18380823 0.18442551 0.20603139 0.16560705
 0.17914473 0.2015495  0.19491923 0.1595197  0.20559279 0.24418569
 0.18711237 0.20980221]
charlie_repeat-2
METEOR
[0.15785334 0.12282601 0.15041385 0.17223673 0.14861814 0.16838267
 0.16239829 0.13171988 0.12450298 0.17640208 0.1669113  0.1474342
 0.13913011 0.15682327 0.15461031 0.16169387 0.14177828 0.14487146
 0.16462066 0.15375245 0.14382076 0.16099963 0.16909569 0.16829356
 0.14999252 0.19050399 0.15585484 0.15725811 0.1425926  0.17536091
 0.16367797 0.18358993 0.11809001 0.18615436 0.13346191 0.12717254
 0.17046054 0.15248107 0.14894627 0.16291617 0.13832533 0.15647158
 0.13092068 0.17308339 0.13227913 0.12717069 0.15919663 0.14332002
 0.13838393 0.12411387 0.16906866 0.16354728 0.1179746  0.14009068
 0.14575197 0.13755928 0.18271615 0.14797877 0.14980033 0.14378956
 0.13215549 0.13854295 0.14145712 0.15853703 0.14587882 0.14060101
 0.12738576 0.15215552 0.13792543 0.14634651 0.10519843 0.14736426
 0.16215319 0.14093886 0.13954246 0.11884536 0.14292332 0.13484626
 0.11981581 0.15570294 0.13962493 0.12783536 0.12771549 0.16919179
 0.16983572 0.12604506 0.16649525 0.14549779 0.13746465 0.17172046
 0.14366033 0.16783081 0.16137232 0.16779617 0.13690822 0.12438654
 0.15816284 0.15483294 0.15295627 0.16683589 0.16267206 0.12160426
 0.16493888 0.17410312 0.14101626 0.16184069 0.1249046  0.14156173
 0.10317946 0.14865645 0.13503302 0.13556647 0.16558516 0.19321507
 0.1348701  0.14129031 0.19536531 0.16839689 0.15576643 0.15486899
 0.15300075 0.15115895 0.15284748 0.15140684 0.15245077 0.16417144
 0.13677282 0.14768323 0.14166757 0.17083536 0.14881332 0.18724356
 0.14615274 0.13247339 0.14912871 0.14467781 0.1575962  0.15342729
 0.16167145 0.13749365 0.15110842 0.13191038 0.16968105 0.14523602
 0.16887129 0.14177195 0.17309115 0.11091183 0.13816183 0.16645812
 0.12917067 0.1451043  0.15438834 0.13445324 0.14134249 0.18777598
 0.16492673 0.15404749 0.16685034 0.14260271 0.18578698 0.15770808
 0.1565506  0.16573635 0.15256429 0.14404571 0.13236502 0.13062695
 0.16123394 0.15736181 0.15305954 0.16129302 0.16225378 0.15882666
 0.11334436 0.13883889 0.15388518 0.17915667 0.16367289 0.16502046
 0.17432246 0.17221709 0.17989706 0.14718942 0.15201831 0.11297384
 0.17277446 0.19355277 0.16331288 0.1550762  0.15295376 0.15359157
 0.14030791 0.15210597 0.14522088 0.14522364 0.18574244 0.19147238
 0.14138511 0.16827063]
charlie_repeat-2
BERT
[0.7819202  0.78161985 0.7843736  0.7838199  0.78575146 0.7827228
 0.7906048  0.7844164  0.78334427 0.7857683  0.7851511  0.7865182
 0.78526306 0.785209   0.78714466 0.7892312  0.7867531  0.78145504
 0.7869969  0.7860754  0.78595036 0.78151983 0.78954184 0.7849474
 0.78035516 0.78451073 0.78562677 0.7857616  0.7823129  0.7915198
 0.79334426 0.7805732  0.7812958  0.78934693 0.7823129  0.7855314
 0.7882611  0.7924199  0.7804593  0.79066396 0.78405976 0.7809036
 0.78812474 0.7860755  0.7822445  0.7836451  0.7831446  0.7901645
 0.78153396 0.7818809  0.7916265  0.7913215  0.7757489  0.77921796
 0.7867014  0.778567   0.78848153 0.7830615  0.78603464 0.7844419
 0.78152835 0.7792243  0.7833532  0.7862824  0.7804308  0.7816992
 0.7831245  0.78531617 0.783331   0.7795327  0.780339   0.7825011
 0.79153246 0.7805995  0.78033435 0.7871949  0.7866776  0.7838833
 0.77871406 0.7843441  0.79140025 0.7798171  0.78238547 0.7846869
 0.7864123  0.780305   0.7895     0.78698087 0.7845056  0.7869337
 0.7866115  0.7912993  0.78705436 0.7832394  0.7870836  0.7848317
 0.78763974 0.78289235 0.7827213  0.78410685 0.78133875 0.78203744
 0.7830117  0.7882981  0.7838609  0.7879027  0.78292996 0.7803343
 0.7869329  0.78290576 0.7815896  0.78984714 0.78868747 0.7938009
 0.78446615 0.78424615 0.7898364  0.7868908  0.78330195 0.78121686
 0.78867304 0.7865886  0.7809065  0.7779983  0.7824601  0.7847046
 0.78123194 0.78228176 0.7907739  0.78479505 0.7838494  0.7907472
 0.78333086 0.78494847 0.7819632  0.78253126 0.79001075 0.7865209
 0.7809788  0.7834257  0.78666294 0.7790741  0.79011035 0.7814152
 0.79461604 0.78526616 0.78791714 0.7862993  0.7862791  0.7837876
 0.7900835  0.782543   0.78035504 0.7815631  0.7813955  0.7876235
 0.7840152  0.7802082  0.78336895 0.78664315 0.78688794 0.78401387
 0.78794163 0.78661615 0.78224045 0.7824647  0.78461593 0.78010464
 0.7855206  0.7871398  0.7845943  0.7823249  0.786438   0.7863595
 0.7831859  0.78303117 0.78178245 0.78439003 0.78955644 0.7845328
 0.7854845  0.7900436  0.7834885  0.7833624  0.7816671  0.7815402
 0.7840283  0.7909775  0.78684235 0.7837057  0.78414696 0.78387034
 0.7824815  0.789068   0.7836274  0.7852662  0.7846918  0.78749835
 0.78536385 0.7923478 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
delta_repeat-1
WER
[-0.1946964  -0.20335824 -0.16694942 -0.17996444 -0.18125107 -0.18281624
 -0.1779509  -0.16307283 -0.18983943 -0.19319717 -0.20038919 -0.18790032
 -0.21950236 -0.21765199 -0.19021415 -0.18263267 -0.20157815 -0.19494364
 -0.19285182 -0.18718436 -0.19544101 -0.18632122 -0.20883427 -0.19364671
 -0.18771924 -0.20510278 -0.18638261 -0.19019773 -0.1918524  -0.1996831
 -0.19965843 -0.18606026 -0.16979344 -0.18534274 -0.22254533 -0.21203714
 -0.19524311 -0.18462176 -0.19973775 -0.19165666 -0.18491283 -0.20087402
 -0.19736064 -0.17727624 -0.19912122 -0.1876003  -0.21784072 -0.20833251
 -0.20722854 -0.18047819 -0.20468136 -0.21154351 -0.19725107 -0.20244535
 -0.1850959  -0.16714748 -0.18244808 -0.19408928 -0.18823268 -0.17700271
 -0.17541442 -0.19370788 -0.20719143 -0.18514699 -0.18322649 -0.19531205
 -0.19684095 -0.17646691 -0.16977418 -0.19778815 -0.20958109 -0.20000515
 -0.19419978 -0.19545499 -0.20743924 -0.19869162 -0.16858329 -0.18708305
 -0.1914435  -0.20445157 -0.19805185 -0.2069413  -0.20095354 -0.20784484
 -0.2012285  -0.20029027 -0.18594305 -0.18417796 -0.19073536 -0.20109711
 -0.18312946 -0.2001817  -0.17813457 -0.17853624 -0.18839644 -0.19235963
 -0.16774701 -0.19835221 -0.20118034 -0.19969904 -0.20820944 -0.1781659
 -0.2079394  -0.19861681 -0.19531914 -0.17138665 -0.18669705 -0.18471671
 -0.21615745 -0.18674949 -0.18043882 -0.17506402 -0.20056552 -0.15760249
 -0.19131975 -0.217061   -0.17979121 -0.19452667 -0.18627284 -0.1866884
 -0.19981352 -0.19931923 -0.17290816 -0.17026114 -0.19453087 -0.18376683
 -0.1777261  -0.18145512 -0.19330853 -0.19518486 -0.17668846 -0.19665168
 -0.18986957 -0.19685502 -0.18065796 -0.19965547 -0.18447939 -0.19877071
 -0.21192796 -0.16858893 -0.18195102 -0.19535096 -0.20122144 -0.18291207
 -0.1898947  -0.19784174 -0.19720976 -0.18378879 -0.18312563 -0.18317654
 -0.15698303 -0.20676945 -0.19071836 -0.20005305 -0.17223713 -0.20657733
 -0.18738203 -0.20208399 -0.21112682 -0.18735744 -0.18370267 -0.20024986
 -0.18949466 -0.17927754 -0.20253417 -0.19678862 -0.20312402 -0.23070157
 -0.19511699 -0.20009881 -0.20667569 -0.19868068 -0.16941969 -0.1856844
 -0.18636827 -0.18563792 -0.19001141 -0.2046039  -0.20838221 -0.17478279
 -0.19196115 -0.16617076 -0.16957233 -0.18680688 -0.19595265 -0.18738336
 -0.17733446 -0.18166345 -0.15996932 -0.17355903 -0.18399998 -0.19102978
 -0.19004997 -0.19451316 -0.18149353 -0.20034682 -0.19973702 -0.20278418
 -0.19914633 -0.1963887 ]
delta_repeat-1
BLEU
[0.20231979 0.16629753 0.23612395 0.20351065 0.19838741 0.19570492
 0.20971805 0.19914847 0.19214436 0.18807247 0.1538429  0.16026786
 0.13838279 0.13989964 0.189337   0.21406997 0.19355358 0.1735707
 0.14779503 0.19640543 0.18362132 0.17128787 0.14835641 0.19265314
 0.18464389 0.20019461 0.18753128 0.14625851 0.17798412 0.20014175
 0.16668647 0.17605268 0.19785399 0.20283723 0.16562382 0.17955055
 0.19507342 0.14711461 0.21537896 0.1746811  0.17311565 0.18828688
 0.18706088 0.19664825 0.17890128 0.20772262 0.16557746 0.1911844
 0.17090658 0.16307266 0.15645278 0.16070728 0.15356594 0.1779662
 0.20667447 0.19451726 0.19793102 0.21661338 0.19044146 0.19902183
 0.17929384 0.15415625 0.1460549  0.19602048 0.18995943 0.20401307
 0.16448996 0.18986509 0.19499969 0.13921595 0.17544818 0.23050812
 0.17475588 0.18226971 0.17407487 0.15466911 0.2329442  0.20967792
 0.14237008 0.18255374 0.16451865 0.16756714 0.15006467 0.19746751
 0.18800324 0.17746515 0.19459132 0.21821004 0.1840297  0.17181322
 0.21935736 0.16641779 0.23954876 0.19683886 0.13004695 0.15592964
 0.17734667 0.19918084 0.16875642 0.16217376 0.19206593 0.24643891
 0.12615943 0.14711263 0.19002947 0.21191216 0.20187274 0.16432747
 0.16260293 0.17137316 0.18897057 0.21169982 0.1736542  0.17425472
 0.18801351 0.15695681 0.21218359 0.17569854 0.20724288 0.20980253
 0.14940012 0.14599098 0.215625   0.21146361 0.20931324 0.23349828
 0.20184489 0.21691769 0.17008804 0.16551076 0.16643739 0.20415768
 0.19206004 0.17396028 0.22858144 0.2102515  0.17004467 0.16964535
 0.17113034 0.25784216 0.21445467 0.15819544 0.1565412  0.22180068
 0.19100831 0.12961602 0.19732328 0.18346415 0.22023108 0.20367056
 0.19097986 0.1530861  0.167228   0.19648355 0.20259124 0.14622472
 0.17278938 0.1946264  0.19231627 0.22019465 0.18813828 0.14441186
 0.16566767 0.16655083 0.15686338 0.14248044 0.17630192 0.15037052
 0.1949734  0.19091353 0.17301393 0.17587755 0.18497461 0.18065307
 0.1810981  0.19825711 0.21794854 0.1711216  0.16588093 0.24014233
 0.17086608 0.23972288 0.18494928 0.21354498 0.18932154 0.17130187
 0.22652401 0.17395904 0.19769204 0.21229851 0.19581364 0.14083871
 0.16487023 0.15196525 0.2273283  0.17526164 0.21147688 0.14952907
 0.24118508 0.18057514]
delta_repeat-1
METEOR
[0.16598312 0.12960969 0.18246845 0.17951236 0.16074888 0.16049293
 0.1837019  0.15475437 0.16603642 0.18315549 0.13250913 0.13220155
 0.1300954  0.11383762 0.17296367 0.17377155 0.14933024 0.15109133
 0.13204383 0.17542925 0.14110757 0.14502108 0.13870708 0.15633587
 0.16051344 0.17504348 0.15257406 0.13486467 0.1499376  0.16380224
 0.1253613  0.13668745 0.16692792 0.15894338 0.12537596 0.16493754
 0.18840883 0.13591135 0.18434776 0.13570172 0.16411848 0.15424498
 0.15002462 0.17539731 0.15534534 0.16880347 0.14626333 0.13649187
 0.15104558 0.13560872 0.12398815 0.13321183 0.12836551 0.1350777
 0.17155322 0.15643807 0.17370355 0.18721803 0.14642082 0.16173007
 0.17489783 0.12929038 0.12334534 0.14536982 0.17866955 0.13949215
 0.1353095  0.16367664 0.15288207 0.13405362 0.14533413 0.1900545
 0.15919441 0.17071156 0.14245824 0.12756017 0.21886076 0.14363509
 0.13429585 0.15647719 0.14704148 0.14878593 0.12631678 0.15104064
 0.15308508 0.15388599 0.16797676 0.17236593 0.17198933 0.15400944
 0.20100882 0.14649762 0.20398551 0.16560724 0.11833388 0.13626357
 0.1596859  0.15889896 0.13968869 0.13044859 0.16444018 0.17324714
 0.11323632 0.12470623 0.1579676  0.16172418 0.16617272 0.1532775
 0.14153588 0.13896    0.14511596 0.17695574 0.1393379  0.14869188
 0.16854208 0.15049555 0.16174744 0.15362349 0.15804129 0.17609948
 0.12037164 0.14760399 0.18727848 0.16803015 0.16554208 0.16478493
 0.18245287 0.16338758 0.13964727 0.12224062 0.14018506 0.15873872
 0.16516206 0.15673085 0.16546027 0.1620094  0.14876693 0.13388748
 0.16952737 0.21539537 0.17876104 0.14079705 0.14942404 0.18061622
 0.16045063 0.11341845 0.16675115 0.16876278 0.18272822 0.17338783
 0.15222264 0.14207541 0.14521663 0.15274772 0.16493504 0.13560379
 0.14145629 0.1513236  0.15396603 0.16669199 0.14661492 0.1385045
 0.13473834 0.17044302 0.1454751  0.12244005 0.13387645 0.12941953
 0.1778218  0.15108139 0.14587285 0.16262879 0.15804263 0.14253039
 0.15295542 0.15948414 0.17505505 0.14983893 0.15274114 0.16649551
 0.15433589 0.20456621 0.14746447 0.16290813 0.15979292 0.13859907
 0.1935195  0.15141237 0.16074857 0.18504836 0.17126052 0.13504051
 0.14866964 0.14122213 0.18716591 0.14937222 0.16728075 0.12852644
 0.19431025 0.1685478 ]
delta_repeat-1
BERT
[0.80720854 0.79912835 0.80350894 0.8015837  0.8093648  0.804764
 0.80561185 0.80462    0.7980783  0.7958989  0.7956674  0.7977874
 0.79299116 0.7921534  0.8061734  0.8019117  0.8022084  0.79551584
 0.7968989  0.7947331  0.80353177 0.8032422  0.7954747  0.8074498
 0.80308586 0.801307   0.811183   0.7999404  0.80051386 0.80937016
 0.7985614  0.8063152  0.80616397 0.8019144  0.795304   0.800036
 0.7976698  0.8022659  0.8109773  0.79606664 0.80386305 0.8052922
 0.8059253  0.8050683  0.80177075 0.8055635  0.7987517  0.7945353
 0.8057689  0.79163826 0.7924443  0.79342705 0.7936576  0.7985476
 0.8086387  0.80182666 0.80303437 0.8012375  0.80212986 0.80554485
 0.79866046 0.79637474 0.79491574 0.8021177  0.79911476 0.79769504
 0.8009878  0.8009353  0.8015566  0.803908   0.79418266 0.8126565
 0.79756474 0.80119103 0.7953702  0.79527146 0.8144632  0.80554664
 0.7984478  0.79600656 0.80088294 0.7985518  0.7972711  0.8037602
 0.79847187 0.79865956 0.8020067  0.81248474 0.80757093 0.7948567
 0.8094     0.7910855  0.81227773 0.7993882  0.79432666 0.7977786
 0.7971504  0.8029074  0.8018689  0.7947516  0.80074066 0.812765
 0.7991336  0.7933908  0.8095869  0.80322105 0.8010975  0.8074324
 0.795273   0.7947285  0.8074382  0.805407   0.8032006  0.8058016
 0.80343044 0.7985206  0.80289143 0.80348486 0.8038884  0.8092202
 0.7951662  0.7963253  0.8009667  0.79599583 0.8029357  0.8018185
 0.8050466  0.80549103 0.7971826  0.7968637  0.7951954  0.80238044
 0.80404913 0.79744256 0.79934514 0.8031152  0.80272835 0.79368824
 0.7984999  0.8108516  0.8067554  0.80077857 0.79691744 0.8050026
 0.8086559  0.79486716 0.80921453 0.8054397  0.8047301  0.80101365
 0.8033535  0.8021749  0.80292326 0.8040629  0.8078803  0.799439
 0.80613875 0.79397655 0.80678207 0.8046635  0.8011886  0.7976806
 0.79671526 0.8022713  0.79920864 0.79587734 0.80006903 0.79038423
 0.800675   0.80173546 0.7939576  0.8059057  0.8047489  0.7995425
 0.79851216 0.79815674 0.8122837  0.79850143 0.80259556 0.8052891
 0.79199106 0.8111056  0.79257387 0.8046258  0.7965252  0.7907675
 0.8073472  0.79807085 0.8049307  0.81364393 0.7979917  0.8003049
 0.80632794 0.7952488  0.81135094 0.80200547 0.81021315 0.79406756
 0.80777323 0.8055758 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
delta_repeat-2
WER
[-0.23161958 -0.2310439  -0.2440769  -0.20234536 -0.23379164 -0.26509439
 -0.24416465 -0.26131697 -0.25124632 -0.23827061 -0.2516167  -0.24303812
 -0.23018868 -0.25613589 -0.25433264 -0.23678806 -0.24790594 -0.19332519
 -0.23486731 -0.23126956 -0.23010455 -0.23390847 -0.23485093 -0.22714038
 -0.2388146  -0.21584619 -0.21455863 -0.22133954 -0.2519962  -0.24293376
 -0.2359146  -0.24420448 -0.25126822 -0.2404167  -0.25382712 -0.24135145
 -0.25675891 -0.23938057 -0.24716442 -0.22840226 -0.24327814 -0.24866644
 -0.24188653 -0.21877359 -0.25654948 -0.24505137 -0.22955675 -0.23840813
 -0.2322078  -0.2551177  -0.26095585 -0.24551662 -0.2426584  -0.21970837
 -0.24270096 -0.22567777 -0.24636665 -0.2447534  -0.23063955 -0.23244719
 -0.23540943 -0.25575754 -0.23697691 -0.22405878 -0.22855816 -0.25675489
 -0.19842212 -0.23717683 -0.23554026 -0.24455483 -0.20555684 -0.24771552
 -0.22625866 -0.26463775 -0.22949579 -0.24360636 -0.22256288 -0.24452868
 -0.23699178 -0.22475741 -0.24379696 -0.25117703 -0.24124513 -0.24670078
 -0.23835947 -0.23841036 -0.22113003 -0.24559417 -0.24421841 -0.21086338
 -0.20529882 -0.24735969 -0.2435972  -0.23193091 -0.23618873 -0.24130169
 -0.24006315 -0.22546181 -0.26148107 -0.23168005 -0.21741282 -0.23317168
 -0.2263983  -0.24849502 -0.23678692 -0.24942081 -0.21727278 -0.23900937
 -0.25155499 -0.23443174 -0.24000729 -0.25052685 -0.23775298 -0.2465539
 -0.24840396 -0.23122071 -0.25641274 -0.23738058 -0.25388467 -0.24274424
 -0.22847889 -0.24943912 -0.23112744 -0.25601115 -0.25066577 -0.25986248
 -0.23697407 -0.24584675 -0.24615487 -0.22976049 -0.19050533 -0.25958773
 -0.21739606 -0.24116964 -0.2440449  -0.23998168 -0.26065164 -0.23917521
 -0.2460867  -0.26139859 -0.27579603 -0.25285627 -0.24817003 -0.24610164
 -0.23030591 -0.2240661  -0.2159208  -0.2345777  -0.25255635 -0.26298957
 -0.21890637 -0.24454176 -0.2341961  -0.23813837 -0.22834018 -0.24404083
 -0.22468919 -0.25636385 -0.23769923 -0.25198854 -0.25713765 -0.23860593
 -0.24352026 -0.23868264 -0.23491253 -0.24170232 -0.24691823 -0.23275897
 -0.22399347 -0.25882584 -0.23438282 -0.24559791 -0.22137247 -0.24908522
 -0.22861004 -0.25301125 -0.25663591 -0.21896018 -0.24971449 -0.24215542
 -0.25033715 -0.24549634 -0.22748903 -0.22101265 -0.23851622 -0.2282479
 -0.24742868 -0.22935858 -0.2278677  -0.23736644 -0.24347004 -0.25182617
 -0.25665312 -0.25161807 -0.24618545 -0.21927059 -0.26177388 -0.24651034
 -0.23894791 -0.22754082]
delta_repeat-2
BLEU
[0.20564854 0.19413584 0.1791276  0.20470899 0.21547006 0.13467062
 0.21402551 0.16804731 0.1337709  0.17735448 0.17964031 0.18575969
 0.17051897 0.15835194 0.13462942 0.21457335 0.15421187 0.18827972
 0.17693553 0.19044631 0.21017512 0.20512442 0.19334137 0.18805115
 0.16075257 0.19233968 0.20080042 0.19794471 0.14837937 0.17092278
 0.18010796 0.21886348 0.17888468 0.17077024 0.12011012 0.18249066
 0.16066183 0.19396396 0.15987844 0.15934503 0.16291661 0.12582669
 0.15897804 0.1654035  0.15192424 0.16595921 0.17005974 0.1213533
 0.19736017 0.16779032 0.16788815 0.17182099 0.19828477 0.17788379
 0.15535976 0.17772653 0.18182566 0.14163737 0.20119243 0.17185169
 0.19454136 0.15488805 0.19554362 0.21663384 0.17870357 0.15675566
 0.18763158 0.17065599 0.18040861 0.20105696 0.17344421 0.17594285
 0.14073559 0.18464522 0.17775131 0.17929193 0.21889729 0.17107055
 0.17307665 0.17875854 0.19164898 0.15098143 0.17388138 0.13230018
 0.1777742  0.17562474 0.22293576 0.21292321 0.16968922 0.22719333
 0.19056738 0.17965389 0.13085723 0.18450804 0.1799384  0.17257542
 0.19003056 0.19638287 0.18883931 0.18171911 0.16054339 0.17653017
 0.17287928 0.15510522 0.14826036 0.16191385 0.18437244 0.19092655
 0.18051466 0.1947871  0.21340183 0.14925437 0.16920952 0.17739445
 0.17829798 0.22231128 0.15175938 0.21972722 0.23552354 0.1889158
 0.1812606  0.16714653 0.18710912 0.15387219 0.1386909  0.18267171
 0.16665825 0.18310579 0.17923463 0.18286296 0.23550462 0.15964862
 0.1896935  0.17284574 0.18206076 0.2177347  0.15808875 0.19743981
 0.19666389 0.1672328  0.14400257 0.16105206 0.164015   0.18762926
 0.20101079 0.19371361 0.18100465 0.19290278 0.18278464 0.14975459
 0.18291483 0.20043667 0.18913718 0.15390444 0.20311579 0.17126462
 0.21191668 0.15231865 0.19151946 0.17843848 0.20539839 0.18373381
 0.14177902 0.17129621 0.20453793 0.18949994 0.15863493 0.20062469
 0.18626873 0.15491923 0.19586641 0.16658932 0.21434736 0.16980369
 0.20689114 0.14628208 0.13684642 0.19586904 0.18745085 0.19095341
 0.19158291 0.18359105 0.18049398 0.19155828 0.18119289 0.19596816
 0.1842708  0.22904701 0.17988104 0.19329361 0.1638603  0.18409856
 0.15517523 0.17697168 0.13892316 0.1995897  0.17496474 0.1615525
 0.18220427 0.20466113]
delta_repeat-2
METEOR
[0.17826477 0.15690533 0.14281359 0.17570169 0.18570082 0.13275996
 0.17224968 0.13840459 0.13553846 0.16129219 0.13638721 0.17475795
 0.14813547 0.1278348  0.11939931 0.19036944 0.14055689 0.17842509
 0.16209817 0.1659863  0.16140298 0.16288619 0.17296288 0.15696285
 0.14461453 0.17532943 0.18196974 0.16754473 0.12865411 0.14111581
 0.17515748 0.17687913 0.14505226 0.14558748 0.13242292 0.15517073
 0.14745239 0.1597541  0.14922677 0.13547846 0.13850901 0.12193616
 0.14417617 0.14961418 0.1563403  0.15061788 0.1475263  0.14130246
 0.18159172 0.14729248 0.14319969 0.15649796 0.15887379 0.17337893
 0.14023921 0.16701161 0.16235416 0.13852784 0.17109983 0.1385728
 0.16502154 0.13774099 0.17864876 0.20230905 0.16256761 0.12011225
 0.18064296 0.16881661 0.15209095 0.14976679 0.14868065 0.16535995
 0.1360882  0.16248196 0.15158543 0.15169125 0.16955133 0.13294449
 0.15381567 0.15253161 0.1664525  0.13412971 0.15618901 0.12730844
 0.15597693 0.14967669 0.17250312 0.19946233 0.12909187 0.21505992
 0.17055897 0.15872818 0.1240577  0.16016608 0.17452804 0.15635928
 0.15094076 0.19924343 0.1465058  0.16066804 0.15713257 0.15861622
 0.16040648 0.13664465 0.11562702 0.14442418 0.15748689 0.17393275
 0.14568259 0.16401351 0.16773154 0.15624856 0.15822951 0.14958397
 0.14859438 0.20760853 0.14712412 0.17561351 0.21670812 0.14851128
 0.18395444 0.139308   0.15574089 0.14858662 0.13542373 0.15702439
 0.15977216 0.16182583 0.16199897 0.16709681 0.17950798 0.14898379
 0.1738581  0.15308177 0.15729299 0.18184548 0.1447444  0.15991933
 0.1454226  0.14428072 0.12899032 0.14864891 0.1526295  0.15882556
 0.16575398 0.16575704 0.18403465 0.16650687 0.15766607 0.13674606
 0.14200651 0.18167968 0.18854189 0.14431538 0.17580582 0.1389022
 0.17189875 0.14495614 0.18323337 0.16081631 0.14963649 0.16737961
 0.12826289 0.1573718  0.18670586 0.16770271 0.14245392 0.17665684
 0.17753918 0.1237578  0.15644229 0.13314001 0.18453389 0.13824127
 0.18316449 0.14043338 0.11845033 0.18841156 0.17042714 0.16560071
 0.16831331 0.16900472 0.16212325 0.1556575  0.15700767 0.18666656
 0.14215445 0.21293927 0.14683984 0.15565672 0.12912277 0.14531489
 0.1360963  0.1600354  0.11787042 0.1699708  0.15016041 0.16028429
 0.1764117  0.18444734]
delta_repeat-2
BERT
[0.8047164  0.80531406 0.8042805  0.8088135  0.8078477  0.79063934
 0.80443925 0.79539055 0.79275966 0.80140233 0.79620713 0.80216897
 0.7942006  0.79740953 0.7974636  0.80651426 0.79805386 0.8022602
 0.8028981  0.8100127  0.80714303 0.8095509  0.79865545 0.80524874
 0.80434513 0.8059377  0.81265527 0.8041518  0.79405904 0.7938411
 0.80303377 0.80566293 0.79756683 0.7972716  0.79392344 0.8052308
 0.80069214 0.8034363  0.7956097  0.7975539  0.8047039  0.79564446
 0.7987344  0.7934338  0.79699606 0.80069745 0.8017005  0.80182546
 0.8119037  0.80088145 0.7936567  0.7974488  0.80875957 0.7992963
 0.798074   0.80254906 0.80194354 0.79511577 0.8080328  0.7977351
 0.8016633  0.8012171  0.79963845 0.80595976 0.800563   0.79890645
 0.8056183  0.8029517  0.8043691  0.7983901  0.8008383  0.7948404
 0.79374945 0.7996868  0.79947495 0.79730934 0.80197453 0.7973794
 0.800865   0.7991041  0.8089936  0.79723024 0.80517364 0.7938395
 0.795001   0.80644226 0.80265677 0.81043476 0.79321474 0.8054763
 0.8006434  0.8031633  0.79643345 0.80692047 0.80370003 0.7929449
 0.8017126  0.8037983  0.80843335 0.80327827 0.80301064 0.79631114
 0.7987668  0.7935788  0.7911682  0.7930998  0.79830414 0.80140615
 0.7991361  0.80845165 0.80942035 0.7948014  0.7992303  0.79913765
 0.80178654 0.81429195 0.79289955 0.8093937  0.8097788  0.801678
 0.8020562  0.7970687  0.8055686  0.79322463 0.7949375  0.79485023
 0.79329187 0.8059309  0.79639715 0.7996868  0.8077001  0.7929729
 0.796331   0.79487956 0.8008839  0.7995886  0.7973738  0.8067861
 0.80285877 0.7969306  0.798378   0.7950862  0.79846096 0.8082209
 0.8049898  0.8051157  0.80558425 0.7961732  0.80462    0.79558086
 0.80308783 0.8070781  0.8032918  0.7956039  0.8052332  0.7958253
 0.8001881  0.79452217 0.80786866 0.8006239  0.8051008  0.8018611
 0.7946596  0.7991292  0.80325615 0.80625397 0.7953677  0.81053793
 0.8026869  0.7973374  0.8033777  0.79786104 0.8017484  0.8000611
 0.80667484 0.7922574  0.79265857 0.80444354 0.8027181  0.8045794
 0.7998981  0.79668117 0.8067838  0.80540013 0.7985589  0.80605984
 0.80247116 0.80629694 0.7969083  0.8010859  0.8070788  0.7988523
 0.80254316 0.79805887 0.79474187 0.7948192  0.7948545  0.7932806
 0.80096155 0.81228954]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
echo_repeat-1
WER
[-0.042185   -0.04586483 -0.04544578 -0.03565104 -0.03204295 -0.03738883
 -0.03768422 -0.03873345 -0.02162509 -0.02865427 -0.02171064 -0.03721604
 -0.01457596 -0.02601088 -0.03282159 -0.04063103 -0.02728945 -0.04648295
 -0.03771237 -0.01819239 -0.03503505 -0.0241345  -0.03167679 -0.03286794
 -0.01445099 -0.03894447 -0.0368445  -0.04011849 -0.02710392 -0.01055641
 -0.03143356 -0.03139737 -0.02548015 -0.02796705 -0.03613563 -0.04148401
 -0.03142976 -0.02819339 -0.03061642 -0.02903757 -0.03290536 -0.0234098
 -0.04110848 -0.03025797 -0.03178931 -0.03962984 -0.03469367 -0.02029885
 -0.02196034 -0.03676883 -0.04739942 -0.02651572 -0.02759388 -0.02578828
 -0.03951594 -0.04667275 -0.02245521 -0.03486063 -0.02142945 -0.03799526
 -0.03933467 -0.03219984 -0.03630259 -0.02024318 -0.0239288  -0.02128965
 -0.02661932 -0.04644145 -0.03276309 -0.01936898 -0.01916764 -0.03337063
 -0.03977128 -0.03241736 -0.01145742 -0.02720182 -0.03302963 -0.04536964
 -0.02971288 -0.03453624 -0.04638917 -0.03233382 -0.03055537 -0.04778385
 -0.02145935 -0.01389295 -0.03109769 -0.0425582  -0.02668256 -0.02171922
 -0.02217837 -0.02077577 -0.02527166 -0.04012168 -0.04566529 -0.03305746
 -0.0520156  -0.03851168 -0.04795702 -0.03034046 -0.04154016 -0.0372811
 -0.04365455 -0.03000831 -0.03515931 -0.02347401 -0.033106   -0.01852553
 -0.02353522 -0.01026534 -0.04161107 -0.01635729 -0.02641926 -0.04787203
 -0.04477509 -0.02678529 -0.0356527  -0.02360047 -0.035649   -0.04750626
 -0.03978626 -0.03051389 -0.05065562 -0.04485975 -0.0249762  -0.03692083
 -0.04142035 -0.04101053 -0.03383594 -0.03973152 -0.02747137 -0.04240977
 -0.03283252 -0.03958503 -0.03439394 -0.04362915 -0.0354234  -0.0198507
 -0.0395377  -0.03733758 -0.03050579 -0.03491189 -0.02961338 -0.03676412
 -0.01877773 -0.02892545 -0.02222668 -0.02991768 -0.03275072 -0.04130149
 -0.04580961 -0.032736   -0.04175765 -0.03522807 -0.04580228 -0.02704285
 -0.04126855 -0.03220257 -0.03082241 -0.04715564 -0.03800033  0.00069421
 -0.0440751  -0.03236341 -0.01992267 -0.04291202 -0.03494327 -0.02290296
 -0.03128424 -0.05000719 -0.03300303 -0.05130008 -0.04535635 -0.02933768
 -0.03071357 -0.03796834 -0.02570764 -0.04040748 -0.02034056 -0.03989879
 -0.04176216 -0.01605091 -0.04870366 -0.0264752  -0.04195907 -0.02929631
 -0.0438234  -0.01391067 -0.03075692 -0.03930577 -0.04054368 -0.03397994
 -0.04401326 -0.04754407  0.00243221 -0.01881429 -0.04414658 -0.04601371
 -0.03373702 -0.02800126]
echo_repeat-1
BLEU
[0.13737074 0.14593378 0.1518035  0.16828723 0.1770621  0.16207554
 0.14899766 0.17632003 0.18168958 0.1794221  0.18816305 0.15374921
 0.16773779 0.15316791 0.20224219 0.12792797 0.18482985 0.1229853
 0.1418125  0.19717658 0.13783023 0.13674626 0.16043627 0.16749137
 0.15897451 0.13711226 0.14601335 0.16440809 0.16449091 0.18540909
 0.17625061 0.12228221 0.14286381 0.13689264 0.1782221  0.14911417
 0.18187437 0.18205508 0.13285158 0.16293121 0.17409775 0.16279005
 0.17572592 0.18180709 0.14018505 0.14403274 0.16745334 0.16661775
 0.17772506 0.11287017 0.12962875 0.1715206  0.17361022 0.18602291
 0.15576408 0.13178117 0.16359947 0.11036837 0.18467173 0.18989801
 0.12398192 0.14011547 0.12596879 0.13995261 0.15859628 0.15482076
 0.16130215 0.14936074 0.17099792 0.17640013 0.16685535 0.12612958
 0.17078061 0.15561674 0.17570596 0.18820068 0.13876144 0.12097741
 0.15624313 0.16746827 0.1608613  0.16446075 0.17294171 0.12638103
 0.20647075 0.16941056 0.16184718 0.15099874 0.14792115 0.15398215
 0.14647279 0.1870074  0.20166833 0.13790675 0.11626844 0.17423535
 0.15441718 0.13155977 0.17860987 0.15247035 0.18997876 0.18489731
 0.14401769 0.15751999 0.13772909 0.13364387 0.19311754 0.16802127
 0.15781535 0.16094843 0.17249253 0.17546262 0.15863972 0.13760893
 0.13621391 0.15560925 0.1560631  0.16504992 0.14282808 0.13331048
 0.14211047 0.15067205 0.1544133  0.17085007 0.13530749 0.14838146
 0.15191191 0.17609736 0.17830415 0.14123658 0.18283155 0.13050044
 0.1477181  0.15796277 0.16539091 0.14416152 0.15277432 0.19478769
 0.15401855 0.14677405 0.1798383  0.19201269 0.14080698 0.15376184
 0.18189578 0.15999756 0.179478   0.16937292 0.16947645 0.1544408
 0.1462495  0.15340872 0.16581067 0.14577247 0.15107704 0.18255921
 0.15648161 0.1706922  0.17367826 0.14308038 0.13061655 0.19032721
 0.10706042 0.16498283 0.17183094 0.16574385 0.14479126 0.20339996
 0.15201841 0.14202573 0.1628898  0.17277742 0.15592543 0.17868037
 0.13155627 0.12404562 0.15382361 0.16796557 0.15547578 0.16413656
 0.13013126 0.17821152 0.15182565 0.1394335  0.13990319 0.1655769
 0.15174872 0.1728671  0.14074655 0.19722193 0.17146029 0.17530126
 0.12472145 0.19369726 0.18169752 0.15877061 0.13342376 0.13914805
 0.19928576 0.1580758 ]
echo_repeat-1
METEOR
[0.10890842 0.09055186 0.10948518 0.10919211 0.10702096 0.11436312
 0.10166718 0.1111266  0.11380187 0.11583264 0.11934455 0.10794184
 0.10352364 0.10775421 0.1154239  0.0908816  0.12883852 0.08230812
 0.09636348 0.12464588 0.08869095 0.08318291 0.09879641 0.09969608
 0.10591083 0.10073911 0.09901145 0.11378211 0.10048744 0.12094764
 0.13536566 0.08849232 0.09426138 0.10411143 0.11537436 0.08807057
 0.10988524 0.1293024  0.07816159 0.11788573 0.11153683 0.1136731
 0.12566914 0.10774184 0.09247183 0.0938597  0.09970751 0.11178126
 0.12163821 0.0874721  0.07895842 0.11779106 0.12233618 0.11952117
 0.0916932  0.08429637 0.11195091 0.06696084 0.11093458 0.1398412
 0.08318257 0.08282715 0.07630092 0.10438137 0.10417645 0.11514773
 0.10274346 0.09766875 0.10843605 0.11318887 0.1067736  0.08662378
 0.11075265 0.11927536 0.10901086 0.10438445 0.10048505 0.08362346
 0.10970103 0.1139534  0.09874964 0.09974618 0.10119504 0.09590311
 0.13461717 0.11387128 0.10122318 0.10576454 0.10746834 0.08794177
 0.10186242 0.10981482 0.1130175  0.09164025 0.07586056 0.11680183
 0.09755219 0.10108463 0.11618437 0.10659181 0.127153   0.12345328
 0.09258662 0.09583366 0.08890608 0.09030261 0.11881549 0.12479566
 0.08687943 0.10617668 0.11926671 0.10766937 0.10850783 0.11271985
 0.09117329 0.10432468 0.10422727 0.11079493 0.09594025 0.08562511
 0.10571196 0.09183336 0.09890385 0.12483586 0.08276373 0.11505588
 0.10535971 0.11668753 0.11165233 0.09084338 0.11912138 0.10126273
 0.09526221 0.10497844 0.11811944 0.10766294 0.09687977 0.12763144
 0.09405597 0.09350941 0.11759137 0.12519861 0.0984731  0.11017764
 0.12137896 0.10339063 0.12018404 0.11478389 0.11847914 0.10669933
 0.11743151 0.09311445 0.1142942  0.10737176 0.10076897 0.12278036
 0.1184304  0.11346955 0.11910782 0.09732426 0.08548033 0.13347261
 0.07241147 0.11238433 0.13166373 0.09695693 0.10648739 0.12100729
 0.11452465 0.10095294 0.10741984 0.11517953 0.11906746 0.12945252
 0.08657838 0.08110294 0.10449802 0.12517831 0.09737034 0.10511664
 0.08789815 0.11157803 0.09328943 0.09707548 0.09443113 0.10622387
 0.1139831  0.11395007 0.10894073 0.11340324 0.12167317 0.10907341
 0.09115767 0.11536348 0.12178396 0.11917446 0.10356025 0.10104815
 0.12509879 0.1014178 ]
echo_repeat-1
BERT
[0.7818022  0.78004813 0.7771811  0.7783132  0.7800881  0.77890176
 0.78154427 0.7782098  0.77963084 0.77957237 0.78125626 0.7769434
 0.78446543 0.7800777  0.7891678  0.77952087 0.7851746  0.7821415
 0.77808887 0.7864419  0.7820173  0.77946436 0.78300625 0.7804266
 0.78023154 0.77983695 0.78125054 0.78190744 0.7820815  0.78124565
 0.78229177 0.77989715 0.777796   0.7795975  0.7814934  0.78051746
 0.78235364 0.7808155  0.7808586  0.78459054 0.78680027 0.78331304
 0.7836114  0.78311723 0.7819544  0.78248316 0.7792709  0.78118235
 0.77903324 0.7809375  0.77919674 0.78144485 0.7849239  0.78245014
 0.7851148  0.7734494  0.7839473  0.7764902  0.78065395 0.78169835
 0.77762055 0.77866846 0.7792828  0.78038883 0.7797353  0.7792718
 0.78422064 0.781519   0.7871667  0.782182   0.7786196  0.78256726
 0.78209555 0.78044    0.7854196  0.78262115 0.78023666 0.7786197
 0.7836878  0.78341854 0.7826591  0.78089046 0.77764416 0.77850205
 0.7827343  0.78733426 0.78521246 0.7820647  0.7789537  0.7842576
 0.77975744 0.7800504  0.78224814 0.77921146 0.7799777  0.7807473
 0.77847093 0.7819573  0.7847894  0.78001904 0.7815134  0.7845263
 0.78006345 0.7828493  0.7797994  0.7796669  0.7861797  0.7801657
 0.7808755  0.78125185 0.783122   0.7811964  0.78051955 0.7819051
 0.7810591  0.783809   0.7806718  0.78175884 0.78014165 0.78587043
 0.7793082  0.781893   0.78148323 0.780192   0.78529245 0.7780616
 0.78262806 0.7800256  0.78426546 0.78242195 0.7820234  0.7780094
 0.77981156 0.7792448  0.78253585 0.77748066 0.78106624 0.7782823
 0.7818356  0.7854284  0.78192043 0.78259677 0.785007   0.7827666
 0.7843558  0.7829968  0.7834242  0.7806421  0.7847054  0.77954215
 0.7835614  0.7833715  0.7825898  0.78424484 0.77883345 0.78406507
 0.7787674  0.77997106 0.78463995 0.7806109  0.77912843 0.7848965
 0.77348655 0.7778703  0.7814127  0.7788754  0.77600133 0.7802903
 0.7784216  0.78274435 0.7767179  0.77992505 0.7820181  0.78596365
 0.78291273 0.7803112  0.78256625 0.78512436 0.78191596 0.7810293
 0.78180647 0.7846237  0.78079915 0.77854127 0.77520716 0.77652603
 0.77870524 0.77833515 0.7820494  0.7800258  0.7850572  0.77915674
 0.7772409  0.78284603 0.7835775  0.78191006 0.7773008  0.7749081
 0.7856482  0.78349614]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
echo_repeat-2
WER
[-0.07169125 -0.06384636 -0.07904812 -0.05344804 -0.07428877 -0.05499518
 -0.08079311 -0.06233746 -0.07343254 -0.07370705 -0.08101431 -0.06065993
 -0.06607809 -0.07611582 -0.07323632 -0.07575236 -0.05764592 -0.08010707
 -0.05923572 -0.08182637 -0.08274813 -0.06188383 -0.06577213 -0.08189773
 -0.0521815  -0.0729268  -0.08010947 -0.07198675 -0.06802888 -0.0616501
 -0.05851988 -0.06615627 -0.05437873 -0.06759091 -0.05973346 -0.05516424
 -0.07847573 -0.08499558 -0.08643922 -0.05990384 -0.07495488 -0.06515115
 -0.06816819 -0.05115119 -0.07676453 -0.07329948 -0.07331208 -0.06518583
 -0.07164987 -0.05707138 -0.06538585 -0.08048866 -0.04571775 -0.05805079
 -0.07537908 -0.07202488 -0.0594139  -0.05382316 -0.06770726 -0.07586597
 -0.06169737 -0.07949659 -0.05689715 -0.07306452 -0.06665302 -0.07315195
 -0.0678559  -0.07936758 -0.05700343 -0.07019996 -0.07826785 -0.05383626
 -0.07214948 -0.07186222 -0.07796257 -0.07722592 -0.0572419  -0.07209573
 -0.06684931 -0.05985108 -0.0568025  -0.07635348 -0.06138362 -0.07999947
 -0.05044678 -0.07349364 -0.06245    -0.06092307 -0.07809941 -0.05330053
 -0.05875419 -0.06273746 -0.07742712 -0.06533558 -0.06682245 -0.04573336
 -0.07109528 -0.06647862 -0.07021292 -0.07439633 -0.07857265 -0.08019025
 -0.07034803 -0.08535426 -0.07261178 -0.05199779 -0.05434306 -0.05655486
 -0.07287254 -0.05636487 -0.07180166 -0.07389004 -0.0768277  -0.05535565
 -0.06846466 -0.06669033 -0.08018209 -0.0560701  -0.07600523 -0.05958259
 -0.06717275 -0.07218133 -0.0676688  -0.06665168 -0.0791881  -0.0695653
 -0.07318756 -0.04971029 -0.0511407  -0.06240307 -0.07036879 -0.06764003
 -0.07533832 -0.07545385 -0.06277935 -0.06444826 -0.06397558 -0.0594082
 -0.06768504 -0.07505107 -0.06356144 -0.07077491 -0.06863874 -0.08055733
 -0.07562384 -0.07152898 -0.05924591 -0.0463284  -0.08322465 -0.06429173
 -0.07383425 -0.07435913 -0.0636685  -0.07284108 -0.06292529 -0.05515798
 -0.0711085  -0.05936715 -0.06481792 -0.06276916 -0.05444515 -0.05058149
 -0.05816644 -0.06654834 -0.06155663 -0.05596076 -0.04233241 -0.06088244
 -0.06982763 -0.07012431 -0.05945923 -0.05327836 -0.07242869 -0.06683951
 -0.06169461 -0.0630719  -0.0709757  -0.06147724 -0.0733232  -0.06085873
 -0.06945985 -0.06706449 -0.059252   -0.05009622 -0.07356273 -0.07131607
 -0.0649657  -0.07561931 -0.06137595 -0.06440771 -0.06670764 -0.05427786
 -0.04709031 -0.07518817 -0.06679727 -0.07484188 -0.05974953 -0.05116661
 -0.06538583 -0.07781027]
echo_repeat-2
BLEU
[0.14380481 0.15963693 0.14502766 0.15470749 0.183344   0.18416751
 0.16835956 0.15827859 0.17970413 0.13625928 0.13129358 0.13503612
 0.16864023 0.17368572 0.17095414 0.14457926 0.15632218 0.15115384
 0.16554688 0.13310601 0.13242277 0.17554406 0.15710711 0.14515722
 0.19050113 0.13866214 0.12193301 0.1543261  0.16357128 0.15374948
 0.18868008 0.14027467 0.17327126 0.15433213 0.1591396  0.1854962
 0.13204445 0.12443454 0.17765617 0.17927775 0.15699453 0.17531198
 0.15567855 0.1962772  0.17812273 0.16427849 0.13132074 0.17660776
 0.1569176  0.16692096 0.180574   0.13675946 0.17798067 0.16243346
 0.14834035 0.17467483 0.19136657 0.18256743 0.16574188 0.15364488
 0.14933639 0.15424608 0.18612045 0.1570752  0.16200697 0.13645613
 0.16997264 0.14971738 0.17662918 0.17811799 0.14237311 0.16418507
 0.12133752 0.15339065 0.15886709 0.15396229 0.14586476 0.14639659
 0.20581003 0.22413188 0.17232993 0.17782581 0.172108   0.14171619
 0.16930308 0.16454326 0.17454523 0.14272089 0.12723457 0.13719867
 0.16894468 0.17738359 0.16048699 0.15706675 0.16994916 0.17145196
 0.15015205 0.16981197 0.15861164 0.16778962 0.15122676 0.13474111
 0.12915984 0.13752527 0.16781811 0.16250423 0.17481892 0.17454395
 0.15733242 0.20983796 0.17471772 0.18461043 0.16144209 0.15830843
 0.16282442 0.14907018 0.11668603 0.15685918 0.1249953  0.17125126
 0.14694329 0.15283373 0.15657604 0.17381907 0.14038976 0.17447184
 0.16381899 0.1739267  0.1618426  0.15015548 0.16255967 0.17523502
 0.14780369 0.11954782 0.17574904 0.16001158 0.19555865 0.19130503
 0.17269195 0.16272095 0.18714631 0.16181146 0.19019173 0.13617321
 0.13525261 0.13867686 0.16073961 0.1934815  0.14331396 0.13345531
 0.15461242 0.16810983 0.16496122 0.16801821 0.15585167 0.17728502
 0.14078805 0.17074566 0.16546671 0.16594916 0.18680747 0.16096185
 0.17122489 0.1504855  0.19865004 0.17325163 0.19245102 0.15218987
 0.11744476 0.19283157 0.18687163 0.16005284 0.13738388 0.18144894
 0.15193552 0.16842721 0.17293461 0.15826398 0.16817156 0.16959175
 0.15341939 0.16115622 0.1587756  0.15664486 0.13833972 0.16573272
 0.15469382 0.14035716 0.18017583 0.18291016 0.16825337 0.16994462
 0.16389017 0.15328755 0.17589974 0.16721257 0.16464549 0.17155487
 0.16319127 0.14381624]
echo_repeat-2
METEOR
[0.09702174 0.11014329 0.09661596 0.1139158  0.11563673 0.11387267
 0.11466776 0.11452155 0.11250754 0.09520864 0.09719027 0.1156625
 0.10997978 0.1162753  0.11603235 0.11696246 0.11353534 0.08969648
 0.122214   0.09055569 0.0871733  0.10935272 0.10654024 0.08255074
 0.12564142 0.1000696  0.08551413 0.09418308 0.11147604 0.09884204
 0.12449116 0.09623079 0.11513665 0.1180868  0.11347576 0.12954452
 0.08719524 0.09322553 0.11234805 0.11428542 0.11546395 0.12879447
 0.11093273 0.12735301 0.11812461 0.10751444 0.10147725 0.11721717
 0.10407271 0.12661615 0.12705374 0.09316551 0.10925846 0.10006834
 0.09518395 0.11521084 0.14026112 0.13645418 0.10350577 0.09365897
 0.09438911 0.11160386 0.1086086  0.094795   0.10919415 0.0993408
 0.1182666  0.09826353 0.12450568 0.1116555  0.08293017 0.10685711
 0.08528497 0.10325203 0.10397356 0.10337425 0.10128365 0.0945464
 0.12484405 0.15119088 0.10679564 0.11448509 0.11007591 0.0956513
 0.10736294 0.12514804 0.11616669 0.09390276 0.08746429 0.09377036
 0.11000852 0.12596579 0.10369291 0.1117561  0.11957458 0.12312165
 0.10438676 0.11784271 0.11079688 0.10960106 0.12170093 0.09390701
 0.09188865 0.09682069 0.12325983 0.11813388 0.12005727 0.1095247
 0.10635151 0.1386564  0.12240702 0.12705778 0.11938838 0.10537149
 0.11511671 0.10582689 0.09880749 0.09040521 0.09248368 0.10899156
 0.09437139 0.09272331 0.12058453 0.11869157 0.09280363 0.11573506
 0.10438035 0.11300809 0.09755912 0.09605918 0.11213138 0.11213811
 0.1105699  0.08578743 0.11512751 0.10057255 0.13871116 0.12140727
 0.122639   0.11048379 0.13261283 0.13300207 0.11689389 0.09410737
 0.10385607 0.10339682 0.10411599 0.14547008 0.09605055 0.10061381
 0.11226072 0.11730716 0.12006515 0.09464885 0.11069031 0.11055893
 0.09551856 0.11824073 0.11889408 0.12068777 0.11224953 0.12023436
 0.13142879 0.10891444 0.12777488 0.11103888 0.12442632 0.09831101
 0.08264219 0.12999872 0.1210599  0.11159823 0.0901353  0.11876162
 0.0964039  0.10093537 0.11923057 0.10250025 0.12069589 0.11177252
 0.12524426 0.11202592 0.09732135 0.1109945  0.09304172 0.10212084
 0.10924045 0.08788233 0.11213671 0.11101553 0.10551217 0.10845306
 0.10800158 0.10049065 0.13496275 0.11098189 0.11277703 0.12532384
 0.10508093 0.11231821]
echo_repeat-2
BERT
[0.78035176 0.78327596 0.78251404 0.78072274 0.78173745 0.784087
 0.78083783 0.78166926 0.78390634 0.7763927  0.77549624 0.7768176
 0.7806579  0.78175706 0.78212404 0.7811878  0.78243035 0.7799213
 0.7843298  0.7764012  0.7848657  0.7876104  0.7805605  0.77802306
 0.781821   0.7787377  0.78165734 0.777797   0.77923083 0.7795091
 0.7815386  0.7798763  0.7854725  0.7813153  0.7796161  0.78143215
 0.7815036  0.78086436 0.7805589  0.77648324 0.7810788  0.77948624
 0.78235227 0.78664833 0.783235   0.7835632  0.78103924 0.78353566
 0.7798649  0.785331   0.7819172  0.77463734 0.78528774 0.78439516
 0.7811732  0.7796007  0.78537154 0.7799213  0.78329456 0.7787366
 0.7858936  0.7807667  0.78083587 0.77929217 0.7792813  0.7814527
 0.78446865 0.77718765 0.78581506 0.78253126 0.7799129  0.78599995
 0.78217846 0.7795297  0.7822339  0.77753735 0.77998185 0.7798826
 0.7817064  0.78564185 0.78520083 0.7833681  0.78158516 0.77789086
 0.78611356 0.7826812  0.78090584 0.7809949  0.78138804 0.7793293
 0.7781986  0.78269166 0.78462416 0.7828528  0.779535   0.7796869
 0.7826448  0.780758   0.7793339  0.7799556  0.7792937  0.7827599
 0.7793313  0.78247684 0.7818869  0.7764677  0.7818141  0.77792805
 0.7825481  0.7804934  0.78177756 0.7802821  0.7804288  0.782756
 0.77870893 0.77627695 0.7801733  0.7831263  0.77622175 0.7822269
 0.7805925  0.7818904  0.7822752  0.7811759  0.7774786  0.78209424
 0.7828104  0.78236884 0.7819492  0.7825211  0.78039825 0.77988654
 0.7817522  0.78356206 0.7804575  0.7841775  0.7803122  0.78060865
 0.7821929  0.78176385 0.7851325  0.7810984  0.7849497  0.77488214
 0.7767651  0.7833904  0.7786072  0.786927   0.78220713 0.7797106
 0.78052276 0.782504   0.78166527 0.78356624 0.7833101  0.78559613
 0.77946496 0.7859187  0.77854526 0.7844532  0.7833691  0.7767236
 0.785822   0.7780349  0.77753145 0.7838538  0.78297585 0.78048444
 0.7798418  0.78056    0.7889607  0.77854526 0.7813497  0.7845613
 0.7808539  0.785243   0.78035426 0.78264374 0.78099555 0.7801963
 0.7814947  0.782136   0.78091896 0.78355396 0.77760106 0.7807451
 0.7807592  0.77726316 0.7812999  0.7797441  0.7784427  0.78049654
 0.78362536 0.7824518  0.782913   0.7836809  0.78321344 0.7844469
 0.78558236 0.78122914]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wheretheressmoke
WER
[0.03181404 0.03813032 0.03926971 0.04023526 0.03708657 0.03210404
 0.03740181 0.03063    0.03681124 0.03405928 0.03282278 0.04236226
 0.03789147 0.03893855 0.03145111 0.03452689 0.03240896 0.03586264
 0.03515567 0.03225639 0.04216381 0.0423935  0.03758071 0.03756361
 0.03842759 0.04031976 0.0380368  0.03225673 0.03436935 0.03582391
 0.03816291 0.03198479 0.03616765 0.03396795 0.03747468 0.03953005
 0.03523165 0.04038963 0.04209793 0.03507579 0.03598315 0.03648986
 0.04097565 0.03519576 0.03584704 0.03445496 0.03700176 0.04077489
 0.03262886 0.03806107 0.03896108 0.04125308 0.03639007 0.03309055
 0.03540781 0.03912855 0.03127671 0.03414359 0.04126492 0.0408783
 0.03501642 0.03889549 0.03989251 0.03526109 0.03558753 0.04338409
 0.04191724 0.04007017 0.03812488 0.03340314 0.0333333  0.03847985
 0.0387145  0.03997633 0.03582378 0.02956844 0.04310892 0.04306469
 0.03562171 0.03592373 0.03627021 0.03760557 0.03859188 0.03740764
 0.0340853  0.04353849 0.03497286 0.0345173  0.03806667 0.03337858
 0.0410939  0.04361516 0.03470886 0.03401942 0.03561864 0.03740803
 0.03735136 0.04024613 0.03650149 0.03749286 0.02641324 0.03187578
 0.0377476  0.03167508 0.03694964 0.03469189 0.03489087 0.03892041
 0.03539266 0.02957964 0.03173705 0.03456564 0.03692755 0.03472826
 0.0363996  0.03514881 0.03801063 0.03896255 0.04109844 0.03291594
 0.03291335 0.03408778 0.03406649 0.03710846 0.03552969 0.03452814
 0.03744062 0.03485235 0.03320488 0.03839625 0.03553217 0.04206972
 0.03354483 0.03891476 0.0401269  0.03783471 0.04327235 0.03529418
 0.03180332 0.03929767 0.04213402 0.04274711 0.04728911 0.0302993
 0.03496137 0.03685532 0.0335671  0.03635545 0.03432855 0.03710078
 0.03789308 0.0360683  0.03491842 0.035421   0.03571667 0.03445494
 0.03909174 0.03872579 0.04096415 0.03345578 0.0381574  0.03539188
 0.03696448 0.04091733 0.03167474 0.03243718 0.03678643 0.03444743
 0.03414596 0.03480544 0.04310567 0.04030394 0.04038011 0.0412262
 0.03104348 0.03698284 0.03173313 0.04031744 0.03899871 0.03678923
 0.03574979 0.03737562 0.03690173 0.03720287 0.04051434 0.03776102
 0.0402985  0.03944454 0.03889168 0.02825812 0.03488829 0.03955065
 0.03726262 0.03743396 0.04660963 0.03269072 0.03565952 0.03381259
 0.0379709  0.04027435]
wheretheressmoke
BLEU
[0.18309333 0.18127426 0.18710482 0.20094574 0.17923135 0.17748694
 0.18426417 0.18499093 0.18266258 0.18118888 0.18087578 0.19303225
 0.19154594 0.19601664 0.18198443 0.1834711  0.18086953 0.18361903
 0.18297908 0.18032093 0.19484913 0.19093562 0.19106441 0.1823355
 0.19629471 0.19420811 0.18796298 0.19082064 0.17728092 0.18997577
 0.19517622 0.17814115 0.18318286 0.1875165  0.19178896 0.19894946
 0.18686699 0.19642192 0.19538676 0.18189116 0.19194963 0.19820983
 0.20025096 0.18654307 0.18695111 0.19040151 0.18942509 0.19318794
 0.18583165 0.195793   0.19609973 0.18535514 0.18812408 0.19108528
 0.18962607 0.19587034 0.1798149  0.17997788 0.19144145 0.20362482
 0.19225912 0.20351811 0.18834293 0.17683918 0.18053615 0.2010136
 0.1915091  0.19378554 0.19991485 0.19220429 0.18748904 0.19321929
 0.19716597 0.18808543 0.18900806 0.18660983 0.20020615 0.19535305
 0.19005451 0.17999793 0.19270522 0.18617319 0.20037341 0.18680288
 0.18784611 0.1985869  0.18450252 0.19623075 0.19670796 0.19169488
 0.20224545 0.19990531 0.19286713 0.19392102 0.19079366 0.19571698
 0.19842577 0.18623593 0.19358587 0.19592188 0.18108439 0.17615776
 0.19625213 0.18679863 0.19215477 0.18267162 0.19222653 0.19945981
 0.17941115 0.18574774 0.18537892 0.19865283 0.19488971 0.18781867
 0.19632042 0.18516408 0.1918911  0.19527578 0.19349414 0.1802453
 0.17965241 0.19356237 0.18043251 0.1947798  0.18641471 0.19634432
 0.19635361 0.1824197  0.18754804 0.17625773 0.19343989 0.19013537
 0.18252551 0.19013775 0.19797881 0.18496871 0.19100326 0.17697126
 0.17867782 0.18847091 0.1998317  0.18867105 0.19382621 0.17489973
 0.17709224 0.19743242 0.18204296 0.19795611 0.18416677 0.18205953
 0.20163915 0.1907589  0.19594648 0.19484784 0.17961777 0.17887917
 0.2029439  0.19445743 0.19561682 0.18521325 0.19981601 0.1847883
 0.18724223 0.19715516 0.17892242 0.18635777 0.18087476 0.17719167
 0.17516701 0.18894333 0.19280484 0.19494667 0.1939061  0.19304299
 0.18829675 0.19601877 0.16964616 0.19948285 0.20236876 0.19414507
 0.18500285 0.18499326 0.18675058 0.18935001 0.19591253 0.19211418
 0.18792705 0.19328974 0.1870619  0.18102738 0.1815278  0.19074265
 0.18985357 0.18428794 0.19912866 0.18964814 0.19442699 0.19006047
 0.18885837 0.19310662]
wheretheressmoke
METEOR
[0.13590084 0.13221025 0.13659215 0.14787867 0.13425267 0.13439132
 0.13428785 0.13258923 0.13552397 0.13567257 0.13295364 0.13788459
 0.14437177 0.14182551 0.13100597 0.1341275  0.13111865 0.13052017
 0.13232494 0.13248097 0.14517217 0.13799815 0.13565069 0.13451739
 0.14377798 0.13888535 0.1339891  0.1382832  0.12919848 0.1373639
 0.14410305 0.13450414 0.13492405 0.13786641 0.13794213 0.14451084
 0.13753314 0.14310929 0.13941444 0.13195317 0.13452476 0.1426718
 0.14696067 0.13601093 0.13433308 0.13800199 0.13823066 0.13682929
 0.137687   0.1445758  0.14009059 0.13273373 0.13350852 0.13363962
 0.13808286 0.1386684  0.12970851 0.13848    0.13493429 0.1407935
 0.14017142 0.1479445  0.13936309 0.1330032  0.13092342 0.14630449
 0.13736644 0.13999742 0.14139816 0.14139043 0.13923311 0.14596738
 0.14781952 0.13153995 0.13648067 0.13750429 0.1426869  0.14289515
 0.13649941 0.13174391 0.14229582 0.13434001 0.14421533 0.1386687
 0.14058907 0.14150853 0.1370593  0.1451113  0.13974026 0.13829827
 0.13897704 0.14452788 0.13748664 0.13601605 0.13700296 0.14090722
 0.15226035 0.13643009 0.14010073 0.13957774 0.13126954 0.12969989
 0.13862436 0.13927863 0.13916348 0.12939819 0.13728017 0.14340961
 0.13866307 0.13652971 0.13746365 0.14257927 0.14046163 0.13879499
 0.13981497 0.13644316 0.13607881 0.14271748 0.13644853 0.13070634
 0.1289959  0.14326662 0.1317945  0.14315163 0.12958211 0.13830866
 0.1418571  0.13261155 0.13712592 0.13105197 0.14056308 0.13557976
 0.13286939 0.1352115  0.14767168 0.13450683 0.14186506 0.13150422
 0.13236232 0.13785309 0.14214381 0.13969877 0.13998087 0.13030917
 0.13171253 0.14216318 0.12744346 0.14128099 0.1366334  0.13357626
 0.14821685 0.14103968 0.1378787  0.14192312 0.13397387 0.13163596
 0.1463691  0.14987516 0.13965187 0.13608079 0.14180228 0.13402274
 0.13472954 0.1462186  0.12925597 0.13171013 0.13052887 0.13340219
 0.13068018 0.14107184 0.14121393 0.14312754 0.14373269 0.14257667
 0.13477823 0.1442162  0.1258957  0.14155769 0.14640213 0.14321718
 0.14030107 0.13597947 0.13406218 0.13360651 0.14018548 0.14127159
 0.13965104 0.1348732  0.1297814  0.13165162 0.13267511 0.1402909
 0.13667545 0.13426519 0.14137469 0.13529866 0.14317101 0.13628479
 0.13632303 0.13988938]
wheretheressmoke
BERT
[0.7892914  0.7885404  0.7905563  0.79113847 0.7898794  0.7882334
 0.7893985  0.7896746  0.7896588  0.7894326  0.78790593 0.79096866
 0.79009503 0.789378   0.7887318  0.7899213  0.7888626  0.79027414
 0.79033977 0.78990376 0.7903139  0.7903538  0.78941923 0.78905296
 0.7903165  0.79087    0.7880331  0.7891212  0.7897069  0.7903992
 0.79190874 0.7879795  0.7899363  0.7923285  0.78889567 0.78933394
 0.7909317  0.79242724 0.7900478  0.78846246 0.7905103  0.7902422
 0.7914434  0.7878052  0.7887377  0.7884266  0.7898156  0.7903325
 0.7891025  0.79123056 0.78997606 0.7897083  0.7911151  0.7884052
 0.79059166 0.79035354 0.7882848  0.7892338  0.7895488  0.7902592
 0.79224676 0.791237   0.79001075 0.7888043  0.78979087 0.79205877
 0.7916726  0.79041445 0.7908528  0.7902981  0.7896348  0.7927104
 0.7892035  0.7885853  0.7904161  0.78986245 0.7902307  0.79131603
 0.7908079  0.78942454 0.7906379  0.78933036 0.7900687  0.792181
 0.78960073 0.7911844  0.7885548  0.79252857 0.78935283 0.79011637
 0.78953516 0.791647   0.7899716  0.7898734  0.7908553  0.7906518
 0.7912294  0.79013467 0.7910748  0.7899347  0.7900224  0.78864706
 0.79164475 0.79185367 0.789346   0.7904559  0.7876169  0.78977686
 0.7889256  0.7889276  0.78700256 0.79032767 0.79026556 0.78899276
 0.79104644 0.7902725  0.78949714 0.79089063 0.7907973  0.7900764
 0.7886851  0.7899137  0.7898395  0.7903906  0.78772074 0.7901118
 0.7892222  0.7881879  0.7894199  0.79035896 0.79146355 0.78918576
 0.7889371  0.78943497 0.7906526  0.7909749  0.7932416  0.79011875
 0.7883455  0.7902616  0.7904324  0.7904384  0.7912923  0.7903961
 0.78922117 0.79059076 0.7900244  0.7890854  0.7909367  0.78877044
 0.7907592  0.79065937 0.7907014  0.78893715 0.78873    0.7903403
 0.7894817  0.7901868  0.7911261  0.79010737 0.79146355 0.7886375
 0.789097   0.79070777 0.7892087  0.7891852  0.7881768  0.78942484
 0.7898251  0.78902805 0.7904283  0.7898476  0.79052144 0.790773
 0.7909112  0.7890585  0.7882868  0.7931371  0.79128355 0.7897654
 0.7880829  0.7901862  0.78925747 0.7897687  0.7909956  0.7904845
 0.79150665 0.7891355  0.7892096  0.7900406  0.79036915 0.7904751
 0.791711   0.7889365  0.79026043 0.7908267  0.791235   0.7881296
 0.7888353  0.7931524 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/net/scratch/q12628ct/semantic-decoding/decoding/evaluate_predictions.py", line 50, in <module>
    ref_data = load_transcript(args.experiment, reference)
  File "/net/scratch/q12628ct/semantic-decoding/decoding/utils_eval.py", line 23, in load_transcript
    with open(grid_path) as f: 
FileNotFoundError: [Errno 2] No such file or directory: '/net/scratch/q12628ct/semantic-decoding/data_test/test_stimulus/perceived_movie/sintel.TextGrid'
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
presto
WER
[0.02525499 0.02940122 0.03035366 0.02650691 0.02861313 0.032496
 0.02731428 0.02501733 0.03269788 0.02515082 0.02894543 0.0329792
 0.04095692 0.03209369 0.03146503 0.0335658  0.03293754 0.04081977
 0.03317812 0.02426443 0.03551627 0.02901045 0.03603577 0.03486369
 0.0391466  0.02725317 0.03296262 0.03975518 0.02598933 0.03423535
 0.02777807 0.03848603 0.03400068 0.02832112 0.03389687 0.03187007
 0.03580483 0.03218038 0.03070567 0.0330047  0.04015292 0.03354546
 0.03118214 0.03077251 0.02812495 0.02829285 0.03352392 0.02441646
 0.04241228 0.03569009 0.03245267 0.02468128 0.02932945 0.02590221
 0.03520335 0.03600964 0.02922809 0.03130335 0.03313767 0.02785629
 0.03415294 0.03611474 0.03790706 0.0370096  0.02417219 0.03257647
 0.03835144 0.02746586 0.02798903 0.03430463 0.03131716 0.03731376
 0.03491943 0.03587861 0.01887862 0.02728833 0.03915931 0.03446746
 0.026851   0.03153219 0.02909946 0.02478481 0.02833266 0.03240435
 0.02770404 0.02760965 0.03415837 0.0311182  0.02910645 0.03248659
 0.03044025 0.03598899 0.03090556 0.0325129  0.02583851 0.03363056
 0.03982338 0.02791942 0.03841934 0.02708539 0.0334969  0.03152145
 0.03201966 0.03392799 0.03557992 0.02854462 0.03800542 0.02767741
 0.03114183 0.03328229 0.03736478 0.03029052 0.03758839 0.03974605
 0.02718546 0.03134433 0.028634   0.03557783 0.03981154 0.03031216
 0.03529267 0.03969914 0.02973477 0.02822258 0.04288518 0.02698472
 0.02461116 0.0303632  0.03259863 0.03319224 0.0362155  0.02302089
 0.02744053 0.03117605 0.02369247 0.0355113  0.04192882 0.04086342
 0.03390849 0.03230226 0.03282549 0.02615149 0.03912433 0.03736749
 0.02638167 0.04072146 0.0355178  0.04096218 0.03444142 0.02504983
 0.03483284 0.03134107 0.02978548 0.0303493  0.02698575 0.026343
 0.0323422  0.02241573 0.03796095 0.03134877 0.0286499  0.02906382
 0.02759057 0.03696495 0.03621611 0.03566763 0.02061348 0.02213912
 0.03973531 0.02907121 0.03938916 0.0390088  0.03115785 0.03129299
 0.02833184 0.02617978 0.03071286 0.02415149 0.03083831 0.03274734
 0.02916354 0.03153611 0.02921657 0.0276788  0.03616575 0.03203434
 0.03081447 0.03060711 0.02657908 0.02377181 0.02813701 0.03578291
 0.0370714  0.03237176 0.03176361 0.02754121 0.02619232 0.03418555
 0.02732915 0.02700801]
presto
BLEU
[0.11904805 0.10508782 0.1174668  0.12876025 0.10551393 0.12458038
 0.10658726 0.11398638 0.11898454 0.10346059 0.10586224 0.11209928
 0.12031027 0.11081238 0.11792779 0.11762791 0.12118017 0.14016842
 0.11408796 0.10285191 0.12376141 0.11674961 0.12869513 0.11574448
 0.12938824 0.10675991 0.12644899 0.13270716 0.11354293 0.11905007
 0.09981435 0.11967779 0.12624843 0.12395329 0.12802669 0.11426928
 0.13417602 0.12976778 0.10934091 0.11874951 0.12403427 0.12914628
 0.11534618 0.1143125  0.11583563 0.10708976 0.12242382 0.10011635
 0.12329261 0.11910248 0.11700397 0.09975256 0.11310278 0.1126892
 0.1293387  0.12876404 0.1262721  0.1218975  0.11379682 0.11702994
 0.1209816  0.1280553  0.13054956 0.13294435 0.11922871 0.11419871
 0.13672875 0.10998271 0.10587788 0.11573041 0.10346356 0.13157898
 0.11420293 0.12219119 0.10009973 0.11497767 0.13224836 0.12174096
 0.10966213 0.12051235 0.12545747 0.09719396 0.10545517 0.10680942
 0.12012965 0.11550998 0.12328109 0.10557829 0.11304629 0.10489783
 0.11231482 0.12677665 0.10715142 0.13116046 0.12025358 0.12080961
 0.12403776 0.11863791 0.13313711 0.10770314 0.11579882 0.12510388
 0.11534378 0.12423263 0.13163698 0.11305073 0.13987419 0.11523124
 0.11817183 0.10636088 0.12205723 0.11774213 0.12042239 0.13498814
 0.10379378 0.12396109 0.11567197 0.12776222 0.13758798 0.11633898
 0.11143202 0.14005217 0.10913041 0.10583068 0.1200578  0.10432224
 0.11038269 0.11602328 0.1212332  0.1264404  0.12325589 0.10052916
 0.10043138 0.12040565 0.10883296 0.12448669 0.12313514 0.14078661
 0.13611916 0.11432332 0.11409019 0.1074018  0.12704634 0.13144604
 0.11380217 0.13044258 0.11582643 0.12353555 0.12937752 0.10737314
 0.11087459 0.11457569 0.11379549 0.12163014 0.11944994 0.09439953
 0.11570819 0.10591148 0.1258923  0.11760194 0.10525397 0.10882002
 0.11216556 0.13514849 0.12750722 0.12454451 0.10189017 0.0953121
 0.13855275 0.10364144 0.13817995 0.13636748 0.10106425 0.12647211
 0.10871019 0.10681547 0.10947134 0.10855939 0.12001449 0.11641386
 0.11498656 0.11529098 0.12040118 0.11885532 0.13378242 0.12136558
 0.1174204  0.11640121 0.09839591 0.10718342 0.11824428 0.11223811
 0.12424973 0.1249657  0.11714582 0.11702651 0.10705852 0.12261676
 0.11855242 0.11126567]
presto
METEOR
[0.08176135 0.07724718 0.08694055 0.08967504 0.0720382  0.08435705
 0.07715877 0.0807111  0.08039377 0.07464527 0.07887621 0.07807897
 0.08345323 0.07929839 0.08052976 0.0850488  0.08477245 0.09497028
 0.08326679 0.07223615 0.08265293 0.08322929 0.08790637 0.07796498
 0.08579054 0.07683596 0.08533347 0.08558394 0.07802334 0.08126989
 0.07134617 0.08791677 0.08867206 0.08217618 0.08876081 0.07968679
 0.08932139 0.08838499 0.07832607 0.08455596 0.08575401 0.08500639
 0.07993492 0.08261623 0.0799618  0.07884938 0.08231814 0.07584416
 0.07928397 0.08109989 0.0801472  0.07392753 0.07866767 0.0726592
 0.09051953 0.08724127 0.08933229 0.08459563 0.08525125 0.08179359
 0.08451056 0.08574635 0.08468962 0.09003647 0.08163639 0.08325525
 0.08757256 0.07645424 0.07733219 0.0822184  0.07472554 0.09204913
 0.08117163 0.08718884 0.07841812 0.07676101 0.08936789 0.08201945
 0.07686902 0.08629036 0.08443477 0.06888414 0.07354698 0.07501055
 0.08021257 0.07827378 0.08980206 0.07947783 0.0772656  0.07307872
 0.08007307 0.08340882 0.07586899 0.08340099 0.07934482 0.07854971
 0.08383999 0.0848796  0.09689612 0.07342696 0.08006197 0.0852342
 0.07658971 0.08495328 0.09249549 0.07652338 0.08926326 0.07642656
 0.07914565 0.07861695 0.08142471 0.08095363 0.08273661 0.09314719
 0.07411937 0.08605358 0.08024488 0.08593387 0.09082407 0.07552168
 0.07885601 0.08978298 0.07722127 0.07970383 0.0835936  0.07377673
 0.07793384 0.07763279 0.08250218 0.08323037 0.08440016 0.07223402
 0.0735302  0.08314927 0.07382893 0.08758216 0.08310494 0.09690051
 0.09344376 0.07814612 0.08211586 0.07732242 0.0870969  0.08504447
 0.08042682 0.08790434 0.08044569 0.08412678 0.08504647 0.08199659
 0.08011887 0.07905183 0.08150579 0.08076464 0.08502881 0.07142325
 0.08056661 0.07060234 0.0859147  0.08208477 0.07549305 0.08004451
 0.07959415 0.08996441 0.08648544 0.08277258 0.07972397 0.06831372
 0.08993964 0.07591626 0.09442534 0.09129119 0.07218762 0.08868486
 0.07787294 0.07645923 0.07591798 0.07545294 0.08912373 0.08240778
 0.08132797 0.08676079 0.08531603 0.08038387 0.08630968 0.07733283
 0.08037745 0.07612182 0.07349747 0.07677642 0.08314036 0.08240015
 0.08585957 0.08675691 0.08032186 0.07672674 0.06812177 0.08573191
 0.08110551 0.07828922]
presto
BERT
[0.7732689  0.77201355 0.7713179  0.7724245  0.7717228  0.771363
 0.76866263 0.77197635 0.7695408  0.7705778  0.7695443  0.77086633
 0.772339   0.7701306  0.7725474  0.7744839  0.7709381  0.7745241
 0.76994735 0.7720463  0.77157587 0.7738119  0.7737408  0.76939785
 0.77128255 0.77172136 0.7710717  0.7714028  0.7696167  0.77147996
 0.770969   0.7712012  0.7732188  0.77233213 0.7704691  0.7705332
 0.77157515 0.77160484 0.77159184 0.774722   0.77036345 0.7759725
 0.7692589  0.7731395  0.7740912  0.7691697  0.7743882  0.7720284
 0.7672526  0.7709993  0.76912856 0.7703256  0.7714788  0.7679995
 0.77488124 0.773878   0.7756071  0.7733919  0.7725268  0.7732234
 0.77063465 0.7728125  0.7740666  0.7721806  0.7708433  0.77046144
 0.77478033 0.76993304 0.77232134 0.7738893  0.77039206 0.7740942
 0.77171606 0.7719555  0.77121675 0.77238953 0.77217346 0.77221596
 0.7732811  0.76933193 0.7711505  0.77147007 0.7698958  0.77118623
 0.77099556 0.7722111  0.77117425 0.7690305  0.7713385  0.7720067
 0.7722485  0.7718983  0.77164626 0.7727734  0.7717403  0.7691627
 0.77135336 0.77029836 0.7752489  0.77139074 0.7716479  0.7717283
 0.77182513 0.77365154 0.77189064 0.7721614  0.77587616 0.7699749
 0.77265906 0.7732522  0.77025443 0.7739089  0.77342176 0.77653354
 0.77120894 0.7708729  0.7686428  0.77457327 0.7736169  0.7713996
 0.76793456 0.77378124 0.77194494 0.7691066  0.77067524 0.7689297
 0.7708607  0.7707956  0.77089083 0.77163255 0.7707709  0.77087116
 0.7655177  0.7720502  0.77417934 0.7727407  0.77280116 0.7738773
 0.7717815  0.77104574 0.77267295 0.7710587  0.7746315  0.77308965
 0.77217096 0.7751996  0.7711871  0.7718949  0.7722939  0.7725989
 0.7727179  0.76996213 0.7705067  0.77119935 0.772633   0.7684793
 0.77506286 0.7728027  0.7711552  0.7730561  0.7712324  0.77201194
 0.77144414 0.76988447 0.7725517  0.7725491  0.76903355 0.7682778
 0.7734398  0.7684778  0.7735358  0.77294    0.7686264  0.77094054
 0.769394   0.77112365 0.7714876  0.7720105  0.7734784  0.77218324
 0.77237546 0.7722908  0.7704632  0.7714749  0.77111745 0.76879174
 0.7684292  0.7735749  0.76870376 0.7681681  0.77178276 0.77321637
 0.772544   0.7729466  0.7712785  0.76872504 0.7701935  0.77418953
 0.7707372  0.7690864 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
partlycloudy
WER
[-0.14280908 -0.13294832 -0.12954632 -0.1393652  -0.13495727 -0.14093017
 -0.14183504 -0.14270639 -0.13203924 -0.14276149 -0.13337418 -0.13362839
 -0.13845517 -0.14334391 -0.12540853 -0.14074534 -0.13823685 -0.14064902
 -0.13973995 -0.13591905 -0.14982081 -0.13954021 -0.12943292 -0.13490892
 -0.13946428 -0.13408583 -0.13858859 -0.14357826 -0.13843588 -0.14491351
 -0.13812618 -0.13126552 -0.13521309 -0.13295197 -0.13587428 -0.14320309
 -0.13348573 -0.13788044 -0.14718797 -0.13508438 -0.13120058 -0.1311585
 -0.13904676 -0.13161357 -0.13898923 -0.14046078 -0.13826289 -0.14123728
 -0.13211779 -0.12890616 -0.12654142 -0.13529884 -0.14145217 -0.13229031
 -0.13025726 -0.13654406 -0.13764034 -0.13093117 -0.14510121 -0.1408897
 -0.13964089 -0.13812909 -0.14161921 -0.1320867  -0.13362772 -0.13893125
 -0.13781331 -0.14842243 -0.14025729 -0.12983256 -0.13896639 -0.13869556
 -0.14346698 -0.13468326 -0.1393322  -0.13827229 -0.13013458 -0.14417037
 -0.1414624  -0.1389587  -0.13191258 -0.14026453 -0.13853877 -0.12930411
 -0.1376099  -0.14834697 -0.12781631 -0.14010534 -0.13468979 -0.13590636
 -0.13347031 -0.14105075 -0.13967718 -0.13940486 -0.13682627 -0.12613667
 -0.14064022 -0.12847028 -0.13996518 -0.14177802 -0.1399787  -0.13355616
 -0.13439982 -0.12249907 -0.14399529 -0.13614477 -0.14000092 -0.13223054
 -0.13598523 -0.14122937 -0.1458823  -0.14792978 -0.12968277 -0.14510101
 -0.13512614 -0.13240169 -0.13048624 -0.14569394 -0.14458817 -0.14705935
 -0.1355926  -0.13310871 -0.13159816 -0.13991331 -0.14137312 -0.13320387
 -0.13100051 -0.12590712 -0.13279339 -0.14276832 -0.13532286 -0.13606332
 -0.13280566 -0.12857882 -0.14452212 -0.14338157 -0.12214179 -0.13783284
 -0.14377354 -0.13464228 -0.12968625 -0.1313406  -0.13357895 -0.13127268
 -0.14009441 -0.14460333 -0.14325133 -0.13283445 -0.14442777 -0.1389162
 -0.13935088 -0.14676985 -0.13114083 -0.12858157 -0.13462061 -0.14489806
 -0.12691409 -0.14182514 -0.14141506 -0.13863963 -0.13589195 -0.14565588
 -0.13939512 -0.13624755 -0.13960274 -0.13220589 -0.13766497 -0.14307202
 -0.13262037 -0.13824185 -0.139357   -0.13158449 -0.13076681 -0.14054609
 -0.14099538 -0.13594794 -0.14283778 -0.13561819 -0.13062493 -0.14368841
 -0.1305402  -0.13069009 -0.13947729 -0.13435391 -0.14078538 -0.14617321
 -0.13879385 -0.14103939 -0.13286412 -0.13222914 -0.12943925 -0.14111579
 -0.1308875  -0.12589313 -0.14190756 -0.13769105 -0.13333767 -0.13127032
 -0.1414861  -0.14217685]
partlycloudy
BLEU
[0.1027904  0.12402368 0.14396451 0.1227791  0.13055218 0.11547917
 0.11856733 0.11937903 0.13644282 0.11038734 0.11928655 0.13128949
 0.12798729 0.12657675 0.13590172 0.120378   0.1204928  0.11873834
 0.11561469 0.12716553 0.10661746 0.12642926 0.13923171 0.13163106
 0.12561873 0.13668256 0.12282909 0.10967163 0.12269111 0.1207149
 0.12790695 0.13615231 0.13941087 0.12863781 0.12364952 0.11430249
 0.11853652 0.12030936 0.10095394 0.12739785 0.13065601 0.12456834
 0.13053727 0.12308479 0.1231523  0.12000848 0.12272353 0.12635606
 0.1258186  0.13395042 0.13164885 0.11473127 0.12137019 0.13687515
 0.14499824 0.12356644 0.11608341 0.1242253  0.12173    0.12471333
 0.10368525 0.1385015  0.1124945  0.14175185 0.11997365 0.12036133
 0.11362322 0.1231231  0.10797616 0.13675736 0.1180384  0.13099072
 0.10025886 0.13073886 0.12847509 0.12131466 0.12983027 0.13011256
 0.11879388 0.12534714 0.14020189 0.1211578  0.1190392  0.12698453
 0.13302839 0.10233316 0.12459596 0.11445324 0.11827011 0.11714886
 0.11863014 0.12116582 0.11201897 0.11765255 0.11255438 0.14042654
 0.10774276 0.13361818 0.12334896 0.12413025 0.1212008  0.13964164
 0.12499285 0.13871581 0.12169222 0.12671399 0.11460638 0.12439234
 0.13393498 0.12966569 0.11828682 0.11668809 0.12718852 0.10301279
 0.13114181 0.12471375 0.12080166 0.11486252 0.12062823 0.11774018
 0.12291686 0.12543063 0.14351075 0.12003006 0.11904094 0.13045828
 0.13016614 0.13979494 0.13472177 0.12541266 0.12013208 0.12767622
 0.1250872  0.12615243 0.12201242 0.11948753 0.14762588 0.1334817
 0.12284424 0.127529   0.13541423 0.14669425 0.13611143 0.12754985
 0.11694551 0.11857257 0.1151144  0.12733989 0.10898906 0.11923275
 0.12801099 0.11243524 0.13411302 0.12823094 0.13112477 0.11498298
 0.14173664 0.11684916 0.1196531  0.12058641 0.11671699 0.10917926
 0.11637908 0.12430955 0.13442727 0.14341839 0.12520952 0.11761626
 0.12778358 0.12149893 0.11856729 0.13980828 0.12986393 0.11929092
 0.12111972 0.13037313 0.12110619 0.13621363 0.12942481 0.11201542
 0.14062934 0.1332858  0.11658686 0.12739747 0.12138589 0.11545236
 0.11126466 0.11789974 0.13424396 0.13413487 0.13462385 0.12191211
 0.12147492 0.1469468  0.13219716 0.11563058 0.12062977 0.14462799
 0.12145547 0.12450246]
partlycloudy
METEOR
[0.08821621 0.10442105 0.11166176 0.09265865 0.10774281 0.09620805
 0.09804385 0.09947437 0.10955546 0.10121253 0.09671886 0.11120451
 0.10312514 0.10435678 0.10897308 0.09893883 0.09276151 0.0971486
 0.09200516 0.10245416 0.09053621 0.10083186 0.11192294 0.11089689
 0.10496524 0.11289891 0.10214038 0.09444764 0.10284261 0.09526186
 0.09287278 0.1057247  0.10922933 0.09933811 0.10117718 0.09503048
 0.09693218 0.09589159 0.09088485 0.10761827 0.1097964  0.1018806
 0.10270192 0.10343214 0.09968319 0.09773534 0.1000863  0.10010193
 0.1047862  0.10554288 0.10678478 0.1008407  0.09781773 0.10646038
 0.11467915 0.09705957 0.09768723 0.10278088 0.09949128 0.10197864
 0.08998331 0.11206655 0.09697603 0.10954409 0.10146782 0.09477929
 0.08785693 0.10111389 0.0927973  0.1108079  0.09913008 0.10457741
 0.08676276 0.10867262 0.10531032 0.09513166 0.1140357  0.10478092
 0.10222593 0.10611337 0.10936634 0.10046034 0.09761084 0.11088657
 0.10787508 0.08644669 0.09936072 0.09170511 0.10171346 0.09637923
 0.09449109 0.10053225 0.09720955 0.10274003 0.09592242 0.10897123
 0.0980006  0.10654174 0.10250175 0.1038365  0.10236448 0.11247271
 0.10090517 0.11417182 0.09794081 0.10300449 0.0930679  0.0989811
 0.10667794 0.10124277 0.09890826 0.09865382 0.10327532 0.09414187
 0.10203021 0.10170777 0.09899286 0.09584375 0.09984132 0.09209858
 0.09981353 0.10308413 0.1143479  0.0972725  0.0967956  0.1109951
 0.10351751 0.1120708  0.11015984 0.09638879 0.09956518 0.10315849
 0.10184341 0.10484035 0.09754793 0.09652384 0.11960415 0.10317824
 0.09983061 0.10837677 0.10815723 0.11435528 0.10973295 0.10105555
 0.10473812 0.0972783  0.0963404  0.09858494 0.09288639 0.09415561
 0.10429258 0.0979216  0.11076791 0.10309388 0.10148288 0.09922825
 0.12104487 0.09747864 0.09659436 0.09929371 0.09636854 0.09923711
 0.09780985 0.10555136 0.1058596  0.11626661 0.09877839 0.09974115
 0.10589704 0.09247206 0.09766699 0.11017174 0.09762815 0.09369829
 0.10882234 0.10084198 0.10186795 0.10754985 0.1072477  0.09644298
 0.10499831 0.10426397 0.09325008 0.10947695 0.09685172 0.09641185
 0.09652101 0.10075553 0.10479143 0.11303876 0.11166179 0.09710018
 0.09947495 0.10966209 0.11223616 0.10550851 0.09538825 0.11408895
 0.10655824 0.10052906]
partlycloudy
BERT
[0.78083205 0.78159916 0.78311765 0.7774044  0.7798412  0.7773014
 0.7784127  0.77855337 0.7804477  0.7780545  0.77990663 0.7784401
 0.78038234 0.78035754 0.78201544 0.7776672  0.78014725 0.7772192
 0.7761054  0.78110164 0.77917194 0.77985114 0.7828577  0.7803959
 0.77918804 0.7816275  0.7806014  0.7784426  0.7804448  0.7770532
 0.78039664 0.78252065 0.7799311  0.78048265 0.77968645 0.7787168
 0.7807161  0.7814391  0.7773617  0.7798097  0.78103304 0.77932405
 0.7797783  0.7810719  0.78163826 0.77798355 0.7809885  0.78001875
 0.7784218  0.7798327  0.78119284 0.78060377 0.7787898  0.77982867
 0.7816879  0.77998763 0.7797061  0.77927065 0.78011227 0.77755445
 0.78031534 0.780087   0.77744544 0.7813146  0.779619   0.7763458
 0.77827525 0.7797495  0.77648306 0.77979165 0.7806864  0.78016806
 0.7781362  0.78120023 0.7824973  0.78154695 0.78133994 0.7801401
 0.77955085 0.7775967  0.78093684 0.7782847  0.77926105 0.7778041
 0.7814851  0.77775794 0.78130347 0.7774667  0.7785224  0.77811456
 0.7783006  0.7815211  0.7796519  0.7788246  0.7802441  0.7801545
 0.77744955 0.7814992  0.77771586 0.7816226  0.7798028  0.7824169
 0.78239596 0.781084   0.779322   0.7786497  0.782502   0.77958715
 0.780829   0.77909863 0.7799752  0.77927136 0.7796277  0.78041
 0.7803285  0.77949953 0.7797556  0.7764712  0.7810133  0.77562946
 0.7799472  0.77857465 0.7797957  0.77794963 0.7792172  0.78285176
 0.7782332  0.78092533 0.78189147 0.7785162  0.7806031  0.77714604
 0.7801543  0.77823746 0.7818051  0.7792731  0.783247   0.7785604
 0.7788603  0.7816673  0.78026766 0.7805778  0.7820545  0.78046703
 0.78140026 0.77885294 0.7822822  0.77995306 0.77948266 0.7810533
 0.7759932  0.77989525 0.7831088  0.7805249  0.77744734 0.7794099
 0.77830964 0.7788348  0.77868235 0.780125   0.7781198  0.77793646
 0.7760249  0.7811896  0.77769774 0.7784642  0.7790394  0.7800113
 0.7835735  0.7777062  0.77720046 0.78047496 0.7789321  0.7784699
 0.7782284  0.78079224 0.7805682  0.7807825  0.7783688  0.77751297
 0.78256    0.7767915  0.77758986 0.7819592  0.7794998  0.7795047
 0.7792861  0.7789668  0.7786836  0.7772489  0.7793833  0.7775681
 0.7797811  0.78057903 0.779318   0.77729523 0.78160155 0.7819694
 0.7769247  0.7791781 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
laluna
WER
[-0.05312534 -0.06330861 -0.0576507  -0.05867518 -0.05919432 -0.05211443
 -0.05866724 -0.05440003 -0.06076979 -0.05901148 -0.05091874 -0.06980367
 -0.06608884 -0.05654168 -0.0610004  -0.04415497 -0.05958474 -0.05918722
 -0.0583534  -0.06054948 -0.05782806 -0.06097334 -0.06158242 -0.06898157
 -0.06221646 -0.05432973 -0.0649175  -0.06005428 -0.06050674 -0.0634819
 -0.05950924 -0.05836831 -0.06184879 -0.05402463 -0.06648564 -0.05755088
 -0.05920245 -0.05326741 -0.06409523 -0.05352748 -0.06180036 -0.06482156
 -0.05068776 -0.05355666 -0.05434248 -0.06155285 -0.04969756 -0.05525687
 -0.05814775 -0.05416813 -0.05891948 -0.05474734 -0.06541054 -0.05930887
 -0.06010225 -0.06514797 -0.05753723 -0.06055196 -0.06186584 -0.05597567
 -0.05522352 -0.06266466 -0.06290205 -0.05577341 -0.05912988 -0.06635172
 -0.06510561 -0.06347078 -0.06107237 -0.05965396 -0.05541949 -0.0691989
 -0.06140096 -0.04195808 -0.06075166 -0.05570206 -0.05867077 -0.05879536
 -0.05479935 -0.07032037 -0.0648993  -0.05581765 -0.054046   -0.05247105
 -0.06307689 -0.06534901 -0.05449324 -0.06556736 -0.04894684 -0.05075994
 -0.06080859 -0.05675703 -0.05630381 -0.05941866 -0.06292886 -0.06547192
 -0.05549076 -0.05981279 -0.05475284 -0.05509834 -0.0612262  -0.05546477
 -0.06215374 -0.06106085 -0.06703554 -0.05835482 -0.05400305 -0.06533545
 -0.05886868 -0.06279488 -0.05773815 -0.06723731 -0.06175897 -0.05799225
 -0.06308853 -0.0592861  -0.0589225  -0.06273025 -0.05520505 -0.06336381
 -0.0596964  -0.06580527 -0.06159255 -0.06114576 -0.0598919  -0.06569893
 -0.06174247 -0.05773047 -0.06199779 -0.05322278 -0.05930821 -0.05567191
 -0.06739601 -0.06731026 -0.05518506 -0.05858032 -0.0639827  -0.06229257
 -0.05747165 -0.06256155 -0.06416092 -0.05711557 -0.06148776 -0.05892143
 -0.06600644 -0.0610729  -0.06512303 -0.06264207 -0.05316437 -0.05834569
 -0.06049895 -0.0606214  -0.06176128 -0.05736858 -0.06261942 -0.05526524
 -0.06568917 -0.05827127 -0.06000013 -0.06033214 -0.05764999 -0.05958689
 -0.06102687 -0.05728804 -0.05494671 -0.06529821 -0.05916396 -0.05935796
 -0.04513163 -0.04990634 -0.06261574 -0.06021364 -0.06050262 -0.06164252
 -0.06748271 -0.05968655 -0.05235182 -0.06087956 -0.05252161 -0.06432077
 -0.06312926 -0.05182649 -0.05825809 -0.05514181 -0.0677949  -0.06761464
 -0.06843928 -0.06783108 -0.05397572 -0.05770056 -0.06180409 -0.05774398
 -0.06975313 -0.05958494 -0.06049089 -0.05785772 -0.05748498 -0.05422929
 -0.06273154 -0.05965402]
laluna
BLEU
[0.13195054 0.12933482 0.11986264 0.1308372  0.12163876 0.14031356
 0.13353174 0.12877112 0.10977407 0.12194241 0.14611716 0.11412269
 0.11845312 0.12957147 0.11676599 0.15932277 0.12058472 0.12219859
 0.11903949 0.11653072 0.12546759 0.13547074 0.12505968 0.11227474
 0.10874026 0.13061857 0.11402108 0.11109296 0.12524793 0.11526292
 0.10830804 0.12573445 0.12704856 0.14012541 0.11916251 0.12772129
 0.11948466 0.13027301 0.11016199 0.13564663 0.11531095 0.11071425
 0.14296881 0.14633991 0.13791198 0.11920619 0.13425328 0.13026266
 0.13481653 0.12977634 0.13131593 0.13549327 0.12200834 0.12861885
 0.12986657 0.11915515 0.12314098 0.12545583 0.11593042 0.129449
 0.13950193 0.11818209 0.12102346 0.13776895 0.1317101  0.11689316
 0.1158918  0.1102201  0.1236618  0.12574598 0.13354998 0.10858236
 0.12069944 0.15197368 0.12553122 0.12217122 0.12380178 0.12260737
 0.14004251 0.11651352 0.12579988 0.12730321 0.12477853 0.14265423
 0.1173468  0.12225715 0.1235095  0.11860855 0.13663756 0.1353361
 0.12773492 0.12919302 0.12156243 0.1289676  0.11370266 0.10899415
 0.12895533 0.1313478  0.13924549 0.13238537 0.12591397 0.13586717
 0.12457674 0.11965804 0.11791172 0.12891459 0.14689011 0.12185087
 0.12924668 0.13057997 0.13369862 0.09956771 0.12577676 0.12639847
 0.13846284 0.12082188 0.12373002 0.11968069 0.12016226 0.12584067
 0.126606   0.11277357 0.11794083 0.12114916 0.12117802 0.11355509
 0.11851257 0.11803342 0.11697983 0.12901847 0.11288919 0.12829061
 0.11867391 0.1120772  0.1195731  0.123737   0.10902966 0.12739168
 0.13206631 0.11714975 0.11690128 0.12773265 0.12974273 0.13177954
 0.10812765 0.11468279 0.12563123 0.12402295 0.1373011  0.12153943
 0.12104257 0.11403385 0.11781806 0.13425727 0.13002521 0.13381191
 0.10674281 0.12957307 0.12165196 0.11776833 0.12256825 0.11815194
 0.12241254 0.12982141 0.13761944 0.12009601 0.13155877 0.12468491
 0.14743499 0.14596063 0.1191149  0.13013968 0.11086755 0.12084142
 0.10938306 0.12917291 0.13639395 0.1281843  0.14467948 0.11742373
 0.11423668 0.14250069 0.11335319 0.13317762 0.12108194 0.10766983
 0.11331033 0.10738357 0.13658348 0.12629092 0.10892826 0.12945401
 0.11160878 0.12611052 0.11910639 0.13102177 0.13625128 0.13841655
 0.12839265 0.1191592 ]
laluna
METEOR
[0.09676403 0.09454652 0.0878035  0.09825948 0.08964911 0.10598142
 0.09426094 0.09970565 0.08339984 0.09031931 0.10054061 0.08777905
 0.09527535 0.09346822 0.08930507 0.11133661 0.0910333  0.09198847
 0.09158314 0.08970652 0.09356182 0.10000881 0.09340232 0.08332404
 0.08349049 0.09628918 0.08832246 0.08541276 0.09885777 0.08852269
 0.08353449 0.09261908 0.09776867 0.09516384 0.09113081 0.09176609
 0.0920479  0.09459701 0.0878895  0.10234744 0.08989171 0.0830223
 0.10597402 0.10758299 0.09592658 0.0887824  0.09678485 0.09627659
 0.10291226 0.09277162 0.09365248 0.10250193 0.09062139 0.09475843
 0.09489442 0.09221837 0.09334111 0.09567443 0.09035479 0.09466208
 0.0976385  0.08903227 0.08823897 0.10201032 0.09824576 0.09102651
 0.08935671 0.08661548 0.09488757 0.09250299 0.10495242 0.08661215
 0.09307269 0.1099145  0.0926776  0.08976839 0.09537949 0.09231971
 0.10249733 0.08262572 0.09519159 0.0929455  0.09286238 0.10096361
 0.08965627 0.09035799 0.09092649 0.09285358 0.10236545 0.09708068
 0.09705746 0.09144966 0.09053952 0.09401631 0.09118289 0.08712871
 0.09815094 0.09625187 0.10324002 0.09927725 0.08945038 0.09766192
 0.0908033  0.08768286 0.08985087 0.09576291 0.10432517 0.09624926
 0.10236768 0.09422003 0.09744849 0.0743078  0.09466761 0.1019519
 0.09874778 0.08519384 0.09307778 0.08869222 0.09039063 0.09234955
 0.09310325 0.08705933 0.08751078 0.09018426 0.09101674 0.08301602
 0.09021128 0.09024875 0.08577291 0.10039066 0.08706349 0.0981274
 0.08862714 0.08989191 0.09031672 0.09406331 0.09084227 0.08973778
 0.09703686 0.09204583 0.08837041 0.09948401 0.09229631 0.09467403
 0.0817016  0.09548967 0.09771249 0.09401431 0.10130351 0.08886019
 0.08780029 0.09188286 0.09030142 0.09067229 0.09495469 0.09749404
 0.08221505 0.09590855 0.09544174 0.08491876 0.09311581 0.09344156
 0.09241997 0.09937169 0.09676458 0.09068904 0.09616077 0.09008883
 0.10804512 0.1007124  0.08759013 0.09889735 0.08139878 0.09034639
 0.08356154 0.09232954 0.09988802 0.0961096  0.10588065 0.08875585
 0.09071037 0.10064052 0.08777054 0.09774163 0.08666647 0.08641968
 0.08719283 0.08269625 0.09539598 0.0920182  0.08733858 0.09208133
 0.08449847 0.09269937 0.09200971 0.09631706 0.10385591 0.09984478
 0.09642129 0.0891846 ]
laluna
BERT
[0.7854284  0.7813339  0.7841783  0.78545016 0.7824318  0.7874234
 0.784038   0.7835581  0.7835082  0.78191346 0.78720534 0.7825434
 0.78191686 0.7826395  0.78483856 0.7872286  0.7840355  0.78224665
 0.7840037  0.7826901  0.78504854 0.7854651  0.7817054  0.78283286
 0.78228354 0.785121   0.78173107 0.78251857 0.78455853 0.78284127
 0.78347003 0.78111595 0.7853567  0.78339654 0.78222483 0.78266895
 0.78235    0.78279763 0.7814161  0.7838966  0.7817781  0.7814013
 0.78452075 0.7862578  0.7840683  0.78516954 0.7851171  0.7854036
 0.78633076 0.78204256 0.7849462  0.78568584 0.78463495 0.7838504
 0.78132045 0.7818     0.7843908  0.7850359  0.78351605 0.7834594
 0.7839691  0.7852999  0.78304964 0.78619975 0.7849763  0.78327817
 0.7828801  0.7834535  0.7845187  0.7846602  0.78111273 0.7803905
 0.78216755 0.7887088  0.78360385 0.784887   0.7834714  0.7853429
 0.7846627  0.7844026  0.7859799  0.7849926  0.7824203  0.7864039
 0.78260326 0.78439283 0.78307146 0.7824978  0.78419316 0.7841537
 0.78128964 0.78470993 0.7840129  0.7853877  0.7804344  0.78128123
 0.7866034  0.78484225 0.78410673 0.7827082  0.7813077  0.78594536
 0.785263   0.7845386  0.7826027  0.784906   0.78434306 0.7837585
 0.7876755  0.7867417  0.7857543  0.7810379  0.7844667  0.7847126
 0.78605783 0.7825321  0.783337   0.7853238  0.7812955  0.78546435
 0.7842319  0.7854248  0.7823685  0.7822586  0.7845889  0.7844585
 0.782365   0.7830417  0.78349686 0.78371674 0.78346545 0.78356105
 0.78437597 0.7806346  0.7850587  0.7873324  0.7821718  0.7846913
 0.78255093 0.78550607 0.78395236 0.78575164 0.7875156  0.7858588
 0.78039217 0.7817398  0.78345203 0.78273505 0.7829619  0.78310615
 0.78169817 0.7812845  0.7843112  0.7863033  0.7838177  0.78491527
 0.783676   0.78252673 0.78444266 0.7861896  0.7840801  0.7809617
 0.7832186  0.78604263 0.7853104  0.78365207 0.7818464  0.78301936
 0.7863935  0.7842158  0.78409654 0.78627276 0.78490484 0.7843588
 0.78344434 0.7845999  0.78634083 0.7829949  0.785828   0.78138727
 0.7865999  0.78554744 0.78407127 0.78767914 0.7824709  0.781437
 0.78288376 0.78081447 0.7889082  0.7868905  0.7816053  0.78537714
 0.7799939  0.7836746  0.78259164 0.7865865  0.78284216 0.78446424
 0.782044   0.7835781 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
attend-M
WER
[0.05960655 0.04910566 0.0502796  0.05164685 0.052317   0.05838729
 0.05461575 0.05815953 0.05501785 0.06079585 0.04284044 0.04503423
 0.05821798 0.05002913 0.05121599 0.05241863 0.04896886 0.04866899
 0.0514471  0.047084   0.05362855 0.05118744 0.04701073 0.04861762
 0.05164141 0.05483655 0.05352592 0.04875376 0.0537876  0.04878019
 0.05410175 0.05060317 0.05610728 0.05517178 0.04970869 0.05640244
 0.05236685 0.0500032  0.05111462 0.04963407 0.04645133 0.06068435
 0.05324979 0.04795077 0.05183498 0.05477715 0.04959281 0.05114041
 0.04110472 0.05317663 0.05666438 0.05979389 0.04587962 0.05566319
 0.04390389 0.0504464  0.04925641 0.05517759 0.05294964 0.04382297
 0.04682224 0.04917149 0.06055574 0.05440532 0.04695449 0.05329612
 0.05293895 0.04950987 0.06032241 0.05902693 0.05262609 0.05654999
 0.05442744 0.04504049 0.05104514 0.06113187 0.04561615 0.04926847
 0.05320164 0.05142006 0.04351057 0.05607023 0.05009261 0.05804998
 0.06126408 0.04825319 0.04707243 0.05320301 0.05183989 0.05809225
 0.05239389 0.05355141 0.06151324 0.05424535 0.05216182 0.05131965
 0.04827822 0.051758   0.05548939 0.04640132 0.05156993 0.04654555
 0.05544313 0.05752052 0.04962783 0.05624249 0.05562993 0.05374344
 0.05566253 0.06097759 0.04621589 0.05036513 0.05612526 0.05645042
 0.05267063 0.05104416 0.05017563 0.05567917 0.04395836 0.05899422
 0.05662579 0.05384491 0.05307297 0.0556179  0.05075828 0.05138183
 0.04851044 0.05520045 0.05455559 0.05845935 0.05132321 0.05126053
 0.05082208 0.04724122 0.04665888 0.05471539 0.05186324 0.05079715
 0.0442966  0.05267167 0.05389136 0.04532534 0.06423661 0.05764735
 0.04425554 0.05157664 0.05744019 0.05571861 0.04823996 0.0511222
 0.05248572 0.04751842 0.05393965 0.05054298 0.05336744 0.05142087
 0.05241359 0.05028882 0.06277447 0.05065935 0.04863818 0.04460128
 0.0498882  0.04778625 0.05399141 0.05312824 0.0602763  0.05423094
 0.05447902 0.04785478 0.05393104 0.04748185 0.04971144 0.04619092
 0.0498     0.04744132 0.05544587 0.05534288 0.04728513 0.04598565
 0.05188703 0.05778385 0.04942448 0.06402148 0.06006918 0.05547012
 0.05898221 0.05331309 0.04848332 0.05130291 0.0545946  0.05271409
 0.05253171 0.05940305 0.0467318  0.05515245 0.05362459 0.0483883
 0.04566963 0.04546205]
attend-M
BLEU
[0.1951924  0.17284893 0.18586861 0.19452306 0.18432727 0.20307919
 0.1954484  0.18539067 0.19811813 0.19680797 0.17771037 0.18363817
 0.19435686 0.19441254 0.18017215 0.19084196 0.18244035 0.16762032
 0.18979539 0.17709942 0.19968396 0.19066645 0.17514798 0.17759925
 0.19883968 0.19431568 0.18989557 0.19506174 0.18176317 0.18317929
 0.20468687 0.18499945 0.19685894 0.18189567 0.17869286 0.19337757
 0.18147891 0.17017181 0.19053763 0.18065631 0.19350799 0.20295509
 0.20014594 0.1702124  0.18942082 0.18997024 0.18568158 0.18697771
 0.17210997 0.19676523 0.19780971 0.20294515 0.17671759 0.19732318
 0.17632385 0.18945025 0.18314227 0.18299216 0.1988111  0.17935201
 0.17220023 0.18686816 0.20194598 0.20192161 0.1695134  0.19500221
 0.19045424 0.17199979 0.20819485 0.19845982 0.19499424 0.19095732
 0.20379883 0.17872887 0.19732577 0.18635129 0.17033917 0.17907522
 0.19197907 0.19289712 0.17110041 0.19472678 0.18879811 0.19945591
 0.1943875  0.17236736 0.1886751  0.20021971 0.20085881 0.19046213
 0.19230703 0.19435863 0.19273549 0.17694389 0.19938186 0.18520835
 0.18832194 0.19456074 0.17874864 0.18530086 0.18964767 0.17627287
 0.20289643 0.18657303 0.18885929 0.18159597 0.1903087  0.19313144
 0.19946624 0.20527099 0.18918739 0.18384168 0.1932253  0.20148327
 0.1730623  0.17990855 0.18898142 0.19434125 0.17901385 0.19855854
 0.18931479 0.18932283 0.19848519 0.19452852 0.19027017 0.19496156
 0.19525969 0.21397745 0.17592953 0.19624912 0.18587984 0.19552589
 0.17990449 0.18031775 0.17798211 0.18270463 0.18540258 0.18776642
 0.16671917 0.1996753  0.20239303 0.17787473 0.20619146 0.2032018
 0.17350801 0.18077968 0.20026411 0.19698906 0.18676366 0.18272202
 0.19096022 0.17744035 0.19725886 0.19071747 0.19716595 0.17818101
 0.18323362 0.18542913 0.19887745 0.185236   0.1758486  0.18375562
 0.17876898 0.17174135 0.18959837 0.17884881 0.19823915 0.19296269
 0.19698026 0.19002625 0.17470753 0.19205583 0.18413498 0.17554107
 0.20241347 0.17522863 0.18661229 0.19055576 0.19173489 0.17893447
 0.18783197 0.19322167 0.19511467 0.20859582 0.19565616 0.20232676
 0.18960517 0.18967147 0.19395312 0.18886299 0.20052794 0.19274393
 0.19537594 0.18152454 0.19445505 0.19035112 0.19028198 0.17168126
 0.18511356 0.17358748]
attend-M
METEOR
[0.1414591  0.12523685 0.13488629 0.13318203 0.13154827 0.1427323
 0.13866645 0.13073431 0.13920838 0.13693287 0.12626545 0.13363787
 0.14089805 0.14379162 0.12851536 0.13697157 0.12546994 0.12135734
 0.13500551 0.12210021 0.13847553 0.13501217 0.1253657  0.13284961
 0.13296289 0.14042483 0.13823302 0.14038053 0.13564086 0.13527527
 0.13843685 0.12883034 0.13454393 0.12597444 0.12416942 0.14297232
 0.12835663 0.12598774 0.13594683 0.13297236 0.13682327 0.13882477
 0.13922329 0.12960663 0.13229569 0.13508921 0.12784591 0.13138927
 0.12968863 0.14199168 0.13988859 0.13886941 0.1289508  0.1420929
 0.12925066 0.13413414 0.13107955 0.12988732 0.13760327 0.12692642
 0.12530661 0.13496155 0.13788365 0.14060958 0.12411243 0.13513767
 0.13453149 0.12980816 0.1473518  0.14219665 0.13878023 0.13389276
 0.14061069 0.12765134 0.13444101 0.13062566 0.12001956 0.12956078
 0.13714967 0.14185418 0.12554354 0.14218083 0.13206155 0.13957218
 0.14860698 0.12370594 0.13468329 0.14146366 0.13699046 0.13809974
 0.14042467 0.14460715 0.1381449  0.12983417 0.14089918 0.13242172
 0.13358691 0.13748266 0.132116   0.13117444 0.13913648 0.12909718
 0.14434182 0.13248125 0.13354053 0.12591559 0.13524004 0.14097571
 0.14059659 0.15001622 0.13930175 0.13681196 0.13510267 0.13426039
 0.12813888 0.13129229 0.13071495 0.13375767 0.12670711 0.14136994
 0.13334418 0.13474626 0.13952251 0.14340752 0.13028353 0.13714498
 0.13765982 0.14612903 0.12963731 0.14111981 0.12802838 0.14046818
 0.12791643 0.13306047 0.13409501 0.13037492 0.12847666 0.13707489
 0.11655154 0.14239724 0.14257108 0.13317543 0.14749486 0.14323622
 0.12301631 0.13877759 0.14710864 0.1380348  0.13581139 0.13550034
 0.14006599 0.13094763 0.14068887 0.14019443 0.13465438 0.13439433
 0.13419215 0.1318768  0.14090568 0.13317471 0.12870113 0.12777787
 0.13311696 0.1257434  0.13401725 0.12521291 0.14221214 0.13511942
 0.14405692 0.13557656 0.12668261 0.14088411 0.14136945 0.12465928
 0.13932762 0.12889974 0.13418513 0.14584024 0.13860342 0.12633266
 0.13154421 0.13428189 0.13959586 0.14619207 0.1412649  0.14530434
 0.1328139  0.13635338 0.13902103 0.13359536 0.14165109 0.13855664
 0.13720984 0.13154701 0.13509526 0.13495013 0.13556893 0.12628397
 0.13056643 0.13249597]
attend-M
BERT
[0.7921624  0.78704786 0.7856343  0.7892862  0.7883429  0.79119575
 0.78972507 0.78560627 0.7885957  0.78922004 0.78784317 0.7874343
 0.791239   0.78912055 0.78998077 0.7909388  0.78905386 0.78656715
 0.78714234 0.79008955 0.79016405 0.7873399  0.7872572  0.787433
 0.7883284  0.7894959  0.7876742  0.7902154  0.78837913 0.7882553
 0.7908753  0.7859453  0.7879724  0.7866331  0.7857996  0.78876024
 0.786938   0.7860583  0.78937095 0.79051787 0.78963816 0.7884851
 0.789808   0.7886056  0.7874316  0.7860772  0.7872237  0.7878172
 0.7895195  0.78895867 0.7885645  0.789383   0.7909819  0.7894417
 0.78717136 0.7882555  0.78681165 0.78970784 0.79165894 0.78825057
 0.7867351  0.79052985 0.789069   0.78858757 0.78899896 0.7904762
 0.7875184  0.7875488  0.79244846 0.790168   0.7884204  0.7905487
 0.79054374 0.7864042  0.78979015 0.7901698  0.7875693  0.787178
 0.788236   0.78822184 0.78780216 0.7920348  0.7864697  0.78911644
 0.78872275 0.78507674 0.7887686  0.78823376 0.7879448  0.789935
 0.78980035 0.7913218  0.78977394 0.78739166 0.78947014 0.7918793
 0.7888648  0.7896412  0.78757536 0.7889635  0.7887473  0.7869382
 0.787281   0.7893126  0.79025465 0.78822255 0.7871108  0.78665733
 0.78865063 0.78929764 0.7880421  0.79083526 0.78926945 0.7905004
 0.78567815 0.7881849  0.7896755  0.7883421  0.7874478  0.78854215
 0.7891871  0.78913176 0.78988993 0.7900787  0.7882306  0.7890269
 0.7883659  0.78995275 0.7877742  0.78853923 0.7902038  0.788894
 0.7865435  0.7891495  0.78834933 0.7902484  0.78762156 0.7880348
 0.78572094 0.7907104  0.79077595 0.7883582  0.78846836 0.7893825
 0.7875693  0.78932774 0.78959644 0.78975064 0.7900715  0.7883444
 0.7881874  0.7879756  0.78678596 0.7863677  0.78802985 0.7869044
 0.78958404 0.78848356 0.7871778  0.79099846 0.7874641  0.78798044
 0.7886965  0.78730565 0.78600734 0.7883322  0.78959614 0.7888294
 0.78974247 0.78835607 0.7888863  0.788657   0.7894108  0.7882711
 0.78971153 0.787236   0.7878069  0.7917027  0.78599006 0.78663397
 0.7888648  0.78716534 0.7887708  0.7910783  0.7908852  0.7893761
 0.78898156 0.7876423  0.79071665 0.7868055  0.7927147  0.7903332
 0.78857857 0.7889414  0.7859827  0.78911513 0.787972   0.7859887
 0.78715086 0.79074836]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
attend-F
WER
[-0.20097581 -0.1904139  -0.18236389 -0.17671494 -0.19980597 -0.20409363
 -0.19385955 -0.19503522 -0.19129174 -0.20285736 -0.18387623 -0.18874959
 -0.19613708 -0.19226518 -0.19255184 -0.19222122 -0.18883568 -0.19251059
 -0.19973232 -0.1895235  -0.18396238 -0.18945999 -0.19121721 -0.18988121
 -0.18808137 -0.18944927 -0.19224075 -0.18457131 -0.19033946 -0.2027401
 -0.19650221 -0.1903111  -0.18340747 -0.18384352 -0.19644662 -0.20375267
 -0.20518314 -0.1837883  -0.18234109 -0.18998265 -0.18988108 -0.1917393
 -0.19746064 -0.17365788 -0.19391683 -0.18618368 -0.18494967 -0.19517227
 -0.18473313 -0.19434287 -0.19943569 -0.20252775 -0.20040686 -0.19004744
 -0.19129231 -0.18314643 -0.19962879 -0.18981188 -0.18961485 -0.1895529
 -0.20067771 -0.19408884 -0.19573298 -0.19065566 -0.18815495 -0.19933062
 -0.19630246 -0.18936616 -0.19783442 -0.17926997 -0.18032729 -0.19626719
 -0.1831207  -0.19321243 -0.18744586 -0.19760674 -0.18504335 -0.19071492
 -0.19722772 -0.19904801 -0.18872815 -0.1964234  -0.20111609 -0.19674641
 -0.19712214 -0.19401627 -0.18732185 -0.17405436 -0.19192612 -0.19315233
 -0.18519134 -0.19716642 -0.20152281 -0.19680148 -0.18788907 -0.1958702
 -0.18868855 -0.18603688 -0.19004334 -0.19691873 -0.19358505 -0.19160583
 -0.18335306 -0.19174383 -0.1823832  -0.18203676 -0.19838605 -0.1960709
 -0.18896158 -0.19117153 -0.19548883 -0.19501987 -0.20038091 -0.19803258
 -0.19289061 -0.18588114 -0.20093252 -0.18835188 -0.19287533 -0.20368519
 -0.19025469 -0.19701455 -0.17980396 -0.18979139 -0.19051676 -0.19776951
 -0.18627691 -0.20436081 -0.18680816 -0.19433389 -0.20706807 -0.18828601
 -0.18938407 -0.18303537 -0.1971614  -0.17731501 -0.19247975 -0.18506729
 -0.1995892  -0.19548846 -0.19316382 -0.19790411 -0.19466745 -0.19893223
 -0.1823191  -0.19471833 -0.19244743 -0.19226217 -0.19032522 -0.18600032
 -0.19159311 -0.20223322 -0.19557935 -0.1857412  -0.1952306  -0.19419698
 -0.18446394 -0.18920663 -0.19141646 -0.18028794 -0.19291862 -0.19230515
 -0.19213736 -0.19509187 -0.18489025 -0.19635554 -0.19408525 -0.19178524
 -0.18023801 -0.18580468 -0.19950558 -0.18263264 -0.19500302 -0.18157783
 -0.18723401 -0.18626768 -0.19779911 -0.18262539 -0.19426119 -0.19057932
 -0.17419071 -0.18739842 -0.2017354  -0.19233482 -0.19767151 -0.19184488
 -0.19546971 -0.18978482 -0.19039056 -0.20126395 -0.19505049 -0.18363851
 -0.19096471 -0.20071033 -0.19562883 -0.19365967 -0.18331161 -0.19811325
 -0.19408102 -0.1881424 ]
attend-F
BLEU
[0.1638123  0.17869452 0.18456098 0.20681205 0.15462457 0.17003148
 0.17345966 0.17639736 0.18298408 0.17433066 0.21373363 0.19115718
 0.1784663  0.19288792 0.19234286 0.17589578 0.18779554 0.16881441
 0.18454653 0.20288935 0.19254773 0.17985888 0.18936958 0.18690683
 0.18556412 0.19087288 0.18051146 0.17874321 0.17900589 0.15908616
 0.17946869 0.17670415 0.20648781 0.19387659 0.18584692 0.17476825
 0.16611329 0.18902836 0.20404032 0.19955997 0.1848242  0.18502424
 0.17981076 0.18986787 0.17346017 0.18335774 0.19767305 0.18037899
 0.19348081 0.17797974 0.17488884 0.17969171 0.17201272 0.18998377
 0.18619511 0.19517302 0.17692792 0.19309329 0.20676991 0.17668655
 0.15698376 0.17981898 0.18950373 0.19619476 0.19580481 0.18306271
 0.18066758 0.19809321 0.17364358 0.20314079 0.20586837 0.16320058
 0.19195531 0.17206163 0.1943265  0.18806953 0.18754337 0.19197149
 0.18583445 0.17453869 0.18952297 0.19273494 0.16232747 0.17883892
 0.16401341 0.199274   0.18987059 0.20586874 0.20219415 0.1753945
 0.19772868 0.16387305 0.15800433 0.18144218 0.20756796 0.16521346
 0.19599303 0.19395971 0.1704276  0.18251795 0.1785318  0.18209961
 0.18541237 0.16683916 0.20991621 0.19047668 0.17929017 0.19724754
 0.18392981 0.19507058 0.18380511 0.16342386 0.16878517 0.16448558
 0.18015855 0.20632947 0.17729743 0.17961408 0.18079051 0.1763406
 0.18703624 0.18687264 0.19053785 0.19100199 0.17850623 0.19553719
 0.19361753 0.14780779 0.20883518 0.17311018 0.16856687 0.17829589
 0.1961966  0.1864911  0.1697634  0.20319426 0.19091966 0.19630851
 0.16177895 0.18178189 0.17672758 0.17529909 0.18626394 0.18736155
 0.20624282 0.18094343 0.17966905 0.17700933 0.18736653 0.19563583
 0.19359271 0.1844548  0.1818459  0.18954702 0.17195442 0.19256377
 0.16971998 0.18930179 0.18031421 0.19099736 0.20478335 0.19237576
 0.1788395  0.18113242 0.19394987 0.18165752 0.17963634 0.17451842
 0.1877592  0.19000592 0.1637631  0.19206136 0.18695727 0.19895671
 0.18901291 0.18670719 0.18315443 0.18712366 0.18087218 0.18867023
 0.20791419 0.19573734 0.16165165 0.18602128 0.18533476 0.18782873
 0.17972383 0.19972387 0.18177729 0.15622131 0.19154207 0.19642695
 0.18738098 0.18938544 0.17429348 0.19293682 0.17846323 0.171483
 0.18470256 0.17363553]
attend-F
METEOR
[0.12326884 0.1346711  0.13498656 0.15224285 0.12264992 0.12689452
 0.13540651 0.13331002 0.1377422  0.12990416 0.15639049 0.13851686
 0.13132429 0.1433065  0.13826663 0.13205698 0.14132505 0.12252168
 0.13654545 0.14801831 0.14433808 0.13968514 0.13931717 0.13866009
 0.14220001 0.14250673 0.13358659 0.13743684 0.13502364 0.11956097
 0.13336933 0.13347592 0.15420973 0.14295618 0.13817663 0.13007382
 0.12628865 0.14279136 0.14457539 0.14872714 0.13670198 0.13830951
 0.13234215 0.14258201 0.12958082 0.14462887 0.1468342  0.13899393
 0.14192667 0.13555272 0.12766189 0.13374075 0.13160584 0.14676339
 0.14274795 0.1410107  0.13750998 0.14291156 0.15086593 0.12663235
 0.11980819 0.12807726 0.1402402  0.147312   0.15147053 0.13481612
 0.13321029 0.14784486 0.13269173 0.14707385 0.15060373 0.1228071
 0.13829279 0.12943879 0.14277818 0.13815213 0.13921351 0.14673105
 0.13655758 0.12802479 0.13977796 0.14687439 0.12584646 0.13382985
 0.12493701 0.14487041 0.13541622 0.15671273 0.14296627 0.12843559
 0.14737541 0.13145593 0.11812676 0.13614352 0.15391436 0.12473949
 0.1428074  0.14710754 0.13233423 0.13879645 0.13043078 0.13592887
 0.13944829 0.12982887 0.14745957 0.13969167 0.13486519 0.14751544
 0.1356488  0.14564138 0.13196894 0.12499835 0.13172917 0.1272469
 0.13659943 0.14930072 0.12865052 0.13436392 0.14004136 0.14079634
 0.14419612 0.1390812  0.14032301 0.14256434 0.13330256 0.13972803
 0.14478365 0.11415993 0.15521714 0.13404557 0.13221597 0.1379486
 0.14180949 0.13531948 0.13075847 0.15223066 0.14182707 0.14008527
 0.12420772 0.13338375 0.13454948 0.13346626 0.13471261 0.14046178
 0.14862413 0.13635034 0.1338398  0.13629333 0.14390045 0.14110894
 0.1399391  0.13212763 0.13525877 0.14375822 0.13098226 0.14577154
 0.13014257 0.13293284 0.13107244 0.14367073 0.14663533 0.14527091
 0.13901595 0.13658225 0.14302635 0.14257029 0.1327746  0.13290885
 0.13615844 0.14226094 0.12096642 0.14782809 0.13301114 0.14618234
 0.14062067 0.13844562 0.13932141 0.14045659 0.13734177 0.13166997
 0.15735676 0.14445946 0.12606098 0.13619152 0.13781817 0.14022587
 0.13914204 0.14909777 0.13867959 0.11835923 0.13751821 0.14474827
 0.13564487 0.13872614 0.13118039 0.14063143 0.13566697 0.12831808
 0.14170021 0.13166535]
attend-F
BERT
[0.7879492  0.7872699  0.78783005 0.79079163 0.78748983 0.78810126
 0.786486   0.78556395 0.7890952  0.7887958  0.7916812  0.7904916
 0.7878957  0.79135346 0.78912354 0.7876215  0.79080683 0.7885124
 0.78720224 0.78974485 0.7895841  0.7885339  0.7882128  0.78812486
 0.7880375  0.7882141  0.7893392  0.79078645 0.7905696  0.7865073
 0.78823245 0.7886346  0.7885952  0.79021895 0.7888867  0.78893924
 0.7868996  0.7896774  0.7925998  0.7912191  0.78780514 0.7897149
 0.7890028  0.7891764  0.786668   0.7882813  0.7901768  0.7893038
 0.7894298  0.7892701  0.78678054 0.7896792  0.7871351  0.7899917
 0.7900631  0.7905218  0.7901829  0.7892036  0.7915569  0.78828716
 0.7859584  0.7881521  0.7904287  0.78986746 0.7888654  0.78698725
 0.7881221  0.7909943  0.7891368  0.7914704  0.79152554 0.78848314
 0.7911495  0.78874034 0.7895574  0.78940475 0.790406   0.78803205
 0.79065555 0.7863285  0.789519   0.79001784 0.78896356 0.7872677
 0.7888296  0.7895066  0.79097164 0.79051405 0.7897624  0.78804004
 0.7900366  0.78622717 0.78621197 0.7889269  0.79158545 0.78715587
 0.78865165 0.7903994  0.78885734 0.78917843 0.7889889  0.78795373
 0.78765947 0.78601235 0.791123   0.7892194  0.7875792  0.7912511
 0.7863956  0.79178643 0.78895915 0.7870607  0.78694266 0.7851575
 0.78904873 0.7909117  0.7887383  0.7889892  0.7889692  0.78829426
 0.78980464 0.78815323 0.78901017 0.78654414 0.78891724 0.7884948
 0.78778297 0.7878444  0.7917397  0.7876146  0.78631186 0.7868971
 0.7904882  0.7875762  0.7857127  0.7908473  0.7891902  0.7891245
 0.7871043  0.7886638  0.7870407  0.78818244 0.78876984 0.78942055
 0.7903554  0.7901323  0.78868747 0.7899107  0.7902172  0.79024494
 0.7894036  0.7891171  0.7885337  0.7885687  0.7877775  0.78970426
 0.7867653  0.7891253  0.78955495 0.78870547 0.78828317 0.7911249
 0.7874142  0.7888464  0.7910518  0.7887863  0.7881393  0.7871056
 0.787655   0.7898197  0.7880845  0.78922915 0.78846693 0.78809446
 0.7899926  0.7882966  0.7889049  0.791151   0.7877712  0.7902704
 0.78929937 0.7920332  0.7874325  0.7890004  0.78812927 0.7894283
 0.78891385 0.78895307 0.7884634  0.7854355  0.7899804  0.7886663
 0.78634214 0.7872897  0.78605396 0.7877119  0.79121774 0.7891004
 0.79100055 0.78685427]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
alpha_repeat-1
WER
[-0.18400274 -0.19322178 -0.20726742 -0.21623254 -0.1681034  -0.20227896
 -0.2091135  -0.17724223 -0.19723317 -0.20798694 -0.18762877 -0.20363127
 -0.16785719 -0.1876682  -0.18212965 -0.16467548 -0.19950911 -0.1626249
 -0.20034921 -0.20263226 -0.21975605 -0.16838737 -0.17803954 -0.18113311
 -0.18543261 -0.19176951 -0.19672605 -0.19983418 -0.19582121 -0.17547845
 -0.17950705 -0.16766374 -0.19766195 -0.16657794 -0.20912267 -0.17325904
 -0.17662129 -0.1693678  -0.15338405 -0.1680026  -0.17603076 -0.1486641
 -0.18243407 -0.18366587 -0.14888514 -0.19994611 -0.15956449 -0.18574403
 -0.16951895 -0.22105014 -0.1994351  -0.19358943 -0.19706873 -0.19916332
 -0.21674334 -0.16632026 -0.16959024 -0.19892135 -0.19915632 -0.20425851
 -0.19801657 -0.17935643 -0.18151866 -0.19103899 -0.23007155 -0.16590481
 -0.1757635  -0.19668898 -0.17169676 -0.14774363 -0.21395414 -0.17508835
 -0.20244114 -0.20358343 -0.16943194 -0.18916378 -0.19791053 -0.17902143
 -0.21811394 -0.17827335 -0.2110774  -0.18945586 -0.20769358 -0.21715089
 -0.18281901 -0.21049182 -0.20774134 -0.17776564 -0.20050694 -0.21067284
 -0.19334368 -0.1554951  -0.19827411 -0.17421091 -0.19483965 -0.19301517
 -0.1789228  -0.17773758 -0.16735155 -0.18531622 -0.16852628 -0.1659206
 -0.21581971 -0.17430655 -0.19469371 -0.20063955 -0.21541557 -0.17371443
 -0.19548528 -0.17990573 -0.1961407  -0.18733651 -0.17876119 -0.18971977
 -0.18734331 -0.21913731 -0.18972611 -0.20080731 -0.17321013 -0.19901286
 -0.182676   -0.16069692 -0.16956504 -0.19760667 -0.19136148 -0.22259097
 -0.15847411 -0.15999987 -0.19680764 -0.17352618 -0.17075313 -0.18796498
 -0.19038251 -0.21991698 -0.15746757 -0.22555653 -0.20511003 -0.15467896
 -0.18703692 -0.18839997 -0.19858773 -0.17599757 -0.18997405 -0.21072272
 -0.16535302 -0.19317079 -0.15346896 -0.20875463 -0.1981516  -0.18437078
 -0.16530852 -0.17135236 -0.18505452 -0.18502204 -0.15705464 -0.18786151
 -0.18813326 -0.16303964 -0.18849396 -0.18776123 -0.15735188 -0.17466337
 -0.2012379  -0.16366713 -0.20931934 -0.1812374  -0.17546617 -0.17914424
 -0.20890729 -0.17496802 -0.17945182 -0.17342843 -0.20410598 -0.202558
 -0.20231353 -0.18119638 -0.21100615 -0.16998575 -0.17482157 -0.20346622
 -0.18188665 -0.18607851 -0.18619931 -0.18927479 -0.16157959 -0.22198821
 -0.16849875 -0.19581147 -0.16720018 -0.19231432 -0.17388599 -0.20217307
 -0.20644065 -0.17302774 -0.19482476 -0.19682474 -0.17263135 -0.16925935
 -0.18165394 -0.20656362]
alpha_repeat-1
BLEU
[0.1795012  0.17307695 0.16849512 0.17693707 0.15457693 0.19209007
 0.16365763 0.19801169 0.18605218 0.14818454 0.21758731 0.20292741
 0.21656561 0.19819846 0.21525776 0.23290691 0.21799857 0.20191687
 0.20751666 0.14095498 0.14222775 0.2127924  0.18034966 0.19065548
 0.18832722 0.18369756 0.17095586 0.17533458 0.20024021 0.16493472
 0.19927757 0.20240906 0.20543371 0.19308899 0.19844366 0.21160113
 0.19136403 0.2205914  0.22133912 0.23282403 0.21905989 0.19680018
 0.17684991 0.17710694 0.23393047 0.15871283 0.20375632 0.17472114
 0.19557433 0.16682191 0.15731625 0.11050472 0.21644395 0.20746625
 0.10069214 0.20763577 0.15671167 0.18851284 0.15310356 0.14212613
 0.18847812 0.17189053 0.19701083 0.19644079 0.14251337 0.198573
 0.21838248 0.16189956 0.16569507 0.20794091 0.15760952 0.2156888
 0.15973119 0.17822456 0.17103612 0.21012063 0.15623388 0.16705839
 0.13807955 0.1868702  0.13354741 0.1657971  0.1142609  0.13260039
 0.18631304 0.14539628 0.15163125 0.15153872 0.18265281 0.13786035
 0.15299015 0.27861569 0.18615067 0.20429566 0.17501309 0.18618131
 0.20641535 0.20442431 0.18619634 0.19730025 0.21058529 0.19676967
 0.14645479 0.18649942 0.14000892 0.15865158 0.11701081 0.22464272
 0.19691598 0.20392878 0.17676223 0.18558496 0.19059925 0.15547095
 0.22356515 0.11191488 0.18158561 0.14938642 0.19530144 0.1650284
 0.2120509  0.21384036 0.1963215  0.16310771 0.23125344 0.14999418
 0.24403399 0.21234299 0.15752439 0.16255859 0.1945242  0.22961683
 0.17252059 0.10685477 0.21921613 0.11461774 0.13272293 0.21631734
 0.20955334 0.18245944 0.1878594  0.18137325 0.18278984 0.17438289
 0.20208757 0.19907424 0.2126619  0.14275066 0.16061153 0.22085487
 0.19041679 0.20768597 0.19091514 0.20376006 0.18448415 0.19974399
 0.17911097 0.20849996 0.22759879 0.17819012 0.22292878 0.19917808
 0.18726678 0.21501277 0.13532994 0.19865801 0.18883901 0.21553998
 0.16209132 0.20605933 0.20292739 0.21728305 0.19469182 0.17885493
 0.17899223 0.17290612 0.10090079 0.20645222 0.19514118 0.18276475
 0.19743168 0.19646597 0.16785754 0.14942803 0.25342087 0.13388714
 0.22655094 0.14217125 0.22823001 0.19025544 0.18698042 0.16457276
 0.18669411 0.17028699 0.15038419 0.16230509 0.2094942  0.17822688
 0.20747603 0.16771382]
alpha_repeat-1
METEOR
[0.1340561  0.13512628 0.13011246 0.14823123 0.11761935 0.13105899
 0.13497056 0.14903053 0.14452116 0.11298203 0.16801455 0.15332705
 0.17062768 0.1642425  0.14679466 0.15779111 0.15328294 0.15428312
 0.14795825 0.10776601 0.10498778 0.17760047 0.13570359 0.13757767
 0.14845166 0.12814288 0.11724162 0.13081115 0.14314886 0.13304793
 0.16271125 0.1488735  0.14943052 0.13134053 0.1351516  0.14483177
 0.14743901 0.18399659 0.16527096 0.16951121 0.15614637 0.1427379
 0.13518436 0.14338809 0.19123093 0.12926811 0.14278017 0.13703808
 0.13975013 0.12273863 0.11691789 0.09229867 0.16071209 0.1503205
 0.08022082 0.16136751 0.11695572 0.14318272 0.12430012 0.09744094
 0.13564128 0.10917139 0.1467363  0.155884   0.10048462 0.14640468
 0.16220031 0.10510176 0.12156102 0.15012398 0.121019   0.16717784
 0.14537659 0.12121582 0.13113078 0.15454969 0.12390481 0.12995319
 0.12373627 0.13481495 0.10916149 0.12332222 0.10371415 0.10417679
 0.131226   0.11323579 0.10061028 0.1287449  0.14143239 0.10365744
 0.11578782 0.20977737 0.13640166 0.17069344 0.12780828 0.13744142
 0.13861237 0.16084973 0.14640697 0.14803548 0.14849189 0.141143
 0.1304744  0.14948611 0.09581267 0.11412298 0.11124777 0.15986351
 0.14830269 0.14702283 0.1314462  0.13434027 0.15839488 0.11155772
 0.16418427 0.09003664 0.14290567 0.10776109 0.14434973 0.1190714
 0.15092519 0.16314937 0.15147163 0.12231236 0.16790246 0.11852188
 0.19084991 0.16134037 0.11780673 0.1261674  0.14161448 0.17319313
 0.13922902 0.09334866 0.16559096 0.09679573 0.09459316 0.16182531
 0.14924882 0.14490835 0.1346176  0.13269353 0.13631606 0.12621849
 0.14622484 0.14455186 0.1546836  0.11590071 0.12399507 0.15824027
 0.13891187 0.1651633  0.14589548 0.15141413 0.1597379  0.13360887
 0.14015586 0.16012768 0.16296907 0.13770969 0.17305049 0.14881372
 0.13792853 0.15239137 0.0997814  0.15067714 0.15182459 0.15276789
 0.11646454 0.16793057 0.14094594 0.16473821 0.13926882 0.13935329
 0.15265315 0.12761559 0.08091466 0.16255733 0.13809302 0.13859213
 0.14677654 0.14271854 0.13041495 0.12606688 0.19101327 0.10894556
 0.1813563  0.12172849 0.15857949 0.13772977 0.1543028  0.11641601
 0.14699536 0.13371814 0.12386555 0.11778227 0.14987651 0.1445163
 0.15689078 0.12297981]
alpha_repeat-1
BERT
[0.7986402  0.7944795  0.79262125 0.79963636 0.7900684  0.79172564
 0.7926073  0.80068237 0.7939821  0.7976266  0.796263   0.7894271
 0.79571104 0.7998136  0.79886216 0.7922782  0.79408675 0.8058311
 0.79399073 0.7889423  0.78710294 0.7924505  0.79695946 0.7901686
 0.79368496 0.7956691  0.79227966 0.79559827 0.7935389  0.79861265
 0.7919986  0.79821414 0.7934645  0.7906774  0.7972202  0.79604274
 0.7931068  0.79977435 0.79775286 0.79738533 0.7954841  0.7961923
 0.79159695 0.7922717  0.7938405  0.7930966  0.7961863  0.7962162
 0.79298186 0.7928328  0.7922267  0.785519   0.7946117  0.7959422
 0.7915723  0.79276377 0.7893856  0.79339683 0.7923206  0.78690916
 0.7901     0.7932353  0.79553837 0.79653203 0.7895494  0.78688943
 0.79535896 0.7912445  0.7917056  0.79634726 0.79270077 0.7945227
 0.7923756  0.79495716 0.79019815 0.7958108  0.78838664 0.7911982
 0.78840154 0.79175913 0.7918208  0.792196   0.7885372  0.7912369
 0.7976495  0.7910063  0.7921776  0.78987217 0.7989703  0.7916923
 0.7956651  0.7962938  0.79462665 0.7959511  0.79605514 0.79490376
 0.79230917 0.79639816 0.78902954 0.7947903  0.80342174 0.79060847
 0.7944407  0.79283834 0.78962857 0.78857756 0.79024404 0.7967028
 0.79548645 0.8005695  0.7875579  0.7925513  0.7948605  0.7946688
 0.8014294  0.79126024 0.796127   0.79164404 0.7904832  0.79423803
 0.7926593  0.79211354 0.7948859  0.7968747  0.7945057  0.7957054
 0.79906315 0.79748505 0.7911593  0.7866908  0.7917782  0.7933003
 0.7914735  0.79037696 0.7964993  0.78575236 0.79155236 0.7999312
 0.79568416 0.7948626  0.7918968  0.7934453  0.7884714  0.78707254
 0.7917348  0.7964644  0.7971264  0.7932851  0.7923888  0.7960656
 0.7989714  0.79049915 0.7916746  0.7968416  0.7945366  0.7922693
 0.79255366 0.7957029  0.7990836  0.79854065 0.79205805 0.79555494
 0.79534316 0.7994567  0.79044825 0.7977829  0.7960701  0.79444605
 0.79483044 0.7985059  0.7974636  0.7946047  0.7950216  0.79299796
 0.7994848  0.7922879  0.7896616  0.7916669  0.7901144  0.7923831
 0.79303455 0.79524666 0.79205835 0.7979594  0.7945935  0.79400754
 0.80047804 0.79268086 0.79697335 0.7945669  0.7987976  0.79609483
 0.7920471  0.7914193  0.7911213  0.79805386 0.79127204 0.7906328
 0.7978476  0.7965771 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
alpha_repeat-2
WER
[-0.1592941  -0.13941527 -0.12627652 -0.13300355 -0.15132719 -0.12948614
 -0.1231252  -0.1342711  -0.10232657 -0.14649641 -0.13749632 -0.08799192
 -0.12548365 -0.11204215 -0.13443623 -0.11469774 -0.12719823 -0.10457489
 -0.10162027 -0.13307304 -0.10984395 -0.11479136 -0.13364048 -0.13359645
 -0.1208315  -0.13537315 -0.1639332  -0.13905272 -0.10867712 -0.10428163
 -0.14254231 -0.14453909 -0.13618906 -0.11611874 -0.11971983 -0.10464059
 -0.1219895  -0.13977228 -0.14626657 -0.13173352 -0.11580665 -0.13267903
 -0.12532704 -0.13353282 -0.15121241 -0.13596375 -0.12149794 -0.1174232
 -0.13486876 -0.15241011 -0.11371711 -0.16416095 -0.11665251 -0.15516942
 -0.13154803 -0.12380377 -0.10626197 -0.11083323 -0.11690181 -0.14404807
 -0.11615101 -0.13558319 -0.11211588 -0.16118997 -0.1598846  -0.10546871
 -0.14061132 -0.13629711 -0.1224146  -0.15746677 -0.16499114 -0.11604321
 -0.1250095  -0.15256637 -0.1116484  -0.13323834 -0.11954099 -0.09626147
 -0.14296567 -0.15269026 -0.09839132 -0.14758258 -0.11867798 -0.12535397
 -0.10105467 -0.13685676 -0.13112785 -0.15486409 -0.14136495 -0.12425967
 -0.11305052 -0.14024014 -0.14464472 -0.12127871 -0.12604161 -0.12734787
 -0.12984483 -0.1536624  -0.1487602  -0.14617082 -0.13746368 -0.11088998
 -0.11427652 -0.122772   -0.0913938  -0.13044879 -0.15188507 -0.14432458
 -0.1055395  -0.1081789  -0.11742148 -0.13335071 -0.13329381 -0.14252809
 -0.14199314 -0.11905288 -0.11334714 -0.15365185 -0.12454267 -0.1404124
 -0.14007063 -0.10015121 -0.14125272 -0.12849233 -0.12589265 -0.13675905
 -0.11560183 -0.15252004 -0.1401842  -0.15338151 -0.1630522  -0.14251954
 -0.12979732 -0.12642795 -0.15553906 -0.14069279 -0.13469364 -0.10846085
 -0.13604899 -0.13824923 -0.12362759 -0.14148845 -0.10515354 -0.13805719
 -0.11615163 -0.14527426 -0.12222984 -0.13210297 -0.15304262 -0.1298569
 -0.11537202 -0.09825546 -0.13263135 -0.14690776 -0.13132312 -0.14744251
 -0.13294823 -0.13348781 -0.1038397  -0.10449874 -0.10371929 -0.11976665
 -0.12755241 -0.12358565 -0.12268162 -0.11708427 -0.10817193 -0.11272246
 -0.14987296 -0.11440055 -0.11391444 -0.09496579 -0.12336902 -0.13382803
 -0.1340926  -0.14004943 -0.13966738 -0.12232666 -0.11155604 -0.14616479
 -0.13072911 -0.12678132 -0.12016791 -0.15278303 -0.13554939 -0.14203623
 -0.10602989 -0.11025286 -0.10864343 -0.09571059 -0.13128949 -0.07726174
 -0.11002234 -0.12141076 -0.13269625 -0.1305443  -0.14806307 -0.10875522
 -0.09757221 -0.15585294]
alpha_repeat-2
BLEU
[0.14593671 0.15964222 0.14638459 0.17828784 0.17764319 0.19360139
 0.22923244 0.19951967 0.20217457 0.13423343 0.15342197 0.23306641
 0.16507703 0.21548493 0.17534935 0.17914257 0.17553752 0.21686073
 0.19213179 0.17754551 0.24239303 0.19829039 0.19500824 0.22427975
 0.15818513 0.1925863  0.12224856 0.18793547 0.25269271 0.22891718
 0.12505341 0.16484269 0.20133081 0.19221013 0.21990141 0.21739855
 0.21408982 0.19802454 0.14419875 0.18477513 0.21730659 0.17218109
 0.21392013 0.18607892 0.12679513 0.22567972 0.21484341 0.18035683
 0.18023274 0.1935192  0.18974    0.12863765 0.21393789 0.12998438
 0.16520589 0.20492889 0.21972826 0.2070142  0.19087982 0.16329011
 0.18433935 0.19402206 0.21202124 0.17009705 0.15870401 0.25188605
 0.15926071 0.21978076 0.17945305 0.17363772 0.1610407  0.2090133
 0.19571768 0.11795693 0.17374097 0.24943334 0.20344488 0.2079579
 0.19288026 0.14441612 0.19444416 0.16857923 0.16932599 0.17828622
 0.18716611 0.1649798  0.22503196 0.19769247 0.19354302 0.17899902
 0.22598126 0.15804213 0.17340854 0.19729386 0.21766722 0.17196624
 0.18796945 0.13773308 0.15148437 0.14824058 0.20199654 0.22371364
 0.21955851 0.22135696 0.23030377 0.17573283 0.16915195 0.14659585
 0.21385172 0.23249499 0.19255559 0.14233324 0.1849242  0.12164077
 0.11729268 0.16232103 0.16239326 0.17078215 0.1816348  0.14742449
 0.1707349  0.22769041 0.17287021 0.18175887 0.20249683 0.17326281
 0.19246653 0.11427412 0.16572951 0.16110691 0.15716986 0.18071564
 0.20016638 0.21710615 0.12879688 0.13670961 0.17334197 0.22710496
 0.21178082 0.13142953 0.20350716 0.19179545 0.20517632 0.15902691
 0.14859069 0.15859319 0.22209842 0.15865005 0.12557865 0.18849811
 0.20068534 0.23251406 0.2075848  0.20243089 0.18837827 0.16676206
 0.17140895 0.13898639 0.19933611 0.19611526 0.23322142 0.23528377
 0.16597743 0.18770072 0.13804036 0.22209884 0.19472142 0.22491524
 0.17496223 0.22922279 0.15716155 0.20904345 0.19521056 0.18903899
 0.17053077 0.13355986 0.21363031 0.220915   0.20768369 0.18051845
 0.16448937 0.18078124 0.21218852 0.14575486 0.19583403 0.14841356
 0.21198984 0.21108566 0.21489196 0.22490009 0.16738249 0.24876384
 0.21369347 0.16324443 0.20176989 0.17664895 0.13936014 0.23608188
 0.25170596 0.14427368]
alpha_repeat-2
METEOR
[0.09956174 0.11935794 0.10576982 0.12482891 0.12421643 0.14656519
 0.16018525 0.14118143 0.16837799 0.10763782 0.10484064 0.16834921
 0.11444036 0.16005144 0.11851002 0.13197249 0.11824252 0.15270966
 0.14124189 0.13426748 0.16940227 0.15418902 0.15448585 0.1538457
 0.11534471 0.13985855 0.07907609 0.14047622 0.18035175 0.14839643
 0.09283279 0.12386759 0.1493034  0.13360992 0.1492689  0.16228993
 0.16363864 0.14321811 0.11110283 0.12970492 0.14952419 0.12122777
 0.15875813 0.1367211  0.09341866 0.17426093 0.1549697  0.13232946
 0.12719171 0.14335128 0.126882   0.08809129 0.15538247 0.09987682
 0.1215638  0.13720782 0.14417968 0.14883564 0.13991686 0.11244648
 0.13465919 0.14291225 0.14491876 0.13583517 0.11327733 0.18598512
 0.12970355 0.15810588 0.14515353 0.11821365 0.10386466 0.15155296
 0.14840404 0.09483595 0.11472904 0.16409044 0.16239055 0.14543694
 0.14106104 0.10568882 0.1403629  0.1232906  0.11721342 0.13953766
 0.14397921 0.14143447 0.15089995 0.15184542 0.1411234  0.12591746
 0.15325119 0.11593047 0.12973444 0.13952745 0.15739763 0.11819747
 0.13533719 0.11627816 0.10525931 0.10778194 0.12555727 0.15525134
 0.1581286  0.15780011 0.15948053 0.13567656 0.13157711 0.09530866
 0.16547845 0.15492605 0.1442669  0.12787358 0.12736634 0.09244693
 0.0903243  0.12374487 0.1187523  0.12883423 0.12888492 0.11105795
 0.13888745 0.14809911 0.1311468  0.13261928 0.13587879 0.13531946
 0.14159214 0.0950568  0.13450957 0.10589243 0.14251109 0.14001856
 0.15409364 0.16480782 0.11256364 0.10438755 0.12964049 0.14922569
 0.14932198 0.10247863 0.15249951 0.12775063 0.16563536 0.11891231
 0.11249504 0.11911111 0.17831267 0.13352594 0.09663634 0.12755842
 0.15774995 0.14622991 0.15004308 0.15801233 0.13954917 0.11192946
 0.14485633 0.09193918 0.13928215 0.14412152 0.1696105  0.16672308
 0.13072414 0.13768587 0.11199259 0.15467009 0.15652968 0.15978802
 0.13227037 0.15646252 0.14045851 0.14849608 0.15173211 0.14106422
 0.13552263 0.10051734 0.14648238 0.16166832 0.14646335 0.12313607
 0.13130145 0.12082601 0.17250862 0.09734423 0.12931608 0.09906532
 0.14514506 0.16515259 0.14918989 0.17414273 0.13010121 0.17682245
 0.15263535 0.13617868 0.14087236 0.12150892 0.10737605 0.16233248
 0.17807979 0.11173689]
alpha_repeat-2
BERT
[0.79099786 0.79482466 0.79046535 0.79278946 0.79278386 0.7976424
 0.7970384  0.7968012  0.7948414  0.79428536 0.79600006 0.7977847
 0.7921833  0.796784   0.79381996 0.79185665 0.7914563  0.79202205
 0.8008146  0.79130226 0.79435253 0.79291403 0.7969141  0.80054694
 0.79563975 0.79149383 0.7886422  0.79663837 0.7960971  0.79186666
 0.787022   0.79389477 0.79463893 0.7920149  0.7929797  0.7977881
 0.79171    0.7967858  0.7902369  0.7953387  0.7970374  0.79172915
 0.7983858  0.7919453  0.7877068  0.79540896 0.79363513 0.79523087
 0.7918937  0.79305136 0.7958545  0.7899513  0.79248947 0.7888869
 0.79851013 0.8009181  0.7986205  0.79704845 0.79207426 0.7937568
 0.7924987  0.7943553  0.79621357 0.7918568  0.7920257  0.79601574
 0.7927637  0.7915926  0.7912135  0.7916079  0.7927914  0.79196465
 0.7976904  0.793574   0.791818   0.79987305 0.79523945 0.7949549
 0.7990953  0.7907431  0.7919338  0.79801327 0.78783435 0.7974351
 0.7920258  0.7941992  0.79696935 0.7989437  0.7972684  0.79163677
 0.7909138  0.7927223  0.7927669  0.79591405 0.7974462  0.78891486
 0.795122   0.7950207  0.79077417 0.79183364 0.78990054 0.7954282
 0.79618585 0.7959591  0.80052555 0.80080813 0.79885894 0.7897777
 0.7958561  0.7969834  0.7958844  0.7927807  0.7939022  0.79597986
 0.7876409  0.7881929  0.7967985  0.7955565  0.79551643 0.79202473
 0.796297   0.7936089  0.79584    0.7927923  0.7975519  0.79200876
 0.79050136 0.789565   0.78985053 0.7905425  0.79338706 0.793511
 0.79904574 0.7952765  0.79029197 0.78937054 0.7950504  0.7932662
 0.79268783 0.7891124  0.79517865 0.78649485 0.7981465  0.7985847
 0.78972226 0.79232526 0.7946189  0.7922956  0.7918839  0.7971746
 0.7945329  0.79369366 0.795746   0.7932098  0.7965962  0.7928339
 0.7969342  0.79094857 0.79481196 0.79034704 0.794317   0.79673284
 0.79724884 0.7932554  0.78939    0.799005   0.7935256  0.7971184
 0.7946182  0.791714   0.7923248  0.79592377 0.7937954  0.796051
 0.7961764  0.78921986 0.7923184  0.79346395 0.7932253  0.78827053
 0.7907511  0.79623526 0.7960671  0.7903386  0.791855   0.7955606
 0.7926377  0.7937732  0.7923615  0.7959107  0.79156244 0.80130816
 0.7885964  0.79082036 0.79274815 0.79443574 0.79572016 0.796027
 0.801887   0.79078525]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
bravo_repeat-1
WER
[-0.06519756 -0.07119653 -0.09275251 -0.08626354 -0.07679054 -0.09537912
 -0.06853343 -0.05553378 -0.0640065  -0.09462243 -0.0923587  -0.06179522
 -0.08385838 -0.07747737 -0.08671383 -0.08480687 -0.08439139 -0.06136511
 -0.08447559 -0.07539951 -0.07284237 -0.07343208 -0.0675828  -0.07428651
 -0.0785348  -0.08959229 -0.06119213 -0.08551159 -0.07645344 -0.06435719
 -0.07548711 -0.07725702 -0.07739669 -0.08022625 -0.07962158 -0.07848129
 -0.05310644 -0.06025065 -0.07028992 -0.08857482 -0.06344046 -0.05938496
 -0.07695034 -0.0571449  -0.09604807 -0.07287511 -0.08644489 -0.08283431
 -0.06859146 -0.05126527 -0.06681743 -0.0836017  -0.0855802  -0.07506369
 -0.05522162 -0.0867185  -0.07935172 -0.0651942  -0.09638472 -0.08098443
 -0.08554327 -0.06412243 -0.05836648 -0.10870095 -0.06571451 -0.07235011
 -0.07320437 -0.08776417 -0.06211962 -0.07301998 -0.07492722 -0.07901387
 -0.07374869 -0.07627943 -0.06015726 -0.08745595 -0.0669311  -0.06379659
 -0.08214542 -0.03072459 -0.07018214 -0.08032664 -0.0680119  -0.08434778
 -0.09125013 -0.07637116 -0.0915912  -0.08748546 -0.07374995 -0.09210501
 -0.08347673 -0.07793137 -0.08115098 -0.07394648 -0.06716672 -0.09347971
 -0.09039934 -0.08032603 -0.08088318 -0.05856466 -0.08606917 -0.07564816
 -0.07053291 -0.0683609  -0.05088581 -0.08930914 -0.0636783  -0.08686305
 -0.07008071 -0.06854114 -0.08885162 -0.0575555  -0.07624138 -0.07514673
 -0.06761519 -0.07687696 -0.07537361 -0.10285851 -0.09803527 -0.0738451
 -0.08402878 -0.07795819 -0.07419683 -0.09805979 -0.06948109 -0.06694661
 -0.05410373 -0.09053083 -0.07951034 -0.08489887 -0.07502821 -0.08537138
 -0.08736793 -0.07854748 -0.09126141 -0.09455239 -0.08141317 -0.06399164
 -0.06740567 -0.07927805 -0.06800431 -0.07546407 -0.07530806 -0.07500762
 -0.07269006 -0.08205846 -0.0676565  -0.08061159 -0.07073082 -0.07530102
 -0.09991373 -0.0756869  -0.06974583 -0.06813422 -0.10011414 -0.07221085
 -0.07331641 -0.08614075 -0.09312439 -0.0611871  -0.06110979 -0.09014001
 -0.07788891 -0.07866291 -0.06894991 -0.0972558  -0.08485714 -0.08075382
 -0.07549481 -0.08791497 -0.07212103 -0.0646759  -0.06530451 -0.06966984
 -0.06186073 -0.079111   -0.08215437 -0.10118169 -0.0664532  -0.08044272
 -0.04612739 -0.06365414 -0.09633722 -0.0734722  -0.09674268 -0.09912139
 -0.07219326 -0.07929381 -0.08155161 -0.08122781 -0.08410311 -0.08396357
 -0.09849384 -0.06710674 -0.08927663 -0.06860846 -0.0799953  -0.05429255
 -0.08585699 -0.08940868]
bravo_repeat-1
BLEU
[0.21600952 0.21449242 0.17179509 0.20188543 0.2369237  0.20678144
 0.20032963 0.25121786 0.23357474 0.13749714 0.18521159 0.26643692
 0.11481508 0.22007513 0.16484273 0.21687562 0.22060641 0.22886396
 0.18313092 0.21006012 0.1919836  0.21795702 0.28502756 0.22116909
 0.1970119  0.20264398 0.20334955 0.21472696 0.16968243 0.23014745
 0.23612815 0.206871   0.22806456 0.24195762 0.18960615 0.19253145
 0.21167753 0.23142836 0.16981984 0.19054284 0.16469367 0.23100004
 0.23551348 0.26757568 0.23512422 0.25470686 0.17886484 0.21284774
 0.22153389 0.20864894 0.24325822 0.22245186 0.23006582 0.26188503
 0.2735751  0.15234263 0.21534933 0.2065839  0.15672388 0.25475574
 0.20935801 0.25679934 0.22981942 0.15819766 0.21625232 0.23350449
 0.20629057 0.17814705 0.21858623 0.27037272 0.17919148 0.18825087
 0.23751377 0.2278451  0.21882535 0.186945   0.24438959 0.22973297
 0.21853898 0.22481531 0.21618561 0.18826641 0.22264242 0.20501788
 0.13795326 0.24739442 0.2231758  0.16384367 0.20194227 0.22446359
 0.18039027 0.20014975 0.20076408 0.19207985 0.22032817 0.15612388
 0.16922959 0.21996286 0.21502531 0.23027109 0.22253634 0.19968141
 0.19172586 0.22219352 0.23486031 0.22687428 0.21776077 0.22642797
 0.21097185 0.17551946 0.16434179 0.23346356 0.14396701 0.23516824
 0.20911483 0.19278398 0.21229561 0.16336749 0.16122006 0.21809243
 0.22262876 0.15626489 0.22217261 0.15207352 0.19444851 0.24810056
 0.22205832 0.23318448 0.17899967 0.19640449 0.24431004 0.19194194
 0.17440436 0.23611484 0.19908714 0.23029167 0.16397035 0.21697157
 0.20379097 0.18658862 0.20331385 0.23774437 0.21512654 0.2344304
 0.18139001 0.18535849 0.22235198 0.22338025 0.23174057 0.24261794
 0.14082598 0.19634672 0.1953546  0.20835366 0.19143076 0.21317404
 0.25714816 0.1962997  0.17201159 0.25465146 0.20379728 0.18193581
 0.21677043 0.1929655  0.19950815 0.17490318 0.15447814 0.18653455
 0.22534778 0.17949305 0.22083953 0.26537242 0.22886007 0.23724195
 0.24449536 0.24072608 0.17336063 0.17268553 0.23301275 0.23180663
 0.27152158 0.21842943 0.16616338 0.21147413 0.19407359 0.16435724
 0.25872241 0.25275859 0.23244359 0.2340706  0.24027287 0.22162725
 0.15252967 0.21341518 0.2265088  0.24349721 0.21083066 0.24446542
 0.22512182 0.21983291]
bravo_repeat-1
METEOR
[0.16434238 0.14386422 0.11711671 0.15063369 0.15882432 0.1411698
 0.15483596 0.17997731 0.16396688 0.09800726 0.127351   0.1872681
 0.10258208 0.14961178 0.10636259 0.16197262 0.15304805 0.15620547
 0.1371166  0.13963519 0.12986663 0.14263241 0.19667539 0.14460972
 0.14278241 0.14144353 0.1434352  0.15764949 0.12680606 0.15272436
 0.14970803 0.14840385 0.1600135  0.15681546 0.13298029 0.13031575
 0.15685178 0.1686148  0.12772583 0.14487179 0.1238405  0.16062662
 0.1923202  0.19240128 0.16532918 0.17912963 0.13032663 0.15925521
 0.16311563 0.14658964 0.18339908 0.1721368  0.15721651 0.16104378
 0.19467965 0.12546295 0.1458517  0.14465259 0.11056242 0.16638471
 0.15114978 0.184041   0.16838207 0.10794538 0.15341037 0.1605077
 0.14700224 0.12252413 0.14369142 0.19669288 0.12811216 0.1383158
 0.17096036 0.16332009 0.15857054 0.14148727 0.18007461 0.15474208
 0.17441797 0.1622407  0.13966954 0.1292529  0.15339807 0.14788595
 0.11127711 0.15919877 0.15767895 0.12258592 0.15839172 0.14846672
 0.12513626 0.15272801 0.14114663 0.13138526 0.16866139 0.12469246
 0.1252275  0.17869995 0.15247293 0.16438899 0.16232506 0.13558857
 0.15989017 0.1637798  0.18122873 0.16166337 0.16313902 0.15144626
 0.16587625 0.13503105 0.1293208  0.1811446  0.09867498 0.16622095
 0.1484505  0.15332894 0.16619055 0.11639768 0.12290528 0.14669818
 0.14936159 0.12308778 0.15249299 0.10866528 0.14459153 0.17511924
 0.14678198 0.1690771  0.12325477 0.13112791 0.18792136 0.1337571
 0.13041119 0.1747634  0.14302736 0.16535921 0.12394528 0.1616682
 0.14069035 0.13614516 0.15205961 0.16482868 0.16341677 0.17760481
 0.14052213 0.14096226 0.16663592 0.15833832 0.17496431 0.16488396
 0.10931448 0.13819631 0.13103014 0.14361948 0.14809519 0.15755822
 0.16600679 0.13281117 0.12672791 0.16689486 0.13591476 0.14275329
 0.15402808 0.13897356 0.12977995 0.12648829 0.11346101 0.13630733
 0.16365154 0.12478849 0.1613085  0.1919486  0.16511377 0.16864106
 0.16280286 0.17414793 0.12697587 0.11593328 0.15359381 0.1723237
 0.17351518 0.14419265 0.12235027 0.15756758 0.1368654  0.11902696
 0.16736539 0.17602633 0.18602944 0.1640049  0.17858008 0.16410557
 0.12337211 0.14414512 0.15649113 0.18021462 0.14346974 0.17930228
 0.17504419 0.16187276]
bravo_repeat-1
BERT
[0.811435   0.8065119  0.80408466 0.8051514  0.8105256  0.8028288
 0.8071481  0.80815357 0.81258976 0.7994689  0.80398494 0.81296957
 0.8012702  0.8097549  0.8015555  0.81011254 0.80864406 0.8069994
 0.8082156  0.80387366 0.8038221  0.80685985 0.8077245  0.80504274
 0.8080794  0.80564255 0.8027982  0.81337535 0.8027811  0.8050419
 0.8054093  0.80369705 0.80494523 0.8051829  0.8032245  0.8047063
 0.80260664 0.8036847  0.803566   0.80587566 0.80036646 0.8091983
 0.80847794 0.8116594  0.8065952  0.80860984 0.8033585  0.80829054
 0.8046573  0.81230026 0.7998729  0.80789727 0.80583006 0.8068722
 0.8080255  0.80265903 0.8061984  0.8074497  0.80027354 0.8110796
 0.8044837  0.8081121  0.8054844  0.80008346 0.81044406 0.8070685
 0.80413854 0.8028546  0.8044428  0.8032621  0.7985785  0.8068725
 0.80968523 0.8116359  0.80629647 0.80205137 0.81008476 0.8096626
 0.803051   0.8142619  0.8064647  0.8031356  0.8116628  0.8034637
 0.8044229  0.807445   0.8056142  0.8019588  0.80684495 0.8044507
 0.8056398  0.8059537  0.8088195  0.8024575  0.8082551  0.79992425
 0.79855376 0.8126904  0.8082388  0.8070551  0.80916    0.8033467
 0.80351126 0.8069586  0.8115213  0.80760884 0.8057225  0.8042102
 0.80455214 0.80009806 0.8037031  0.80937356 0.8042138  0.80664164
 0.80404973 0.8008461  0.8073452  0.8034084  0.80160505 0.80570155
 0.8043936  0.80507845 0.8028397  0.8040192  0.8035575  0.8071772
 0.8056022  0.8089928  0.80252343 0.81062186 0.8072831  0.80189115
 0.80652934 0.80481267 0.80341154 0.8045654  0.8026641  0.8076583
 0.80847514 0.8007394  0.80700123 0.80976766 0.80767494 0.8077383
 0.8072407  0.8008058  0.80510825 0.80988175 0.8076046  0.81308514
 0.8036701  0.80361176 0.8085203  0.80210716 0.8001926  0.8054203
 0.8077826  0.80722314 0.8062903  0.8082375  0.80155396 0.80222154
 0.80895215 0.80044526 0.8046207  0.80534494 0.80032146 0.8092568
 0.8071695  0.7999836  0.8051214  0.8064522  0.8101959  0.8071092
 0.80668396 0.8113358  0.79959375 0.80367035 0.8117455  0.8040121
 0.8080625  0.8106152  0.80396307 0.8005569  0.8075052  0.80087876
 0.8053575  0.8059749  0.8115029  0.80533105 0.8088885  0.80758023
 0.8059952  0.8024371  0.81019646 0.80121183 0.8054494  0.8116241
 0.80576205 0.8069753 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
bravo_repeat-2
WER
[-0.08594182 -0.08412101 -0.07900718 -0.06431861 -0.07716881 -0.0830954
 -0.07639872 -0.06421594 -0.08568273 -0.09031978 -0.08374305 -0.09347912
 -0.09420397 -0.07691349 -0.04732989 -0.06963317 -0.06543654 -0.08839464
 -0.07250222 -0.07703829 -0.08368424 -0.07821894 -0.06350242 -0.06875493
 -0.05795209 -0.06751528 -0.04830276 -0.05059817 -0.06273156 -0.09436835
 -0.07196036 -0.08604132 -0.08799215 -0.09522671 -0.06321132 -0.07974741
 -0.0772143  -0.09636512 -0.09050709 -0.0818923  -0.07643045 -0.08106842
 -0.08314461 -0.07461363 -0.09209941 -0.08681462 -0.09089154 -0.08492478
 -0.095719   -0.07806423 -0.08104295 -0.08426553 -0.09411089 -0.06956862
 -0.08212793 -0.08277614 -0.0774882  -0.05833261 -0.078245   -0.06799791
 -0.08445345 -0.09173034 -0.04842035 -0.08318974 -0.08738992 -0.07081701
 -0.07971957 -0.07680194 -0.06957763 -0.09293776 -0.07588132 -0.08343671
 -0.08706513 -0.07703356 -0.07056599 -0.08533817 -0.06047979 -0.07949584
 -0.07178913 -0.0827316  -0.07340431 -0.09191959 -0.07703944 -0.09116361
 -0.06034927 -0.06839396 -0.08149497 -0.07271085 -0.06672623 -0.065415
 -0.08025468 -0.05220479 -0.0621715  -0.08058882 -0.05620813 -0.09025058
 -0.09529636 -0.0747213  -0.07973355 -0.05845065 -0.08719076 -0.06383072
 -0.06692589 -0.08979687 -0.07726259 -0.07845878 -0.09233689 -0.07410903
 -0.07478423 -0.071189   -0.07698932 -0.07085551 -0.08057327 -0.08426224
 -0.09561489 -0.0734807  -0.11153653 -0.0808653  -0.07405295 -0.0738981
 -0.09020538 -0.06814229 -0.08967796 -0.09416198 -0.07847787 -0.08959801
 -0.07801049 -0.07827682 -0.07363132 -0.06981228 -0.07317607 -0.05895374
 -0.08333652 -0.08213009 -0.08189172 -0.07266871 -0.07652448 -0.06740051
 -0.07884388 -0.09047147 -0.05371347 -0.07284739 -0.05624086 -0.06999971
 -0.07816659 -0.09174645 -0.10084869 -0.0905924  -0.08296973 -0.07312861
 -0.09104085 -0.08246588 -0.06201281 -0.07244492 -0.07485493 -0.07919621
 -0.08492032 -0.08005655 -0.09499872 -0.07471555 -0.08571371 -0.08896945
 -0.08808925 -0.0569814  -0.07249866 -0.08232203 -0.0873025  -0.07581643
 -0.07558438 -0.06508493 -0.07973246 -0.07703945 -0.07183991 -0.10737724
 -0.05822478 -0.08407815 -0.09140578 -0.08328137 -0.08798785 -0.06209166
 -0.08428093 -0.07623172 -0.07482515 -0.08159678 -0.07419239 -0.07487564
 -0.06793859 -0.06470125 -0.08924103 -0.07875302 -0.077209   -0.07510491
 -0.08181279 -0.06640281 -0.08739727 -0.08055181 -0.0847301  -0.07856042
 -0.0593817  -0.07468651]
bravo_repeat-2
BLEU
[0.19817614 0.19626911 0.22459275 0.25384876 0.14805638 0.20173575
 0.25222057 0.21778765 0.17918691 0.1822393  0.18847976 0.17717225
 0.1749339  0.22817512 0.26749315 0.23757309 0.26792547 0.18884524
 0.22240029 0.21964956 0.23094888 0.1956332  0.23817119 0.20842699
 0.22755999 0.27310106 0.23019908 0.23015025 0.26163019 0.22835129
 0.2477874  0.16261233 0.1851863  0.14530061 0.21775926 0.23750527
 0.22548004 0.21068251 0.18154599 0.1693141  0.17694641 0.18988154
 0.21604119 0.21561436 0.14955744 0.2422292  0.20542648 0.22725738
 0.22481532 0.24086294 0.1962049  0.24218578 0.1942185  0.23156052
 0.27084007 0.17392304 0.23767678 0.23545417 0.22850236 0.20039702
 0.19873962 0.19681833 0.2614134  0.23003022 0.25464843 0.22568781
 0.15396203 0.23044825 0.23979018 0.15060394 0.19842227 0.22747626
 0.24738906 0.19675394 0.22094211 0.20248033 0.19502645 0.23784887
 0.21866478 0.2261161  0.23496719 0.19184585 0.18701021 0.18418397
 0.18263587 0.20889002 0.19097656 0.2350441  0.21146731 0.22086072
 0.18297098 0.2299065  0.18789546 0.20270826 0.22176499 0.24097981
 0.17033875 0.22301152 0.26634932 0.24715721 0.17480875 0.19314585
 0.23513861 0.16666005 0.24522594 0.19833697 0.1803608  0.23394893
 0.2213455  0.22243142 0.23434145 0.21099837 0.16047337 0.26098051
 0.22496264 0.23123987 0.15744754 0.21611743 0.23530251 0.24127466
 0.15196551 0.20271544 0.20621917 0.21598154 0.17225874 0.20463085
 0.17822339 0.24035363 0.2167467  0.21759945 0.22467554 0.21488028
 0.20631097 0.19698764 0.21278293 0.24769985 0.21685152 0.24400822
 0.20330087 0.23032098 0.22124802 0.20819858 0.2069464  0.23423362
 0.22946638 0.19444011 0.17223915 0.15447224 0.23671242 0.25744452
 0.19858694 0.23535349 0.25267261 0.20428796 0.20890102 0.23519742
 0.18391204 0.19462896 0.2513792  0.20185892 0.21590661 0.19971401
 0.16163293 0.21020477 0.1995606  0.2133791  0.22440363 0.2134266
 0.20264756 0.2265398  0.2010485  0.19893779 0.23165998 0.15059647
 0.20788477 0.20720768 0.14592806 0.20516473 0.20203883 0.20243363
 0.20544841 0.24792687 0.2642165  0.23294301 0.22241796 0.25334537
 0.20572839 0.21765743 0.23265971 0.21263581 0.23569188 0.19597528
 0.22363086 0.21699051 0.16594556 0.19844011 0.17152719 0.21558337
 0.21005291 0.19512153]
bravo_repeat-2
METEOR
[0.14125865 0.14109639 0.15353306 0.16460206 0.10964506 0.14589511
 0.16725244 0.14740626 0.13786789 0.14126431 0.13229119 0.11381991
 0.12283664 0.14828749 0.18270755 0.15524591 0.19888102 0.12581641
 0.15936188 0.14489819 0.16654337 0.14439948 0.15736799 0.13834704
 0.17105793 0.18650105 0.15887804 0.16643907 0.17751166 0.15847412
 0.16995123 0.12858735 0.1381962  0.1080574  0.1581818  0.15573945
 0.15054116 0.17499335 0.14635719 0.11174532 0.13828739 0.12999568
 0.14236413 0.15295946 0.12627148 0.16445983 0.13784132 0.16115656
 0.1652695  0.15774464 0.13985236 0.17047687 0.14719766 0.16817
 0.177006   0.13002435 0.16079096 0.16711241 0.15911292 0.12751735
 0.13730922 0.12715164 0.18447816 0.1535322  0.16220334 0.15320307
 0.09564338 0.16892848 0.17422476 0.12242103 0.13356078 0.15809213
 0.17653127 0.13424848 0.15706203 0.14762843 0.14554849 0.17042595
 0.14765766 0.15658484 0.16951445 0.14161878 0.13854768 0.13263911
 0.12666928 0.14688128 0.1457272  0.15679332 0.16436355 0.15633874
 0.13051129 0.15264895 0.13491935 0.15714557 0.13643577 0.16239608
 0.13113819 0.15804436 0.18162952 0.17897526 0.11537616 0.12256417
 0.18572357 0.11951963 0.16256403 0.12434638 0.12162092 0.15132284
 0.15526176 0.15909695 0.17457886 0.14637327 0.11522274 0.18270863
 0.15585857 0.17633962 0.14116001 0.15827626 0.15553943 0.16703563
 0.12069226 0.13644574 0.14213773 0.16157165 0.13719589 0.14792991
 0.14519483 0.15967272 0.14572494 0.15899311 0.16244963 0.14130106
 0.14424157 0.14567001 0.14802972 0.17836028 0.14619453 0.17176557
 0.1437858  0.15299725 0.16079933 0.16017559 0.13975158 0.16145243
 0.15992459 0.13916529 0.124506   0.12119089 0.1656471  0.16927737
 0.14720682 0.18011063 0.1758506  0.14563053 0.14485125 0.16497465
 0.12243836 0.13938103 0.16943571 0.13692277 0.14460038 0.13596795
 0.11349801 0.16077867 0.14869552 0.15455547 0.15023998 0.16358879
 0.13851081 0.15496924 0.14744154 0.14105691 0.16549174 0.12361285
 0.16753553 0.14682302 0.09465866 0.13960809 0.14333548 0.144598
 0.1481084  0.15525138 0.17253707 0.16390938 0.15689212 0.1746147
 0.14443553 0.16435859 0.16569308 0.13858725 0.1591866  0.12743939
 0.14491151 0.15293839 0.11443316 0.14317923 0.12648984 0.16971265
 0.16224784 0.13280338]
bravo_repeat-2
BERT
[0.8028314  0.805448   0.8067557  0.809385   0.79788786 0.80366933
 0.8077203  0.8045266  0.802551   0.80523545 0.80167884 0.8008123
 0.80198884 0.8021995  0.80980897 0.8096931  0.80908    0.80022544
 0.8046251  0.80006164 0.8044755  0.80608815 0.79862165 0.8052142
 0.8084702  0.8038823  0.8043415  0.8068554  0.80724406 0.80847466
 0.80803585 0.8028815  0.80647624 0.8055516  0.8032729  0.801989
 0.80454797 0.8036566  0.8047323  0.8027798  0.80544627 0.79827636
 0.802541   0.8023912  0.8043901  0.8026979  0.80638283 0.81158745
 0.8055574  0.80283356 0.80452335 0.8053091  0.8041105  0.8065139
 0.8058699  0.8015782  0.81000406 0.8093124  0.8046352  0.8060863
 0.803427   0.8057372  0.80794775 0.8088683  0.8072996  0.8083402
 0.80294436 0.8066796  0.8057503  0.8012405  0.8064552  0.80765903
 0.81179017 0.8078432  0.80707234 0.80622303 0.80553865 0.81020933
 0.8051463  0.80851316 0.80237764 0.80568683 0.8075686  0.806748
 0.80199236 0.8052493  0.8055511  0.80878395 0.8077527  0.80566585
 0.8091283  0.8053488  0.80441105 0.8079894  0.802638   0.80748945
 0.8039107  0.8029706  0.8141196  0.8071203  0.8001246  0.8027059
 0.80655324 0.80168617 0.8079728  0.80158883 0.801688   0.8041617
 0.80422604 0.8076613  0.8023743  0.8008458  0.802578   0.8069896
 0.79890144 0.81103134 0.8056021  0.8014903  0.8058442  0.81197536
 0.80409026 0.80358803 0.8049731  0.808988   0.80238926 0.8055071
 0.80522585 0.80392325 0.8059459  0.8022781  0.80767894 0.80308247
 0.8097158  0.80032766 0.8016017  0.8056503  0.80369765 0.8101335
 0.80677676 0.8087473  0.8036854  0.8074964  0.80814594 0.80204654
 0.80588335 0.8053418  0.8034446  0.8024781  0.8019608  0.8122325
 0.801031   0.8082037  0.80860656 0.8048439  0.8013005  0.8061903
 0.8038667  0.80650645 0.8079295  0.80697083 0.8060274  0.80283487
 0.8028256  0.808767   0.8073386  0.8058427  0.8048797  0.80392605
 0.8071517  0.8025446  0.80407417 0.8017674  0.80727077 0.80011237
 0.8081577  0.8037422  0.8038849  0.8031611  0.8079356  0.8065698
 0.8062039  0.8021348  0.8087925  0.8059351  0.8078199  0.80549526
 0.8060883  0.8016787  0.8129662  0.8052947  0.8035763  0.8038441
 0.8032882  0.80420864 0.8032427  0.8005641  0.8028131  0.80830413
 0.80885094 0.8045791 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
charlie_repeat-1
WER
[-0.13581613 -0.14220802 -0.12600818 -0.13464772 -0.13446053 -0.15644058
 -0.12966008 -0.14176124 -0.14610011 -0.14351547 -0.147631   -0.12107649
 -0.1443278  -0.13016102 -0.15835226 -0.16515338 -0.1589271  -0.14983592
 -0.13603578 -0.14859902 -0.14320459 -0.11928513 -0.14534089 -0.14572287
 -0.147003   -0.13817098 -0.16256591 -0.11862739 -0.16288896 -0.1389154
 -0.13604014 -0.14652534 -0.14088634 -0.12493575 -0.14752759 -0.12344012
 -0.14412038 -0.12987506 -0.13531189 -0.13210735 -0.14365149 -0.1629005
 -0.15174    -0.1574314  -0.15762532 -0.13661044 -0.13672366 -0.16482351
 -0.16482023 -0.14392966 -0.12133451 -0.14993644 -0.12452804 -0.12662012
 -0.14582445 -0.14166375 -0.13846941 -0.14536474 -0.13218544 -0.13516091
 -0.13604846 -0.13216895 -0.13627289 -0.14704909 -0.14825207 -0.12795696
 -0.12401553 -0.13336559 -0.13409413 -0.15545169 -0.14230512 -0.14872599
 -0.13185307 -0.12834712 -0.13351058 -0.13251087 -0.12740529 -0.14858207
 -0.15971998 -0.12819096 -0.1455111  -0.14480205 -0.14830131 -0.14596454
 -0.13490303 -0.16240237 -0.13577385 -0.12993118 -0.13659125 -0.16165763
 -0.13707987 -0.11790375 -0.14330667 -0.14526264 -0.14215526 -0.1508333
 -0.12830338 -0.14430464 -0.14445511 -0.14450327 -0.14285062 -0.13242882
 -0.15469826 -0.14074957 -0.13909401 -0.13975311 -0.15501539 -0.14419229
 -0.15930867 -0.1699415  -0.14609511 -0.12733426 -0.14531156 -0.14123596
 -0.1292013  -0.14269798 -0.13230569 -0.14248635 -0.12496294 -0.14690215
 -0.14400886 -0.12671009 -0.15157744 -0.13409064 -0.1397849  -0.13399541
 -0.14140567 -0.13393958 -0.14729882 -0.16376379 -0.14377966 -0.13289812
 -0.14796775 -0.15444419 -0.13690575 -0.14581966 -0.14331524 -0.1514392
 -0.12940219 -0.13461738 -0.15222659 -0.14682483 -0.12243516 -0.1363187
 -0.11446425 -0.13677025 -0.12147559 -0.15592077 -0.15169789 -0.14570402
 -0.151312   -0.13384403 -0.13545525 -0.15527217 -0.12974998 -0.15249525
 -0.14515389 -0.14309604 -0.13116939 -0.13476259 -0.12892503 -0.14084854
 -0.12000974 -0.14061697 -0.15918966 -0.12816476 -0.13483037 -0.15314259
 -0.15071709 -0.13158705 -0.13140159 -0.15831736 -0.12836265 -0.11454553
 -0.15867082 -0.13781068 -0.13218376 -0.14555996 -0.13812921 -0.11249977
 -0.13233958 -0.14731137 -0.16195702 -0.16205459 -0.14897289 -0.13023626
 -0.14387606 -0.13401434 -0.14616688 -0.15469371 -0.14105719 -0.13483197
 -0.12440701 -0.15149204 -0.14095503 -0.14355719 -0.13180107 -0.14040353
 -0.14700494 -0.14039799]
charlie_repeat-1
BLEU
[0.21076852 0.17970054 0.2056552  0.17962115 0.18858521 0.17804401
 0.19157366 0.15590813 0.20280064 0.17755364 0.18286891 0.19899684
 0.15031961 0.18375073 0.15838179 0.16659613 0.16610094 0.18530182
 0.17729994 0.20388325 0.16596967 0.22662603 0.15778189 0.18500263
 0.18755001 0.20937334 0.1756734  0.21526439 0.13012022 0.1840259
 0.17747188 0.13865949 0.16424326 0.18009581 0.18195484 0.16569256
 0.1854075  0.20471493 0.18039397 0.16997047 0.17540589 0.18156839
 0.18236935 0.14744038 0.17311439 0.17000704 0.20003732 0.14655233
 0.15581176 0.17933315 0.20635233 0.1626438  0.21934663 0.19529139
 0.19186849 0.18959932 0.17860657 0.2021086  0.19909454 0.18509579
 0.20332475 0.17592714 0.18135343 0.16378685 0.17877288 0.19211587
 0.19983862 0.19052768 0.18099799 0.17478934 0.22055341 0.15853707
 0.19369227 0.21196868 0.21222169 0.20328465 0.19542281 0.19883417
 0.166435   0.17526802 0.18147754 0.16061027 0.16746923 0.17226201
 0.18731057 0.13521381 0.192222   0.19038361 0.1892968  0.15330995
 0.16563727 0.22476505 0.13998173 0.18521608 0.18209153 0.17702717
 0.16125859 0.19236547 0.15716782 0.19762154 0.16152385 0.1736596
 0.15682982 0.19299306 0.20553329 0.18871219 0.16376217 0.19613135
 0.1602692  0.13491875 0.17785194 0.20306516 0.17474616 0.18013556
 0.17324141 0.18790157 0.22419953 0.14680438 0.18764305 0.1886784
 0.16000374 0.16576439 0.1719758  0.1759304  0.18249195 0.18165245
 0.19851159 0.19828912 0.18236695 0.17191186 0.18272482 0.19127942
 0.17244789 0.16787435 0.23544346 0.20394159 0.18934646 0.15932967
 0.21750342 0.1868623  0.15309987 0.17257286 0.18626064 0.18187346
 0.18860217 0.20989778 0.20012559 0.14894512 0.17276277 0.16384751
 0.15584227 0.19527215 0.18084865 0.17069538 0.16242542 0.16194567
 0.17975825 0.19368076 0.18597764 0.18552792 0.19850344 0.18057736
 0.21398332 0.17106033 0.18488334 0.17905974 0.1514624  0.15730547
 0.18300707 0.16978171 0.17066119 0.15568488 0.18741324 0.1779342
 0.15490871 0.18259307 0.18337313 0.18841279 0.19821283 0.19391782
 0.20608748 0.15856402 0.16036053 0.15808139 0.16108902 0.16609142
 0.15154381 0.18271364 0.18611521 0.16772281 0.20601656 0.19983267
 0.21326678 0.1704429  0.18086632 0.17536151 0.1881302  0.16804731
 0.15472356 0.18908961]
charlie_repeat-1
METEOR
[0.16363734 0.16256403 0.14805535 0.15298177 0.16393048 0.13216452
 0.16318296 0.16169431 0.16007744 0.11640576 0.15771847 0.16600524
 0.14406573 0.15004933 0.12964736 0.13976233 0.13030501 0.15738269
 0.13811044 0.15728657 0.13840046 0.18659966 0.11894979 0.13471021
 0.14533552 0.1700084  0.1397409  0.1841926  0.09743835 0.13354033
 0.14083242 0.11737172 0.12532898 0.15672301 0.14465566 0.15021451
 0.13350045 0.16822517 0.14635799 0.12821103 0.12616239 0.13559841
 0.13416253 0.12575387 0.1271177  0.14177305 0.15280276 0.14334888
 0.1335635  0.14980638 0.17691524 0.13660079 0.16629518 0.16450275
 0.16082941 0.17563152 0.1572204  0.16808937 0.16665828 0.15587419
 0.1759085  0.15051737 0.13523634 0.157754   0.13193224 0.14577098
 0.17178988 0.12974334 0.12958973 0.15168936 0.16444221 0.12083309
 0.18855571 0.18716944 0.15230614 0.15921274 0.18860388 0.1745632
 0.12993428 0.19089115 0.15128351 0.12709051 0.14809118 0.13769945
 0.15045567 0.13371358 0.16105691 0.15881738 0.15438202 0.12567136
 0.15908097 0.16623949 0.13151816 0.13665361 0.16033864 0.13840044
 0.1457456  0.16025333 0.15467713 0.17071166 0.13974324 0.15002494
 0.12230047 0.16968456 0.16297866 0.14426756 0.13123979 0.14323701
 0.12089293 0.11068097 0.14083282 0.19048371 0.15535758 0.13780482
 0.13772069 0.13656866 0.16853533 0.12788043 0.18214765 0.14870478
 0.14102091 0.14199025 0.14707155 0.16016366 0.14934167 0.14175381
 0.16047796 0.15549789 0.12523812 0.14997762 0.15180198 0.15735181
 0.16729992 0.12255849 0.17324473 0.1673358  0.16004511 0.1514297
 0.17841176 0.16561103 0.11897219 0.15300575 0.15895002 0.13673202
 0.16889305 0.15640337 0.14996032 0.12810054 0.1564048  0.1347319
 0.11734795 0.16862166 0.15439534 0.14788876 0.1403247  0.11646617
 0.14597529 0.14854426 0.14733733 0.13972466 0.15843838 0.16110003
 0.16797553 0.14100891 0.17839271 0.14951605 0.13350029 0.1285095
 0.15034009 0.15589924 0.16169335 0.13109177 0.13651467 0.13966252
 0.13313084 0.14989791 0.14531272 0.14099377 0.1575854  0.17524412
 0.1968824  0.1617282  0.12929691 0.1400569  0.12191169 0.14168011
 0.12651732 0.14516853 0.14424851 0.16435909 0.16034507 0.16667563
 0.17409214 0.13607908 0.14039126 0.14068936 0.17570526 0.12232665
 0.13782263 0.14457165]
charlie_repeat-1
BERT
[0.7893784  0.78537995 0.7923773  0.7780739  0.78862995 0.78168476
 0.78601575 0.7889705  0.78285515 0.7808233  0.78274786 0.78756815
 0.7846715  0.7789446  0.787154   0.7854395  0.78840476 0.7841324
 0.7836839  0.7847156  0.7824696  0.78550225 0.78436893 0.7848799
 0.7838851  0.78705865 0.78275967 0.7885133  0.7833358  0.7789182
 0.78294075 0.78205204 0.78658575 0.78793305 0.781138   0.7873185
 0.7842364  0.78559124 0.7834735  0.78559494 0.78185105 0.785587
 0.7830072  0.778819   0.7839344  0.7864738  0.7853466  0.77998513
 0.7859126  0.7868267  0.78612536 0.78633314 0.79118013 0.78972095
 0.78329515 0.7852201  0.78875    0.78663355 0.78478557 0.7834639
 0.7866905  0.7845766  0.784453   0.78899544 0.7804053  0.7808248
 0.7891466  0.78773636 0.7808186  0.78550273 0.78929985 0.78219485
 0.7835476  0.79030144 0.7879582  0.7831102  0.79315174 0.7845015
 0.7779876  0.787228   0.78387487 0.77972    0.78243524 0.7790324
 0.7887549  0.78094167 0.7849406  0.78102696 0.7836461  0.78659874
 0.7809053  0.79656255 0.790111   0.78556275 0.7851057  0.7826555
 0.78628325 0.7864551  0.7855764  0.7861167  0.7856384  0.7849302
 0.78515804 0.79587626 0.7855372  0.78103656 0.7850196  0.7852123
 0.78363985 0.7856618  0.7779289  0.7864873  0.7821103  0.7849932
 0.78120303 0.78494483 0.78776765 0.7809117  0.7842678  0.78364617
 0.7861325  0.7899054  0.78681844 0.787186   0.782104   0.78116345
 0.7811112  0.7812749  0.7858563  0.7830462  0.7916418  0.78894407
 0.7870138  0.78273076 0.79665405 0.7848421  0.7827303  0.7860076
 0.78473496 0.78434724 0.7815701  0.7844915  0.78053415 0.7820881
 0.78222775 0.7849562  0.78469086 0.7811677  0.7788506  0.77978665
 0.78289944 0.78923106 0.7849224  0.78668326 0.78421384 0.7901555
 0.7849707  0.7894553  0.78648114 0.7837851  0.78301686 0.78235596
 0.7864004  0.7839564  0.7882974  0.78492767 0.79201424 0.78283954
 0.78141767 0.7798515  0.7853399  0.78251034 0.7844404  0.78082424
 0.7813188  0.78450066 0.7823672  0.7826487  0.78237283 0.78412145
 0.7855971  0.7837284  0.78568715 0.7812248  0.7808261  0.7899178
 0.7868812  0.7800572  0.7789849  0.7827143  0.7892135  0.78620166
 0.7843751  0.7873366  0.78607595 0.7854132  0.7918019  0.78131855
 0.78204507 0.790776  ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
charlie_repeat-2
WER
[-0.16789432 -0.16504529 -0.16834889 -0.15521223 -0.16883457 -0.17126683
 -0.18521318 -0.18667431 -0.16754334 -0.17562924 -0.14431485 -0.16766208
 -0.15099492 -0.16698702 -0.17844447 -0.16992137 -0.17977786 -0.19196342
 -0.16996491 -0.16045047 -0.1731332  -0.18311225 -0.17113099 -0.1699025
 -0.16651707 -0.18030619 -0.16170184 -0.180205   -0.16766374 -0.18220143
 -0.17254438 -0.17833904 -0.16185507 -0.17981357 -0.15988271 -0.1854169
 -0.15582218 -0.17354857 -0.17022724 -0.17424852 -0.17128813 -0.17254625
 -0.16986865 -0.18503863 -0.17145475 -0.16260016 -0.17875319 -0.1590621
 -0.18540181 -0.15604301 -0.17500531 -0.15934486 -0.17070044 -0.16963612
 -0.18495804 -0.17729238 -0.17362605 -0.16000268 -0.17012444 -0.15629686
 -0.17386734 -0.1739542  -0.15820521 -0.18871072 -0.17827501 -0.16837728
 -0.17155218 -0.12828532 -0.17879142 -0.15747848 -0.1547412  -0.17597087
 -0.16882486 -0.16294398 -0.17932214 -0.16156318 -0.16728836 -0.17888024
 -0.17700081 -0.17880343 -0.18834507 -0.19072875 -0.17916504 -0.18845115
 -0.15201226 -0.16200609 -0.17624159 -0.18593022 -0.16542009 -0.15822516
 -0.14387324 -0.15018927 -0.16880777 -0.15280253 -0.15909899 -0.18175841
 -0.16159454 -0.16591138 -0.17944127 -0.15428089 -0.17435972 -0.17880502
 -0.17763744 -0.15689664 -0.16011486 -0.16894512 -0.18055096 -0.16955027
 -0.18875903 -0.17639875 -0.18347956 -0.17035414 -0.16642218 -0.16164205
 -0.18909446 -0.16642619 -0.15251784 -0.17267777 -0.1735605  -0.14833119
 -0.16614761 -0.16815502 -0.16565669 -0.18211457 -0.15922478 -0.1794184
 -0.1631722  -0.16098445 -0.17445324 -0.17024122 -0.1882231  -0.16937306
 -0.17586553 -0.16701605 -0.16817736 -0.16651828 -0.17887833 -0.17721958
 -0.15720345 -0.18160492 -0.17668275 -0.1786934  -0.1697379  -0.18459643
 -0.17696025 -0.17037218 -0.16724432 -0.15980964 -0.19526231 -0.16471797
 -0.16968576 -0.16060124 -0.15890584 -0.13831931 -0.18472499 -0.17280958
 -0.1836255  -0.16606709 -0.17207923 -0.18461393 -0.16729615 -0.17183925
 -0.1678406  -0.13939324 -0.1674911  -0.17853374 -0.14536388 -0.16257836
 -0.176752   -0.15764107 -0.14837885 -0.18148017 -0.17164071 -0.14770667
 -0.17528161 -0.1696722  -0.15863197 -0.18472443 -0.15645599 -0.16322019
 -0.17155163 -0.1738313  -0.19063874 -0.16390364 -0.16750939 -0.16476155
 -0.18056483 -0.17497857 -0.15335199 -0.15904703 -0.18336119 -0.15501557
 -0.158245   -0.16711491 -0.17417312 -0.17657752 -0.15657936 -0.17731613
 -0.18298264 -0.17144814]
charlie_repeat-2
BLEU
[0.17250983 0.18751774 0.18651456 0.20668641 0.16455867 0.18717981
 0.17285277 0.17002633 0.16684788 0.18513242 0.20642979 0.19605841
 0.18609961 0.2163293  0.16218214 0.20496947 0.17857053 0.15288528
 0.18195641 0.2139147  0.18694333 0.17367434 0.1795395  0.14205184
 0.21996623 0.20530342 0.21052663 0.16848744 0.17315296 0.17875855
 0.21671463 0.1675767  0.14782514 0.16462992 0.18596838 0.15994638
 0.17568223 0.18683047 0.17044208 0.19873908 0.17568451 0.18149612
 0.19993945 0.17635878 0.19786353 0.18992332 0.17515127 0.18669239
 0.1786364  0.21873012 0.18654413 0.18462522 0.20785588 0.1601195
 0.17976981 0.14472183 0.16957908 0.1864304  0.17525581 0.21484203
 0.1472162  0.16283685 0.18630826 0.13598271 0.20119795 0.17958232
 0.19221253 0.19329786 0.17335062 0.20126906 0.20529653 0.17485205
 0.18232056 0.18878551 0.17333401 0.1760238  0.20385241 0.15022222
 0.20126089 0.18357072 0.20320717 0.1632734  0.18073882 0.1350391
 0.16955693 0.16618034 0.18630632 0.18643761 0.19151671 0.19981309
 0.21634357 0.19963825 0.15154944 0.23163755 0.21224147 0.17379463
 0.16172198 0.18140693 0.14113972 0.20115142 0.18895827 0.1723383
 0.16698704 0.19528046 0.17041406 0.19495885 0.15362051 0.20904993
 0.14142597 0.18144738 0.16828923 0.20581763 0.14481333 0.22190313
 0.19006721 0.2028085  0.20127252 0.1817547  0.14644477 0.19289886
 0.17311475 0.18950525 0.19695196 0.18643957 0.16882318 0.17768159
 0.18076094 0.1786292  0.19321525 0.18136439 0.17877267 0.17755864
 0.1859473  0.19427572 0.2106583  0.18626706 0.21990075 0.21057003
 0.18935017 0.1808944  0.17458999 0.14886457 0.18260902 0.1820597
 0.16564402 0.19406724 0.21599628 0.19149043 0.15093485 0.17305177
 0.17882858 0.2256977  0.19916065 0.20628196 0.16463541 0.19725106
 0.16641413 0.21225191 0.16227299 0.16615029 0.1853888  0.18269808
 0.1728036  0.23520856 0.19100237 0.1564212  0.20915381 0.21267972
 0.18402482 0.20008913 0.19505658 0.17169864 0.17562083 0.17801553
 0.17879527 0.18513096 0.18962744 0.18531017 0.20322556 0.19610689
 0.16839663 0.18043281 0.14845776 0.19290369 0.19892579 0.19053201
 0.16677168 0.18862684 0.19365794 0.19002136 0.19147697 0.17166392
 0.16108968 0.20794353 0.17790007 0.17734098 0.20502309 0.18332252
 0.18827343 0.18071331]
charlie_repeat-2
METEOR
[0.15532059 0.15481079 0.14213591 0.17457791 0.14186919 0.16064406
 0.15497445 0.13144447 0.13752938 0.17393732 0.16827922 0.17862306
 0.15480985 0.17317164 0.14070246 0.16726424 0.16366377 0.16074814
 0.14587288 0.19544805 0.14393651 0.15459807 0.14649474 0.1104246
 0.18290398 0.15272867 0.18347902 0.14054261 0.13767526 0.14593705
 0.16102308 0.13541363 0.1440626  0.12223831 0.17532427 0.12743694
 0.14316002 0.15281313 0.15583103 0.1573369  0.14461313 0.13165668
 0.16886214 0.14409045 0.16944054 0.14359224 0.1452881  0.14590461
 0.12929527 0.17182677 0.1559656  0.14667698 0.16930463 0.12249496
 0.16259892 0.14183114 0.13210592 0.13998347 0.16087097 0.17293811
 0.12950114 0.13291261 0.15732017 0.1143478  0.16033826 0.13548376
 0.16507998 0.1439229  0.15120696 0.17218484 0.18109592 0.13084209
 0.16623755 0.13714301 0.12984683 0.13373809 0.16161292 0.13886
 0.15808827 0.15429484 0.15620216 0.12416886 0.16965588 0.14103638
 0.11888472 0.15742112 0.15077663 0.15750594 0.15626813 0.17077688
 0.15877253 0.1517062  0.11803293 0.18565408 0.17734106 0.14450946
 0.13733896 0.15147743 0.13329438 0.133716   0.16559317 0.1410085
 0.15114111 0.1689123  0.13409635 0.15253774 0.12509198 0.17533656
 0.12385702 0.15414039 0.14350699 0.18074205 0.11594964 0.1746885
 0.16721471 0.17464231 0.16543468 0.14302061 0.1268627  0.16428927
 0.1645943  0.16115229 0.17181327 0.16370169 0.16090923 0.17039802
 0.16674599 0.15886569 0.15151747 0.14225578 0.14410013 0.1494904
 0.15402366 0.17448567 0.16548882 0.14248853 0.16210653 0.14398932
 0.19246668 0.16114609 0.15251367 0.11932619 0.13783784 0.15187996
 0.14266572 0.1580999  0.18043574 0.17887295 0.14332924 0.14828203
 0.15052059 0.17071285 0.17405545 0.18729136 0.14997521 0.17495862
 0.13862035 0.18706258 0.12286351 0.13056124 0.14229496 0.14579695
 0.14871777 0.18337935 0.15288742 0.13501436 0.16103519 0.1618817
 0.14176265 0.18696643 0.15183841 0.14496417 0.15676761 0.13349667
 0.13775406 0.16876142 0.1513643  0.16721484 0.14668484 0.15680521
 0.14642581 0.15219354 0.13676027 0.15901244 0.16605467 0.14794208
 0.12855255 0.17169719 0.16505342 0.16956556 0.16723866 0.16070744
 0.12932742 0.16232165 0.1446315  0.15626758 0.14528615 0.15016534
 0.15630811 0.1544102 ]
charlie_repeat-2
BERT
[0.77953374 0.7848225  0.7818574  0.78311425 0.7858379  0.785781
 0.781694   0.788181   0.7866776  0.78682506 0.7836606  0.7854418
 0.7786637  0.78419495 0.7806609  0.78258383 0.78732055 0.7822475
 0.78338844 0.7867685  0.78321123 0.7883734  0.7846655  0.78303033
 0.7891253  0.7911443  0.78725713 0.7850816  0.7836173  0.7868009
 0.7903714  0.78069675 0.7818384  0.78798556 0.7895155  0.7784431
 0.7838121  0.7870029  0.78740853 0.78871053 0.7871784  0.7830136
 0.78142345 0.78501725 0.78718054 0.78419864 0.78863263 0.78415215
 0.7865637  0.7905675  0.7876012  0.7801025  0.7872737  0.7812993
 0.7846522  0.78304726 0.78093123 0.78940386 0.78954595 0.7861413
 0.78412163 0.7841178  0.7843638  0.78347796 0.78626853 0.79155254
 0.7848533  0.79081297 0.7825407  0.78813237 0.78578717 0.78298914
 0.7840354  0.78496635 0.7832524  0.7831852  0.7824841  0.7796353
 0.7851733  0.7919977  0.78557855 0.7833078  0.7817035  0.7840448
 0.78357416 0.7907287  0.7850856  0.78423935 0.7834226  0.7899881
 0.7842863  0.78104115 0.7757705  0.78542346 0.7798868  0.7796915
 0.78249377 0.78133357 0.7787855  0.7861915  0.7865581  0.78311473
 0.78853756 0.7887671  0.78151786 0.7871519  0.7825309  0.7867589
 0.7810672  0.7851229  0.7906733  0.78436625 0.78375876 0.7861431
 0.7834361  0.7878481  0.7887015  0.78755623 0.78538746 0.78093505
 0.7827588  0.78714085 0.78625804 0.7849712  0.7831316  0.7822553
 0.78030473 0.78327847 0.7917077  0.7821745  0.781703   0.7886276
 0.7841528  0.7891871  0.7849869  0.7842427  0.7858807  0.7868933
 0.7830641  0.7929661  0.7843629  0.7817879  0.78174263 0.7885417
 0.78901786 0.7896699  0.7875233  0.7849015  0.7780875  0.78774375
 0.7857892  0.7818253  0.7899482  0.78729945 0.78330445 0.7903626
 0.78375846 0.78815824 0.7827596  0.77974385 0.78388417 0.78016263
 0.78560656 0.7830591  0.78447175 0.7807307  0.7919307  0.78016335
 0.78703105 0.7926508  0.78298765 0.7857333  0.78544104 0.7830228
 0.78494656 0.7816318  0.7847078  0.7868827  0.7845768  0.78340733
 0.78609157 0.7848625  0.7824236  0.78532726 0.7846593  0.78352237
 0.78234136 0.7859361  0.7874971  0.7963877  0.7883953  0.79232246
 0.78390217 0.7848925  0.78371245 0.78679967 0.79278785 0.7834172
 0.7878818  0.7844616 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
delta_repeat-1
WER
[-0.2541518  -0.22611607 -0.25053831 -0.23428421 -0.20628374 -0.2374392
 -0.22908859 -0.23837949 -0.23717842 -0.21883149 -0.24231686 -0.2277324
 -0.22012999 -0.23547027 -0.23188116 -0.22070225 -0.23175741 -0.26142057
 -0.24821501 -0.21293134 -0.24177061 -0.20512722 -0.26439679 -0.24974523
 -0.23104526 -0.20325757 -0.20965282 -0.23712352 -0.22825192 -0.21354094
 -0.22577187 -0.21750083 -0.21115931 -0.2283507  -0.21103495 -0.21094356
 -0.23041637 -0.24835126 -0.24953966 -0.24294339 -0.23447483 -0.23032681
 -0.24170244 -0.21876727 -0.21839661 -0.24020659 -0.22334511 -0.21723275
 -0.21721271 -0.23715297 -0.2191862  -0.18332125 -0.2414476  -0.20358959
 -0.21089084 -0.22773412 -0.21917967 -0.2405374  -0.22628667 -0.23423922
 -0.22107298 -0.25977746 -0.22194149 -0.22567934 -0.24249998 -0.25290024
 -0.24367934 -0.21410195 -0.23636604 -0.2255704  -0.25341874 -0.23936588
 -0.2501169  -0.24553826 -0.22771549 -0.23214713 -0.24873816 -0.23470039
 -0.22955009 -0.24416038 -0.22431155 -0.21487394 -0.24770524 -0.21402066
 -0.21970105 -0.23267902 -0.21932374 -0.23844671 -0.21816878 -0.23486686
 -0.22608813 -0.2374665  -0.22440461 -0.21226942 -0.2155695  -0.23552385
 -0.22967638 -0.22203112 -0.22506899 -0.21707139 -0.23559919 -0.22990723
 -0.23619051 -0.24036152 -0.22806062 -0.22276468 -0.24317732 -0.22682124
 -0.19449706 -0.22688434 -0.24372299 -0.2208394  -0.2390602  -0.25882596
 -0.23979669 -0.21271431 -0.23592947 -0.23676345 -0.22909641 -0.21964611
 -0.25629789 -0.22666714 -0.22853526 -0.20277363 -0.25284784 -0.23257354
 -0.24529875 -0.23124818 -0.23150019 -0.24603548 -0.23589545 -0.22774057
 -0.2234649  -0.21986519 -0.20518358 -0.21407611 -0.24643686 -0.22454698
 -0.22806192 -0.25444261 -0.24837137 -0.23240281 -0.23216786 -0.22040666
 -0.2321204  -0.20638192 -0.2392204  -0.25753281 -0.20827172 -0.25026379
 -0.22207683 -0.24025825 -0.231583   -0.2387802  -0.21708793 -0.23525099
 -0.25151263 -0.23406604 -0.21751972 -0.24103886 -0.2170049  -0.23431962
 -0.22426064 -0.22759348 -0.22901144 -0.23578039 -0.22436226 -0.24246682
 -0.21173623 -0.23651425 -0.22110174 -0.21070847 -0.23099108 -0.19398191
 -0.23933886 -0.21853665 -0.22889249 -0.23343423 -0.23305999 -0.22724135
 -0.2448762  -0.24449456 -0.23708258 -0.21844631 -0.22317771 -0.20540688
 -0.24153467 -0.23707755 -0.24044692 -0.24262982 -0.21994204 -0.20790787
 -0.23876518 -0.23348141 -0.22516746 -0.25368613 -0.23474818 -0.24142118
 -0.20292724 -0.205605  ]
delta_repeat-1
BLEU
[0.14998086 0.19000572 0.13251588 0.17328268 0.21795105 0.21272678
 0.18407389 0.16736894 0.20699952 0.18100875 0.15745772 0.18160536
 0.18045448 0.14327665 0.18269482 0.20784961 0.1777742  0.14553985
 0.19634004 0.19796555 0.16316982 0.20660862 0.15836045 0.15418965
 0.15311246 0.21152113 0.2079974  0.1886696  0.15569289 0.24880748
 0.19894992 0.17658283 0.19340915 0.18049671 0.16286945 0.21611167
 0.17943132 0.15123077 0.13613009 0.17377488 0.18560658 0.16595697
 0.15466249 0.13709882 0.17411386 0.19351703 0.18721536 0.22528826
 0.18274557 0.17796149 0.20568129 0.22062308 0.17167413 0.22282306
 0.21192882 0.17069086 0.1939195  0.17238831 0.2040824  0.18295145
 0.2086948  0.1315675  0.18210681 0.19505402 0.16059062 0.14709578
 0.18549223 0.18772186 0.18675503 0.18267692 0.18126775 0.20294145
 0.15478068 0.14935178 0.2001983  0.17150401 0.13543468 0.19307766
 0.18582451 0.1814304  0.13552553 0.19041918 0.13707126 0.16943423
 0.20587844 0.21507207 0.20373196 0.1499256  0.18880084 0.19483057
 0.16867109 0.13314656 0.16863421 0.20303325 0.22596321 0.21206855
 0.18517491 0.18006604 0.22082292 0.21216762 0.18969525 0.15844514
 0.15982027 0.21521944 0.19313429 0.21372009 0.17769688 0.17835995
 0.20991788 0.17219762 0.18861803 0.1802765  0.17146911 0.16733557
 0.17160441 0.19514927 0.18153537 0.1891254  0.17633794 0.18446777
 0.11530361 0.1975361  0.1787973  0.19629279 0.17433683 0.18360594
 0.19829735 0.1903053  0.17647619 0.15422385 0.16439884 0.14045724
 0.21189498 0.19729438 0.21917991 0.16249766 0.1923437  0.17523884
 0.21148491 0.13737362 0.14437025 0.18043056 0.20067932 0.15207582
 0.19858446 0.16953194 0.16649566 0.16668614 0.21300268 0.14182361
 0.14752154 0.18857028 0.19939869 0.19984051 0.19481433 0.18555545
 0.17510188 0.15403848 0.17616038 0.15367596 0.20038337 0.18289755
 0.17619064 0.18481243 0.19035127 0.18215601 0.20409436 0.19941772
 0.1821086  0.1548561  0.17828577 0.19595279 0.21553917 0.22029494
 0.19247905 0.20195965 0.20657168 0.16448044 0.18575988 0.18204778
 0.1528302  0.15506011 0.1724297  0.20114271 0.18506277 0.19504994
 0.18715277 0.18047746 0.16265601 0.15334176 0.20994596 0.18725159
 0.19313601 0.17497878 0.16409223 0.15377204 0.21333393 0.1570221
 0.17094719 0.21272207]
delta_repeat-1
METEOR
[0.13292125 0.14511518 0.12368211 0.14886584 0.17787078 0.17089354
 0.17408612 0.13738971 0.16604657 0.1530273  0.12817333 0.15204374
 0.17166512 0.14364125 0.15422322 0.1806931  0.17010666 0.12130812
 0.16438725 0.15180774 0.13970705 0.19725233 0.12358596 0.14758733
 0.12196567 0.18776869 0.17496413 0.13732008 0.13171653 0.22369231
 0.16658918 0.14757931 0.16100542 0.16503062 0.15691021 0.17850462
 0.14769886 0.13843178 0.14320828 0.15040019 0.15678835 0.15346243
 0.12249887 0.12402019 0.15566664 0.17306955 0.15328081 0.19680459
 0.15522671 0.1540547  0.17791708 0.17763979 0.15826579 0.20488944
 0.15898056 0.15357352 0.16182756 0.16520686 0.1911346  0.16705935
 0.17167889 0.11127729 0.16196653 0.1468231  0.16855796 0.13071089
 0.17344712 0.16478364 0.16304774 0.17548191 0.15947323 0.16602318
 0.13649465 0.1313199  0.17268907 0.14228736 0.11234357 0.14755689
 0.15114594 0.15620008 0.13301697 0.15815194 0.1270446  0.14446593
 0.16539215 0.20385432 0.16251549 0.1362741  0.14878265 0.17277485
 0.14069376 0.11800809 0.15035492 0.18331474 0.19112929 0.16899043
 0.152373   0.15936127 0.17796677 0.16861629 0.16742995 0.1431489
 0.12963626 0.16178513 0.14683332 0.21354458 0.15800259 0.14950514
 0.17902494 0.15754375 0.16086263 0.17796201 0.15966607 0.127917
 0.15194634 0.17674574 0.14830373 0.15607007 0.14546719 0.16800985
 0.10043351 0.19265532 0.14580741 0.1770265  0.14977444 0.15863972
 0.16728944 0.13344183 0.16557902 0.13167713 0.13640995 0.15000495
 0.17656229 0.17560891 0.19663464 0.13567855 0.16849998 0.12911212
 0.18752297 0.12931717 0.14129115 0.18381329 0.19574268 0.12737137
 0.15831994 0.14514224 0.1585836  0.1350258  0.16765425 0.13705268
 0.14481141 0.16995191 0.16172905 0.15245164 0.18642324 0.15654678
 0.13793767 0.14241918 0.14340361 0.14985222 0.16414789 0.16625705
 0.14972832 0.16138469 0.17302089 0.1673019  0.17880823 0.16717307
 0.16427706 0.1332745  0.17740378 0.16771396 0.20485801 0.17698581
 0.1566009  0.1581093  0.1598733  0.15484339 0.13750408 0.17464842
 0.14735342 0.13608264 0.15279335 0.16140046 0.16555001 0.16671528
 0.14664956 0.15651288 0.14589817 0.13751323 0.18598402 0.16414685
 0.17854099 0.15288259 0.17215855 0.11707251 0.16071427 0.14958861
 0.15948595 0.17024458]
delta_repeat-1
BERT
[0.7954267  0.80437505 0.79752266 0.798934   0.8072553  0.80495125
 0.7975743  0.79659355 0.8088827  0.7998863  0.7937022  0.8043702
 0.802699   0.7951741  0.7997484  0.8111574  0.7990724  0.8016936
 0.8078151  0.7994926  0.79467    0.801314   0.8052653  0.79653317
 0.79456943 0.80843204 0.8063781  0.79370767 0.79686254 0.8134091
 0.8073427  0.8005633  0.8044057  0.8032435  0.8109971  0.80056363
 0.80613923 0.7959498  0.7963408  0.80521643 0.8036658  0.80367315
 0.8017774  0.79505473 0.7940009  0.80363476 0.8091059  0.8099409
 0.7989031  0.80210125 0.8086475  0.80585545 0.80254    0.7975837
 0.8083738  0.8028341  0.7962264  0.80132955 0.8000983  0.802988
 0.80486035 0.7955259  0.7956193  0.8081431  0.80807924 0.7960751
 0.8040934  0.80302644 0.7950328  0.8044885  0.80833423 0.8026709
 0.8034345  0.7931231  0.8088513  0.79867864 0.79335904 0.80585545
 0.8054763  0.80817807 0.79480016 0.8021142  0.78871185 0.8008856
 0.7993441  0.8072838  0.80394864 0.7925969  0.796951   0.80663115
 0.7940747  0.79626393 0.79887205 0.80410165 0.8053219  0.7955344
 0.80271345 0.7924377  0.80461186 0.80560106 0.80046993 0.79264253
 0.79686725 0.8040823  0.8057638  0.81104296 0.79983765 0.80315566
 0.81105393 0.7982067  0.80188304 0.79965144 0.80739474 0.80003804
 0.79698765 0.7997984  0.79519516 0.80523264 0.79649884 0.8057475
 0.7944432  0.8063345  0.80067897 0.7982304  0.79714894 0.7948713
 0.8038975  0.8040556  0.8076522  0.7926487  0.79187393 0.79696023
 0.80673057 0.80607086 0.816282   0.8014161  0.799577   0.8031837
 0.80411327 0.7961376  0.7939083  0.8072697  0.8074941  0.79420733
 0.80410665 0.800398   0.8025884  0.80008477 0.80707586 0.7948549
 0.79548395 0.79581285 0.812051   0.80476594 0.8021607  0.7985355
 0.79782987 0.80309397 0.8083539  0.804038   0.8035706  0.79827744
 0.79682076 0.8012982  0.8031795  0.8032946  0.8041705  0.8053068
 0.80344254 0.7985891  0.8015118  0.80331105 0.8040726  0.807315
 0.80386597 0.8032581  0.80258065 0.79622746 0.79710734 0.7993591
 0.7925378  0.8018293  0.79849213 0.8008098  0.7964181  0.7973725
 0.7956581  0.80601734 0.7995267  0.7961064  0.8049949  0.80375767
 0.800211   0.80582625 0.8002583  0.7946715  0.80192184 0.80334705
 0.80158865 0.8016002 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
delta_repeat-2
WER
[-0.26969162 -0.27216829 -0.28452606 -0.2860326  -0.27681038 -0.27961993
 -0.29016133 -0.28329346 -0.29280963 -0.27141106 -0.27126836 -0.25572577
 -0.30104614 -0.28316297 -0.28396067 -0.26110227 -0.25884255 -0.2425877
 -0.2584064  -0.23292093 -0.26476325 -0.26983992 -0.28931113 -0.2904187
 -0.2722064  -0.2849119  -0.24657574 -0.29871683 -0.27014429 -0.30318814
 -0.27060686 -0.25721022 -0.27537381 -0.29266707 -0.27562439 -0.23247677
 -0.2696964  -0.27244498 -0.26469504 -0.29141587 -0.26052457 -0.23273223
 -0.26814573 -0.25957714 -0.25828436 -0.24931823 -0.27845992 -0.2915231
 -0.25983425 -0.2557996  -0.30626237 -0.28110765 -0.28293197 -0.28718611
 -0.27590182 -0.26922876 -0.27810452 -0.26678432 -0.27545146 -0.29508866
 -0.26841377 -0.26951348 -0.28902655 -0.27981848 -0.27176603 -0.29254631
 -0.28204416 -0.29072737 -0.28835921 -0.25313516 -0.2608451  -0.25733214
 -0.27599373 -0.27074871 -0.26169906 -0.29077294 -0.26206393 -0.22940705
 -0.28903004 -0.26860771 -0.24166012 -0.25345195 -0.27077078 -0.28782078
 -0.25371738 -0.25321043 -0.26944971 -0.25038744 -0.26921029 -0.28498519
 -0.26947579 -0.26129001 -0.27314934 -0.28981676 -0.28751858 -0.31706058
 -0.2541798  -0.26019538 -0.27091119 -0.28544056 -0.285754   -0.2580081
 -0.28324197 -0.28621903 -0.25097659 -0.25476953 -0.25134268 -0.25407519
 -0.26232978 -0.29063475 -0.24001781 -0.28007395 -0.27070719 -0.28652498
 -0.27209523 -0.28247607 -0.2723038  -0.27354128 -0.2695792  -0.23753748
 -0.27544382 -0.2652877  -0.29288631 -0.27733449 -0.22163336 -0.2730928
 -0.2815943  -0.27202754 -0.28264338 -0.25564988 -0.23443044 -0.2806512
 -0.27298202 -0.24240865 -0.27221336 -0.25466081 -0.26520073 -0.27767006
 -0.26114066 -0.29619583 -0.2520175  -0.25358339 -0.25596379 -0.28331175
 -0.26905059 -0.27508826 -0.29820096 -0.29790858 -0.27039263 -0.28138525
 -0.26931789 -0.26138531 -0.27099555 -0.25978848 -0.25198445 -0.28962691
 -0.27504022 -0.26291142 -0.26282006 -0.26366061 -0.28036046 -0.24838939
 -0.29155718 -0.25934861 -0.25776035 -0.24215696 -0.26627293 -0.26950536
 -0.28847835 -0.25417132 -0.25209171 -0.26780521 -0.27339589 -0.28804795
 -0.2924742  -0.29207781 -0.27021969 -0.26509269 -0.2473922  -0.29364204
 -0.26741487 -0.27517369 -0.26274122 -0.25356076 -0.29415495 -0.26465363
 -0.29526985 -0.26663874 -0.27060658 -0.26618462 -0.25602328 -0.27078125
 -0.28398363 -0.2858423  -0.25457839 -0.27934114 -0.29540592 -0.24500411
 -0.26477905 -0.29201922]
delta_repeat-2
BLEU
[0.16413836 0.17959732 0.1663856  0.15160442 0.12671294 0.16800332
 0.14897702 0.19641865 0.15628208 0.17680686 0.17744627 0.19991409
 0.1660108  0.18507523 0.15160636 0.20971246 0.17851942 0.19134976
 0.21277955 0.2013701  0.2151647  0.17791996 0.16025365 0.1540269
 0.20871608 0.14272874 0.17752977 0.13971721 0.1453247  0.12011423
 0.16559143 0.19072306 0.14145437 0.13492489 0.1890039  0.16949953
 0.14346453 0.20713798 0.18754142 0.17116288 0.20194019 0.21846228
 0.17886918 0.19551505 0.16105816 0.19151152 0.13545441 0.16016207
 0.18086181 0.20635615 0.18329248 0.18228163 0.13480852 0.14286409
 0.1681613  0.18888879 0.14872105 0.16789145 0.17253017 0.16611251
 0.18781999 0.20027619 0.17269319 0.18461981 0.19378984 0.1576872
 0.1929671  0.13724049 0.17740006 0.16775957 0.21785893 0.18842736
 0.15377447 0.19950783 0.18439398 0.2037033  0.17380437 0.21441929
 0.1506911  0.18745097 0.171002   0.22571901 0.21488882 0.15727692
 0.19352355 0.17428098 0.22953273 0.18898412 0.17724989 0.18862467
 0.16068054 0.17414818 0.19219078 0.16065507 0.1647431  0.13166564
 0.17894099 0.21775932 0.19701885 0.14324017 0.14760603 0.17383299
 0.1726711  0.14251016 0.22123626 0.18301679 0.20080584 0.17497495
 0.21242109 0.13107679 0.18992958 0.17793084 0.17300885 0.16513788
 0.17986053 0.14208607 0.17923495 0.18519317 0.16134278 0.1953066
 0.17768122 0.23246415 0.15467013 0.13871823 0.20398196 0.16455544
 0.17828569 0.16751896 0.20593379 0.17629121 0.19652416 0.15716151
 0.1532702  0.19021028 0.16081293 0.1879826  0.15245965 0.1749039
 0.22356041 0.15207794 0.16158797 0.21501711 0.20712131 0.15677133
 0.1854063  0.20204981 0.15623376 0.1481114  0.20531809 0.15432895
 0.20168519 0.17195147 0.18265517 0.1871303  0.15542401 0.20026946
 0.19834543 0.20566694 0.21318518 0.19781513 0.17828338 0.20700576
 0.14569231 0.19944968 0.21536933 0.19628859 0.19894745 0.19836387
 0.17451686 0.1897042  0.16723655 0.20528697 0.17675924 0.18111761
 0.15803942 0.12664756 0.21015684 0.19044502 0.18452412 0.13246529
 0.16409597 0.19353973 0.18468608 0.20980663 0.15311955 0.17521766
 0.19858598 0.1960488  0.19405368 0.18129524 0.19743755 0.19956107
 0.20211015 0.18302708 0.21244257 0.17806036 0.1479841  0.19303872
 0.19026128 0.14205158]
delta_repeat-2
METEOR
[0.1448371  0.17298931 0.16589255 0.13662465 0.1384973  0.15926731
 0.13991518 0.15431523 0.13424074 0.17007276 0.16559421 0.18907006
 0.13594617 0.1610585  0.118978   0.1837514  0.17558999 0.16510524
 0.19532004 0.1998454  0.19304069 0.16689964 0.1487916  0.14542876
 0.18043005 0.12745385 0.18165539 0.13273166 0.14574176 0.11753179
 0.14841871 0.16583706 0.14712104 0.1201018  0.19054115 0.1669642
 0.13759334 0.17831738 0.17137597 0.16976944 0.18078012 0.20089019
 0.16333768 0.16753014 0.14208368 0.17049924 0.12996302 0.13052096
 0.15697587 0.17441427 0.15115367 0.17161143 0.13988033 0.15145764
 0.14758789 0.17321713 0.14769294 0.16573565 0.18882696 0.14224626
 0.1686567  0.17202594 0.15387344 0.16320372 0.16204139 0.14435005
 0.16851441 0.12373858 0.1612058  0.15747157 0.17809864 0.1674387
 0.15062222 0.18611538 0.15676299 0.16681786 0.1800533  0.19231153
 0.12864184 0.16175211 0.14991151 0.20349793 0.19658802 0.14598923
 0.15981764 0.15942449 0.16865811 0.16517857 0.17111969 0.17280514
 0.16789211 0.14654537 0.17036901 0.15751141 0.12850583 0.11320015
 0.18513655 0.16410376 0.16839916 0.11626564 0.13769916 0.15680941
 0.14566071 0.13122063 0.17785238 0.14406352 0.19281431 0.18872268
 0.19374765 0.12645228 0.17377    0.15986718 0.14456781 0.15358003
 0.17248091 0.12591357 0.15871495 0.17661206 0.1344994  0.18456902
 0.16055718 0.20013177 0.142938   0.1258365  0.18021487 0.1444167
 0.17539402 0.15163455 0.18351359 0.15893416 0.16505409 0.14133878
 0.13594371 0.16349673 0.15309754 0.192629   0.1368924  0.14863851
 0.1811664  0.14371244 0.15331668 0.17358283 0.20304918 0.13919177
 0.15222267 0.18758817 0.14374083 0.14004641 0.17642875 0.13256407
 0.18323103 0.15767459 0.16841844 0.15315192 0.14836444 0.18259836
 0.1786775  0.18202582 0.19571568 0.1685112  0.16372091 0.15801561
 0.14442416 0.1637909  0.19448947 0.1573454  0.18046677 0.16448236
 0.16779838 0.16736192 0.15437069 0.19718827 0.14599827 0.16303164
 0.16590007 0.13627705 0.18297428 0.17510303 0.17714138 0.12092599
 0.14771465 0.16802243 0.14316017 0.16347989 0.12513543 0.15026703
 0.19310234 0.1688489  0.18389407 0.16830209 0.17475701 0.19102447
 0.18125588 0.15756688 0.21130879 0.15779905 0.15526795 0.19124247
 0.15231012 0.13826864]
delta_repeat-2
BERT
[0.79845697 0.80569047 0.7974033  0.7987638  0.79545605 0.79908353
 0.79582226 0.8079635  0.7895596  0.80159557 0.79780227 0.7969338
 0.7950858  0.7987323  0.79136586 0.79599196 0.8022426  0.79930747
 0.8054944  0.8024849  0.8105797  0.8020565  0.79590213 0.79975176
 0.8021078  0.79698575 0.80042845 0.79717684 0.7946155  0.7978169
 0.79479617 0.8003605  0.7945731  0.7956332  0.8061895  0.80616295
 0.7997506  0.8034171  0.8049275  0.7972724  0.7988806  0.8045657
 0.8077527  0.8068574  0.7965877  0.80334294 0.79498476 0.7956717
 0.7998742  0.80673236 0.79773897 0.79548275 0.7937911  0.8029465
 0.79907906 0.80415076 0.795102   0.7967065  0.81083876 0.7993462
 0.8061463  0.8083606  0.80082816 0.8011735  0.8044472  0.80068684
 0.80895203 0.80028015 0.8019529  0.7998817  0.8018501  0.8017885
 0.8034205  0.8075714  0.80012226 0.8036436  0.7990517  0.80158985
 0.79497397 0.7988882  0.8045644  0.8094168  0.80085605 0.7928599
 0.8009853  0.79834944 0.80171424 0.8091346  0.7962052  0.7988021
 0.7961054  0.80129176 0.80572844 0.7993432  0.79547787 0.7967869
 0.8065046  0.81117964 0.8012659  0.8011615  0.7954831  0.7954244
 0.8081564  0.79191107 0.8117357  0.8021434  0.8018253  0.7977396
 0.8101386  0.79789937 0.80246335 0.79889435 0.800869   0.79685014
 0.80231625 0.7981889  0.792984   0.80189294 0.7943135  0.8034549
 0.7981196  0.8075115  0.7993057  0.7936788  0.804616   0.7973869
 0.804684   0.7931144  0.8039081  0.79797214 0.7990074  0.7950887
 0.795718   0.80262935 0.795927   0.79522306 0.8004145  0.80074775
 0.8054495  0.7973737  0.8035963  0.8029236  0.8021474  0.79399294
 0.7965965  0.80386835 0.79541194 0.7931149  0.79873806 0.7990472
 0.8056716  0.7943631  0.79650265 0.794109   0.7976316  0.80495965
 0.81044865 0.8021431  0.8012635  0.8014966  0.79668957 0.8085845
 0.7976719  0.8068107  0.8024388  0.80054706 0.7988273  0.8027326
 0.79725647 0.8015555  0.8014437  0.8072549  0.7975323  0.79902416
 0.79830074 0.7916485  0.7999917  0.7963535  0.8080748  0.79762256
 0.7944157  0.8031488  0.8078845  0.802636   0.7932144  0.803781
 0.8021175  0.80375004 0.8087719  0.8005597  0.80838776 0.80491686
 0.81294316 0.80249333 0.80385095 0.8014754  0.79748815 0.8082814
 0.80771834 0.7971488 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
echo_repeat-1
WER
[-0.04928731 -0.0520275  -0.05441843 -0.07173378 -0.05791791 -0.07342719
 -0.06316608 -0.04141629 -0.0573416  -0.05180788 -0.06860693 -0.05355345
 -0.05630193 -0.05570216 -0.04871324 -0.05988008 -0.08212006 -0.07755736
 -0.06388765 -0.0636502  -0.05983345 -0.06487227 -0.07932326 -0.06771364
 -0.07792164 -0.07693941 -0.05185011 -0.06539735 -0.05999424 -0.06737969
 -0.06485808 -0.04000475 -0.06921224 -0.06806102 -0.05831707 -0.05953582
 -0.05705308 -0.05927599 -0.06160357 -0.07207094 -0.05233747 -0.04787157
 -0.05992266 -0.04935912 -0.05283982 -0.07211118 -0.07297068 -0.06580759
 -0.04896355 -0.08624406 -0.04926071 -0.04273908 -0.05695534 -0.06584146
 -0.07103523 -0.072513   -0.06691309 -0.05085447 -0.06118348 -0.07044274
 -0.08175268 -0.05875508 -0.06983709 -0.0740703  -0.05406645 -0.07070087
 -0.07471209 -0.06702468 -0.06787019 -0.06487245 -0.04875323 -0.07822292
 -0.07861451 -0.07143732 -0.0544678  -0.0650274  -0.05510664 -0.06699558
 -0.06527707 -0.07374674 -0.0810786  -0.04984424 -0.06590169 -0.0675284
 -0.05198257 -0.06019279 -0.06583281 -0.07306186 -0.07244672 -0.07498719
 -0.07046654 -0.06217859 -0.05971419 -0.07334535 -0.04989725 -0.06156503
 -0.07107337 -0.07641576 -0.07141544 -0.06339713 -0.06528203 -0.04173586
 -0.06512312 -0.07155506 -0.06084663 -0.06572103 -0.05926579 -0.07874167
 -0.07889056 -0.06202977 -0.07166985 -0.04538754 -0.05924986 -0.06750137
 -0.03958742 -0.04775706 -0.07538762 -0.07153147 -0.05845239 -0.07151493
 -0.06243455 -0.0694579  -0.06331156 -0.06669923 -0.06314964 -0.06808425
 -0.07091631 -0.0678556  -0.05044945 -0.07213885 -0.06176803 -0.05109505
 -0.0640303  -0.05778211 -0.07040013 -0.06076093 -0.06443084 -0.071427
 -0.05803893 -0.05932955 -0.06726763 -0.06088405 -0.0518469  -0.08228644
 -0.0606525  -0.06658646 -0.04543897 -0.06855463 -0.05827453 -0.07505303
 -0.07104442 -0.06406279 -0.04661615 -0.06100732 -0.0668642  -0.05624328
 -0.04730077 -0.07047991 -0.05848762 -0.08142234 -0.06068296 -0.07551606
 -0.07407467 -0.08211063 -0.06074289 -0.05822473 -0.06390511 -0.06485182
 -0.06038059 -0.07292061 -0.05792352 -0.0709196  -0.06068812 -0.0502794
 -0.05490653 -0.05347975 -0.07367157 -0.06291795 -0.06932895 -0.06308978
 -0.07445596 -0.05781781 -0.07695426 -0.06587183 -0.0608876  -0.06083321
 -0.06084109 -0.08050283 -0.05762209 -0.0695334  -0.08548571 -0.05231902
 -0.06192684 -0.0721985  -0.05766374 -0.07562379 -0.06620747 -0.06711311
 -0.07815929 -0.0539598 ]
echo_repeat-1
BLEU
[0.22596241 0.16195435 0.19271306 0.16652559 0.19027656 0.16520812
 0.18740376 0.19762178 0.14984769 0.17726596 0.15707035 0.1659219
 0.16304538 0.19879577 0.14657112 0.14503458 0.13043664 0.1396204
 0.17227239 0.14231048 0.16394245 0.1928138  0.14140145 0.11984907
 0.14055912 0.16291607 0.1654649  0.18451418 0.16088676 0.16594934
 0.15137129 0.18665464 0.15478121 0.17872982 0.18571729 0.14517352
 0.16263562 0.10857019 0.18537991 0.13651653 0.18788545 0.14980568
 0.17655368 0.18306647 0.19004396 0.1405257  0.1464161  0.17128763
 0.15114586 0.15781553 0.18935562 0.1871652  0.17139531 0.14793122
 0.13001705 0.13521842 0.16915613 0.1688086  0.16119743 0.15817069
 0.13352629 0.16096536 0.16432952 0.17741029 0.20607221 0.16203693
 0.13238664 0.14291228 0.15433037 0.16147638 0.15697087 0.1825055
 0.13777434 0.14743022 0.15803866 0.18519086 0.14392711 0.15859812
 0.14220293 0.14497881 0.16914459 0.18575339 0.15941074 0.12657295
 0.19252426 0.1638237  0.19513138 0.14075249 0.1532157  0.12667826
 0.1473576  0.17979102 0.16938409 0.14529721 0.20654687 0.1790954
 0.15960157 0.14245972 0.1223729  0.15723617 0.16438194 0.19949289
 0.15440708 0.15872466 0.15119603 0.14741931 0.16384246 0.11436148
 0.14503357 0.15717881 0.15452719 0.16240771 0.15202108 0.1602997
 0.1506627  0.19455763 0.14656691 0.15693712 0.18066053 0.11123259
 0.14914643 0.15803376 0.18426506 0.17409846 0.14251727 0.14492579
 0.15246152 0.15389513 0.1664825  0.17511231 0.16607109 0.20267072
 0.17576863 0.14144457 0.1544146  0.15209449 0.17208779 0.16629665
 0.15110495 0.16457013 0.17469772 0.16389592 0.19316845 0.16691669
 0.14188094 0.17366961 0.17426235 0.18875769 0.1800075  0.17254497
 0.12299707 0.15373209 0.16061189 0.13264187 0.16842237 0.1643513
 0.14652938 0.19373931 0.16450904 0.1285087  0.1625455  0.16733546
 0.12180089 0.1524595  0.1448029  0.15400479 0.18557074 0.17918766
 0.18872393 0.18465336 0.15563009 0.13490394 0.15862929 0.15357573
 0.16924647 0.16573965 0.18520297 0.1810047  0.13622753 0.15807234
 0.1685575  0.1442708  0.13501268 0.15506064 0.18140231 0.17001997
 0.15468335 0.16087365 0.18689935 0.12626877 0.11487882 0.16704057
 0.15658589 0.14628441 0.17648486 0.13515534 0.16363179 0.16732483
 0.12809491 0.17757892]
echo_repeat-1
METEOR
[0.13513022 0.12271458 0.11552249 0.11500175 0.11765778 0.11037696
 0.12207985 0.14169741 0.10023578 0.12616798 0.1083684  0.10182051
 0.09866512 0.13811593 0.12180106 0.0984407  0.09543624 0.09664379
 0.11144556 0.08852364 0.10127953 0.12413077 0.1049652  0.10347104
 0.09594549 0.10745522 0.11796315 0.13577341 0.12727172 0.12573547
 0.12066739 0.11880252 0.10505723 0.12387667 0.11955731 0.10546269
 0.10366071 0.0799494  0.13180711 0.09826465 0.11796934 0.09762792
 0.11877839 0.11271512 0.13245483 0.1031906  0.09461529 0.12263979
 0.10759556 0.11062352 0.12933074 0.12601736 0.12215902 0.10404602
 0.10305358 0.10270882 0.11164305 0.13262465 0.10665381 0.10507175
 0.10169897 0.10131091 0.10695975 0.11868193 0.12008742 0.1169736
 0.084299   0.10864236 0.10147263 0.11345101 0.10610187 0.13108922
 0.09686736 0.08941208 0.11114726 0.11564166 0.09574663 0.10897592
 0.09116338 0.1069449  0.10970327 0.12593632 0.10502853 0.09057073
 0.11250846 0.10564869 0.13396278 0.10784204 0.10075526 0.09067203
 0.10948999 0.1112454  0.1110154  0.11917033 0.14178522 0.11917329
 0.09808868 0.08585544 0.09515129 0.10704323 0.10709214 0.13023203
 0.1135449  0.10345761 0.09849588 0.10290999 0.10670822 0.07602263
 0.09731912 0.1101921  0.10038362 0.11169184 0.09903286 0.1125295
 0.10761918 0.13269663 0.10418086 0.10929012 0.11415927 0.08356476
 0.10797594 0.1157907  0.11586264 0.11183165 0.10156564 0.08365263
 0.09852939 0.11618806 0.1050754  0.11894788 0.10728316 0.12461263
 0.11558622 0.11151958 0.09402483 0.10510305 0.10053803 0.10225433
 0.09384705 0.11275755 0.1171545  0.11229584 0.1442401  0.11649823
 0.09007074 0.11686006 0.11144561 0.12321407 0.13006302 0.11633339
 0.08402797 0.10314758 0.09653514 0.09787198 0.1185242  0.10519906
 0.1018218  0.13998988 0.11911102 0.08064529 0.11255358 0.1159585
 0.07920357 0.0878311  0.11111547 0.10330426 0.11225687 0.10421637
 0.12139714 0.11178934 0.09858606 0.10090776 0.10427109 0.11208306
 0.12437484 0.11221305 0.12207151 0.12517517 0.10814037 0.10272855
 0.11054903 0.09785382 0.09725785 0.10107582 0.10802168 0.10156407
 0.10396748 0.10454477 0.11830139 0.08376827 0.0747588  0.10725326
 0.11125057 0.09688004 0.11385501 0.09156526 0.10353205 0.11436472
 0.08337592 0.11287425]
echo_repeat-1
BERT
[0.784113   0.7807913  0.78137904 0.78212804 0.7796029  0.78527117
 0.7816764  0.781365   0.78016376 0.7834002  0.78038085 0.78175235
 0.7801374  0.7813774  0.7820313  0.78195834 0.77915037 0.77955246
 0.78494483 0.78196913 0.7835332  0.78702444 0.779158   0.7755358
 0.78257406 0.78142494 0.77798903 0.7795222  0.78488046 0.78541964
 0.7809223  0.78457206 0.78266907 0.77779204 0.78668964 0.7862452
 0.7756119  0.7837131  0.7839135  0.77633196 0.7800935  0.77674496
 0.7798154  0.78011227 0.7859117  0.77840805 0.7780868  0.7838989
 0.78149    0.7779576  0.78399146 0.78269064 0.7843215  0.77616894
 0.7785561  0.7818436  0.778732   0.78217864 0.78131115 0.7816766
 0.7827989  0.78256637 0.77734995 0.783398   0.7837168  0.783563
 0.7800238  0.78286076 0.7784016  0.77925354 0.77981067 0.7808589
 0.7755295  0.78022134 0.7793573  0.7856532  0.777446   0.77832144
 0.78242755 0.7764728  0.7804124  0.7833811  0.78160906 0.77772397
 0.78266394 0.7790083  0.7831071  0.78366333 0.782264   0.78230536
 0.7796507  0.77718997 0.78139067 0.7808549  0.78211546 0.7854874
 0.7821257  0.7814827  0.7813986  0.78766066 0.78496104 0.78007007
 0.78450143 0.779025   0.77674574 0.7830678  0.78704935 0.77775043
 0.77862465 0.78498197 0.78149647 0.7835924  0.7781199  0.7845513
 0.78346866 0.78143793 0.7791978  0.78515095 0.78258884 0.78024477
 0.7861023  0.78126895 0.7834385  0.7846783  0.7806109  0.77947944
 0.78183335 0.779363   0.78141606 0.77821475 0.7812929  0.78108436
 0.7796445  0.77848417 0.7846801  0.78186667 0.7838213  0.7854469
 0.782991   0.78402615 0.7800538  0.7843059  0.7819564  0.77898455
 0.77896863 0.78550917 0.7846897  0.7804162  0.77936035 0.7807243
 0.780929   0.7767005  0.78506786 0.7781009  0.7842611  0.78000623
 0.7813932  0.78100085 0.7812082  0.77871126 0.78541636 0.78509325
 0.780061   0.77931863 0.7820302  0.7827514  0.77987164 0.78140604
 0.78111535 0.78427243 0.78324276 0.7806068  0.7782549  0.7805253
 0.7828381  0.7814951  0.7822869  0.77936184 0.77950126 0.7801406
 0.77980906 0.78168094 0.7776026  0.7774891  0.7836019  0.78033817
 0.7853336  0.78231525 0.7773503  0.7797041  0.7789151  0.78539306
 0.77892816 0.77829117 0.7825301  0.777143   0.77811944 0.7794059
 0.7765661  0.7792771 ]
[nltk_data] Downloading package wordnet to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /mnt/iusers01/fse-
[nltk_data]     ugpgt01/compsci01/q12628ct/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
/mnt/iusers01/fse-ugpgt01/compsci01/q12628ct/.conda/envs/semantic-decoding/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
echo_repeat-2
WER
[-0.11472845 -0.11450308 -0.13119201 -0.11750484 -0.10097063 -0.09126495
 -0.11508856 -0.1046154  -0.12306708 -0.08910787 -0.12721413 -0.08853593
 -0.10674938 -0.11647875 -0.10221199 -0.12085053 -0.10869587 -0.12207425
 -0.11284396 -0.10459222 -0.11442513 -0.14107816 -0.08903536 -0.09255712
 -0.11558806 -0.12108537 -0.1146713  -0.11513843 -0.10718274 -0.09679617
 -0.10077661 -0.10916944 -0.11144442 -0.1055998  -0.10184841 -0.11368684
 -0.10938973 -0.11492844 -0.11012309 -0.10004381 -0.12202328 -0.11203514
 -0.10101016 -0.08250627 -0.08757609 -0.1039746  -0.1328192  -0.12490402
 -0.11406071 -0.09656601 -0.11945539 -0.12288295 -0.12290267 -0.10469398
 -0.09878682 -0.1119114  -0.11395802 -0.10408601 -0.13896525 -0.11043424
 -0.1317471  -0.11323543 -0.13118775 -0.11783531 -0.08434353 -0.11747984
 -0.11591602 -0.12747325 -0.11771353 -0.11340054 -0.11417958 -0.11864115
 -0.10073274 -0.10882006 -0.12528742 -0.1038622  -0.10835839 -0.11007592
 -0.11531823 -0.10017074 -0.11376271 -0.10717224 -0.10478404 -0.12033217
 -0.11853938 -0.1064824  -0.12413844 -0.11199576 -0.10473753 -0.11293111
 -0.10737094 -0.11210626 -0.12143955 -0.08939834 -0.12370336 -0.10524867
 -0.11148572 -0.10080127 -0.10189144 -0.10644472 -0.11297368 -0.11675311
 -0.10266154 -0.13024756 -0.11887868 -0.13189771 -0.11253612 -0.11953064
 -0.11670581 -0.10903754 -0.09903852 -0.11473854 -0.10050754 -0.09248019
 -0.1143802  -0.10662415 -0.09617663 -0.07077119 -0.11127294 -0.12315289
 -0.11383577 -0.09577467 -0.09504104 -0.11000905 -0.11231572 -0.08302067
 -0.13880094 -0.09729304 -0.08801073 -0.09904418 -0.11649157 -0.11495248
 -0.13327407 -0.10670605 -0.10310734 -0.11817596 -0.10744618 -0.10920154
 -0.1167955  -0.10704699 -0.12626162 -0.10920041 -0.11225606 -0.11089771
 -0.1049515  -0.12885937 -0.11086139 -0.1066027  -0.10544084 -0.10540164
 -0.12292931 -0.09910446 -0.11917464 -0.11761647 -0.11887051 -0.12593488
 -0.12012952 -0.10460031 -0.11796942 -0.10123893 -0.11138368 -0.10832383
 -0.09580438 -0.11517517 -0.11446847 -0.11338279 -0.10412446 -0.12380278
 -0.11086857 -0.11608798 -0.12972126 -0.12853983 -0.12133082 -0.11748309
 -0.09622752 -0.1083203  -0.10242539 -0.10558219 -0.12054232 -0.0735986
 -0.11199202 -0.11906576 -0.11375399 -0.11104842 -0.11477285 -0.10591134
 -0.12616804 -0.10456707 -0.09933194 -0.11061651 -0.11374844 -0.1204425
 -0.11216628 -0.09905809 -0.07787188 -0.10216124 -0.11359742 -0.12215364
 -0.10673704 -0.0956505 ]
echo_repeat-2
BLEU
[0.13831686 0.16659857 0.16294897 0.17371486 0.17880741 0.16541835
 0.1714245  0.15257374 0.15110493 0.16851648 0.16580432 0.17538348
 0.1542887  0.16310049 0.15227781 0.142283   0.17339528 0.14119267
 0.13237685 0.18263704 0.14075213 0.13112959 0.19354546 0.17905635
 0.16128157 0.14093339 0.1724937  0.15569601 0.16350688 0.15448068
 0.17201498 0.14092933 0.14772889 0.12719348 0.16847377 0.14278596
 0.15409768 0.15029138 0.15288915 0.1790995  0.15533092 0.14887436
 0.14648694 0.20262438 0.16484437 0.18480153 0.14285526 0.16147018
 0.15646187 0.15381421 0.12480729 0.12467442 0.11483277 0.16898011
 0.16030352 0.13321192 0.1350871  0.16948463 0.15023037 0.14787762
 0.16487847 0.1340807  0.17495534 0.14367702 0.19246992 0.1412887
 0.19129758 0.11903384 0.14945764 0.17092496 0.14336807 0.16401125
 0.17708471 0.13999249 0.1447141  0.14201075 0.14461573 0.13607946
 0.15578714 0.16791899 0.13061915 0.18150608 0.14277601 0.16058312
 0.13952584 0.14223579 0.17159394 0.15777846 0.18814492 0.13948144
 0.14744    0.14723965 0.11815441 0.177334   0.13498982 0.16391687
 0.17381972 0.16925153 0.15876228 0.16619003 0.14272294 0.15164293
 0.19482387 0.13847112 0.16768366 0.12966061 0.18405641 0.16847714
 0.15952092 0.16878153 0.17570425 0.16306065 0.19179611 0.16774646
 0.13813979 0.16090102 0.13941056 0.18042953 0.16012573 0.15209182
 0.17929609 0.17069514 0.16698213 0.16149116 0.1563226  0.19472206
 0.14381283 0.20937936 0.18575134 0.16910568 0.13987188 0.13692725
 0.13734667 0.19937677 0.14832642 0.14191625 0.17019952 0.16070881
 0.18851786 0.18091532 0.14458699 0.12249676 0.15795996 0.15351364
 0.18536302 0.14859825 0.16917048 0.17941318 0.15508121 0.17194478
 0.14849487 0.17382562 0.17040287 0.15743431 0.16331    0.14165014
 0.14068645 0.14149654 0.13674162 0.14488367 0.13986478 0.16312413
 0.14200333 0.14909976 0.15056485 0.15321391 0.19563165 0.14835813
 0.18116498 0.17394572 0.13234459 0.14452005 0.1512146  0.13591336
 0.16595057 0.1510978  0.14470199 0.17371289 0.13859935 0.17975757
 0.12856298 0.14838648 0.15699628 0.12762563 0.17622709 0.16775399
 0.16906708 0.14264203 0.12867676 0.1607284  0.14818283 0.12075364
 0.16976333 0.17185388 0.1915848  0.16362527 0.17952404 0.14174104
 0.16332749 0.17913274]
echo_repeat-2
METEOR
[0.09004568 0.1166891  0.1239216  0.12438356 0.13811563 0.10667044
 0.11416322 0.1012883  0.11217352 0.11643454 0.1287703  0.13114689
 0.10156722 0.10772259 0.10386324 0.0944544  0.11373773 0.11249886
 0.08862389 0.12595806 0.0937665  0.10363876 0.1279672  0.11495912
 0.11909383 0.10407601 0.1213808  0.11786675 0.1219264  0.12016397
 0.11994071 0.12077588 0.10714443 0.09191405 0.13041073 0.09430617
 0.13243972 0.10385017 0.09732427 0.10811184 0.12546227 0.10266146
 0.10611887 0.14018367 0.11637733 0.12565261 0.09028699 0.10393516
 0.1197194  0.1290069  0.09470553 0.08797258 0.08032402 0.12417992
 0.13901251 0.08864987 0.12853197 0.11520662 0.10295755 0.11159803
 0.12275584 0.08815829 0.1133483  0.11594175 0.14118791 0.09393721
 0.14121676 0.09052985 0.09989133 0.1146908  0.0978639  0.10246258
 0.14287937 0.1137697  0.11117129 0.11457184 0.1105476  0.10501683
 0.10519728 0.11523469 0.09031126 0.1234488  0.10812359 0.12229996
 0.10159353 0.10282615 0.10683815 0.10768706 0.14249351 0.09979054
 0.1092983  0.10700982 0.08201374 0.12197261 0.08202308 0.1101367
 0.11185082 0.10084766 0.1086331  0.12771929 0.09096219 0.11508531
 0.12966811 0.11234421 0.11232551 0.08868771 0.13806725 0.14386583
 0.10609718 0.11914202 0.12147551 0.11729154 0.13051497 0.12032454
 0.09457831 0.10121491 0.0912603  0.14075492 0.11095068 0.10465231
 0.12204399 0.10747077 0.11070992 0.11964978 0.11942501 0.13563034
 0.11028082 0.14109651 0.1377939  0.11138081 0.1024381  0.10476843
 0.10002355 0.13475768 0.10783375 0.09345132 0.11671129 0.1314098
 0.11888443 0.13047897 0.11262539 0.08289551 0.10759642 0.09862923
 0.131207   0.097199   0.12343137 0.12088375 0.11602677 0.13963078
 0.10595402 0.11534584 0.105055   0.11282603 0.10759861 0.10170576
 0.09364465 0.11310211 0.09621182 0.13262049 0.11492875 0.10562401
 0.09659915 0.11114346 0.10831431 0.11441053 0.13744446 0.10762327
 0.12347757 0.11191857 0.09170815 0.1040618  0.10257605 0.09938924
 0.13465556 0.1127244  0.09287928 0.111169   0.09373801 0.13623163
 0.09491008 0.10305936 0.09624548 0.09150074 0.1373999  0.11471986
 0.12102933 0.10958596 0.09955177 0.11415424 0.09401744 0.0961678
 0.11849627 0.11284403 0.13140896 0.10273264 0.11844158 0.11797183
 0.11238783 0.12337551]
echo_repeat-2
BERT
[0.7773951  0.7876922  0.7806567  0.7801866  0.7845897  0.78468424
 0.77937275 0.78085196 0.77479947 0.78004336 0.78416944 0.77918774
 0.78525245 0.7823826  0.781627   0.77941877 0.7826049  0.7796
 0.7782459  0.7841323  0.7785165  0.7792056  0.78117436 0.7868961
 0.7807383  0.7825405  0.7779251  0.7822753  0.784309   0.7808901
 0.7828462  0.7796357  0.7811196  0.7777272  0.78127307 0.78058606
 0.78171486 0.78043646 0.78018963 0.7810171  0.7875233  0.78143346
 0.7807504  0.784554   0.77875674 0.7848637  0.7769225  0.780786
 0.7793126  0.78284    0.7824029  0.78140724 0.779873   0.7821207
 0.78231263 0.78114223 0.7788818  0.78058314 0.7824122  0.7837251
 0.78200597 0.7818929  0.7826848  0.77923787 0.78602767 0.7798675
 0.78682345 0.78030825 0.7786869  0.7796199  0.77938926 0.7810523
 0.79016083 0.78241444 0.77963525 0.77904856 0.7789211  0.78611785
 0.78434783 0.7793315  0.77883154 0.7832915  0.7803683  0.78241444
 0.7841689  0.7805922  0.78488076 0.7786134  0.78434    0.77836937
 0.7839     0.77950525 0.77836365 0.78236884 0.7795212  0.78159165
 0.7803924  0.77691764 0.7785918  0.7802188  0.7831744  0.7789456
 0.78212655 0.7815561  0.7834493  0.77977055 0.7801087  0.7836602
 0.7801106  0.77983516 0.78581256 0.7861502  0.7820017  0.78420526
 0.77986526 0.78119445 0.7780278  0.7836854  0.783542   0.7798836
 0.78076524 0.78430957 0.78167605 0.7815228  0.7857706  0.7775256
 0.78460956 0.78571236 0.7819333  0.7811297  0.7778282  0.7797454
 0.7766631  0.78081447 0.78066355 0.7851171  0.7811573  0.77953553
 0.7835162  0.782688   0.7796229  0.773967   0.7784548  0.77891135
 0.78178036 0.78052485 0.7813904  0.781429   0.7793784  0.77999234
 0.7801585  0.7799189  0.78237015 0.78233916 0.7802282  0.7814794
 0.7817194  0.78010803 0.7791885  0.78117913 0.7799249  0.78245234
 0.7834446  0.7838349  0.7854185  0.7808156  0.78277326 0.7779437
 0.7849348  0.78492606 0.7781282  0.7842863  0.78123784 0.78024304
 0.7819499  0.78443927 0.78787196 0.78208774 0.78198075 0.7848251
 0.77734303 0.78120524 0.78159994 0.7815472  0.78300476 0.7825764
 0.7802758  0.77895427 0.77792686 0.7837918  0.7817986  0.7797244
 0.78227603 0.7843467  0.7808632  0.7848225  0.7815205  0.78269744
 0.7839069  0.7815044 ]
